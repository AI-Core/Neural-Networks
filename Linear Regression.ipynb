{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is machine learning?\n",
    "\n",
    "### Three types of Machine Learning\n",
    "\n",
    "There are three distinct problems within the Machine Learning field.\n",
    "\n",
    "Unsupervised learning - Where we only have an input and try to model the distribution in order to better understand the underlying structure of it. E.g. we have census data and try to split segment people into different unknown categories\n",
    "\n",
    "Reinforcement Learning - We have an agent in an evironment and it has to learn what actions to take to maximize the reward. E.g. we are trying to get an algorithm to learn how to win at tic-tac-toe autonomously\n",
    "\n",
    "**Supervised Learning**- Where we create to model that can predict an output from a input, given examples of input-output pairs. E.g. we take as input different features about a house such as location, number of rooms, etc and try to predict the price. \n",
    "\n",
    "This is the paradigm we will be implementing in this notebook.\n",
    "\n",
    "\n",
    "Inputs and outputs can take different forms.<br>\n",
    "Other examples of supervised learning include:\n",
    "\n",
    "- Taking in an image as input and outputting the probability that there is a car in the image\n",
    "- Taking in a sequence of words and outputting a probability distribution over the next word\n",
    "\n",
    "\n",
    "Common synonyms\n",
    "- Loss, cost, criterion\n",
    "- Input, Features\n",
    "- Output, Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does data look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/NN1_xy.JPG)\n",
    "\n",
    "Lets create a function that generates some artificial data. <br>The function should return any noisy linear data of size *m* which is a parameter of the function. <br>Although data collected in the real world often has much more complex correlations, linear functions are good simple function that we can test our learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sample_linear_data(m=20): \n",
    "    ground_truth_w = 2.3 # slope\n",
    "    ground_truth_b = -8 #intercept\n",
    "    X = ##\n",
    "    Y = ##\n",
    "    return X, Y #returns X (the input) and Y (labels)\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a model look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create our own model and use it to make a prediction on our data.<br>\n",
    "We will be using a linear model which has a single weight and bias.<br>\n",
    "![title](images/NN1_singlevar_lr_equation.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHypothesis:\n",
    "    def __init__(self): #initalize parameters \n",
    "        self.w =  #weight\n",
    "        self.b =  #bias\n",
    "    def __call__(self, X): #how do we calculate output from an input in our model?\n",
    "        y_hat = ##\n",
    "        return y_hat\n",
    "    def update_params(self, new_w, new_b):\n",
    "        self.w = ##\n",
    "        self.b = ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = LinearHypothesis()\n",
    "y_hat = H(X)\n",
    "print('Input:',X, '\\n')\n",
    "print('W:', H.w, 'B:', H.b, '\\n')\n",
    "print('Prediction:', y_hat, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets visualise our hypothesis vs the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_h_vs_y(X, y_hat, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r', label='Label')\n",
    "    plt.scatter(X, y_hat, c='b', label='Hypothesis', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_h_vs_y(X, y_hat, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we know how good our model is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets calculate the cost. In this case we will use mean squared_error.\n",
    "\n",
    "![title](images/NN1_cost_function.JPG)\n",
    "\n",
    "Complete the function below to return mean square cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(y_hat, labels):\n",
    "    cost = ##\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = L(y_hat, Y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we find the right weight values for our model?\n",
    "\n",
    "#### Random Search\n",
    "\n",
    "![title](images/NN1_randomsearch.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(n_samples):\n",
    "    best_weights = None\n",
    "    best_bias = None\n",
    "    lowest_cost=100000 #initialize it very high\n",
    "    for i in range(n_samples):\n",
    "        ##\n",
    "    print('Lowest cost of', lowest_cost, 'achieved with weight of', best_weights, 'and bias of', best_bias)\n",
    "    return lowest_cost, best_weights, best_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_cost, best_weights, best_bias = random_search(1000)\n",
    "plot_h_vs_y(X, H(X), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "![title](images/NN1_gridsearch.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "def generate_grid_search_values(n_params, n_samples=100, minval=-2.5, maxval=2.5):\n",
    "    ##\n",
    "    return grid_samples\n",
    "\n",
    "def grid_search(grid_search_values):\n",
    "    best_weights = None\n",
    "    best_bias = None\n",
    "    lowest_cost=100000 #initialize it very high\n",
    "    for ##\n",
    "        y_hat = H(X)\n",
    "        cost = L(y_hat, Y)\n",
    "        if cost<lowest_cost:\n",
    "            lowest_cost=cost\n",
    "            best_weights = H.w\n",
    "            best_bias = H.b\n",
    "    print('Lowest cost of', lowest_cost, 'achieved with weight of', best_weights, 'and bias of', best_bias)\n",
    "    return lowest_cost, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_values = list(generate_grid_search_values(2, n_samples=1000))\n",
    "lowest_cost, best_weights = grid_search(grid_search_values)\n",
    "plot_h_vs_y(X, H(X), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is another optimization algorithm that we could use. We can use gradient descent when our model is a differentiable function. Linear functions are pretty simple to differentiate hence why we can use it here.\n",
    "\n",
    "The algorithm starts by randomly initializing our parameters. We then calculate the cost of those parameters and the derivative of our cost w.r.t each parameter. This tells us the direction of steepest ascent. We update each parameter value by taking a step in the opposite direction, proportional to the learning rate\n",
    "\n",
    "![title](images/NN1_grad_descent.JPG)\n",
    "\n",
    "When there is only one parameter, we have a loss curve as shown in the diagrams above. When we have more than one parameter, we have 3d loss surfaces which we perform descent on.\n",
    "\n",
    "Lets calculate the derivative of the parameters with respect to the loss for the linear function we are using.\n",
    "![title](images/NN1_single_grad_calc.JPG)\n",
    "\n",
    "Complete the function below to return the derivative of our loss w.r.t the weight and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHypothesis:\n",
    "    def __init__(self): \n",
    "        self.w = np.random.randn() #weight\n",
    "        self.b = np.random.randn() #bias\n",
    "    def __call__(self, X): \n",
    "        y_hat = self.w*X + self.b\n",
    "        return y_hat\n",
    "    def update_params(self, new_w, new_b):\n",
    "        self.w = new_w\n",
    "        self.b = new_b\n",
    "    def calc_deriv(self, X, y_hat, labels): #what is the derivative?\n",
    "        dLdw = ##\n",
    "        dLdb = ##\n",
    "        return dLdw, dLdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = LinearHypothesis()\n",
    "y_hat = H(X)\n",
    "dLdw, dLdb = H.calc_deriv(X, y_hat, Y)\n",
    "print(dLdw, dLdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can complete the derivatives, complete the train function below to iteratively improve our parameter estimes to minimize the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "H = LinearHypothesis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, X, Y, H, L, plot_cost_curve=False):\n",
    "    all_costs = []\n",
    "    for e in range(num_epochs):\n",
    "        ##\n",
    "        all_costs.append(cost)\n",
    "    if plot_cost_curve:\n",
    "        plt.figure()\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.plot(all_costs)\n",
    "    print('Final cost:', cost)\n",
    "    print('Weight values:', H.w)\n",
    "    print('Bias values:', H.b)\n",
    "    #return cost, H.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_epochs, X, Y, H, L, plot_cost_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_h_vs_y(X, H(X), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling more advanced functions\n",
    "Lets try fitting more complex curves than just a straight line.\n",
    "Complete the function below to return random polynomial data of a given order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial import Polynomial\n",
    "def sample_polynomial_data(m=20, order=3):\n",
    "    coeffs = ##\n",
    "    X = ##\n",
    "    Y = ##\n",
    "    return X, Y, coeffs #returns X (the input), Y (labels) and coefficients for each power\n",
    "\n",
    "m = 20\n",
    "order=3\n",
    "X, Y, ground_truth_coeffs = sample_polynomial_data(m, order)\n",
    "print('X:',X, '\\n')\n",
    "print('Y:',Y, '\\n')\n",
    "print('Ground truth coefficients:', ground_truth_coeffs, '\\n')\n",
    "plot_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear fit\n",
    "As we can see below, our current model lacks the capacity to find a great fit for any polynomial higher than order 1. We call this high bias. To reduce the bias, we need to use a model with higher capacity.\n",
    "\n",
    "![title](images/NN1_bias.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.03\n",
    "H = LinearHypothesis()\n",
    "train(num_epochs, X, Y, H, L, plot_cost_curve=True)\n",
    "plot_h_vs_y(X, H(X), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-variable Linear regression\n",
    "Lets change our model to a polynomial one. We can think of this as a specific case of the general multi variable linear regression problem, where we are passing in higher powers of x as extra input features to our model. Multi varible regression is when we have more than one input feature<br>\n",
    "Our X variable looks like this now since we have multiple input features\n",
    "\n",
    "![title](images/NN1_multi_x.JPG)\n",
    "\n",
    "Our weights become a vector as opposed to a single value\n",
    "\n",
    "![title](images/NN1_weights.JPG)\n",
    "\n",
    "The weights variable (w) becomes a row vector so we need to transpose it when we multiply it by the X matrix\n",
    "\n",
    "![title](images/NN1_lr_equation.JPG)\n",
    "\n",
    "Our gradient calculation changes slightly to account for the fact that we have more weights than one\n",
    "\n",
    "![title](images/NN1_multi_grad_calc.JPG)\n",
    "\n",
    "Change the \\_\\_call\\_\\_ and calc_deriv functions of the class below so it works for multiple input variables.<br>\n",
    "Also complete the create_polynomial_data function to return a copy of the original dataset with extra features which are the orginal x feature raised to higher powers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVariableLinearHypothesis:\n",
    "    def __init__(self, n_vars):\n",
    "        self.n_vars = n_vars\n",
    "        self.b = np.random.randn()\n",
    "        self.w = np.random.randn(n_vars)\n",
    "    def __call__(self, X): #input is of shape (n_datapoints, n_vars)\n",
    "        ##\n",
    "        return y_hat #output is of shape (n_datapoints, 1)\n",
    "    def update_params(self, new_w, new_b):\n",
    "        self.w = new_w\n",
    "        self.b = new_b\n",
    "    def calc_deriv(self, X, y_hat, labels):\n",
    "        diffs = y_hat-labels\n",
    "        dLdw = ##\n",
    "        dLdb = 2*np.sum(diffs)/m\n",
    "        return dLdw, dLdb\n",
    "\n",
    "def create_polynomial_data(X, order=3):\n",
    "    ##\n",
    "    return new_dataset #new_dataset should be shape [m, order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.0000001\n",
    "highest_order_power = 4\n",
    "\n",
    "X_polynomial_augmented = create_polynomial_data(X, highest_order_power)#need normalization to put higher coefficient variables on the same order of magnitude as the others\n",
    "H = MultiVariableLinearHypothesis(n_vars=highest_order_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_epochs, X_polynomial_augmented, Y, H, L, plot_cost_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_h_vs_y(X, H(X_polynomial_augmented), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we run the train function with higher order polynomial inputs, we often get NaN errors. Lets examine why this happens.\n",
    "\n",
    "When we square, cube, etc our original feature, the new features will have a much higher mean. Because the derivative of our cost w.r.t a particular weight is proportional to the value of that feature, the derivatives for the weight will be extremely high. This will lead to huge steps along that weight and even higher gradients. This cycle continues until our gradients have exploded to NaN.\n",
    "\n",
    "In order to solve this problem, we must normalize each of our input features to put them on the same order of magnitude. We do this by subtracting the mean then dividing by the standard deviation.\n",
    "\n",
    "![title](images/NN1_normalisation.JPG)\n",
    "\n",
    "Complete the function below which normalizes our dataset along each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(dataset):\n",
    "    ##\n",
    "    return normalized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.01\n",
    "highest_order_power = 20\n",
    "\n",
    "X_polynomial_augmented = create_polynomial_data(X, highest_order_power)\n",
    "X_normalized = normalize_data(X_polynomial_augmented)\n",
    "H = MultiVariableLinearHypothesis(n_vars=highest_order_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_epochs, X_normalized, Y, H, L, plot_cost_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_h_vs_y(X, H(X_normalized), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Generalisation\n",
    "\n",
    "We build machine learning algorithms to make predictions. So far, we have been testing our algorithm on data points it has already seen but the real measure of success in machine learning is when we can make correct predictions on samples that the algorithm has not seen yet. So lets make a function which will generate us test data by sampling from the same distribution as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_more_polynomial_data(coeffs, m_test=20, rng=3):\n",
    "    poly_func = np.vectorize(Polynomial(coeffs))\n",
    "    X = np.random.randn(m_test)*rng\n",
    "    Y = poly_func(X)\n",
    "    return X, Y#returns X (the input), Y (labels)\n",
    "\n",
    "m_test = 20\n",
    "X_test, Y_test = sample_more_polynomial_data(ground_truth_coeffs, m_test)\n",
    "print('X:',X_test, '\\n')\n",
    "print('Y:',Y_test, '\\n')\n",
    "plot_data(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_polynomial_augmented = create_polynomial_data(X, highest_order_power)\n",
    "X_test_normalized = normalize_data(X_polynomial_augmented)\n",
    "plot_h_vs_y(X, H(X_test_normalized), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting - need for regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we have the opposite problem to high bias. That is, our model's capacity is so high that it can easily fit all the points perfectly but can't extrapolate well. This is called high variance. To reduce this, we can either reduce the capacity of our model or introduce regularization.\n",
    "\n",
    "![title](images/NN1_variance.JPG)\n",
    "\n",
    "Regulatization is anything that biases our algorithm towards a subset of all possible parameters. In this case, we bias the values towards 0. This encourages the coeffecients of all features 0 if they are not contributing to significantly reducing the cost. In this case, we should see lower values for coefficients of high order features.\n",
    "\n",
    "![title](images/NN1_regularization.JPG)\n",
    "\n",
    "Complete the calc_deriv function below to calculate the derivative for our weights with regularization. Use the class property set on intialization called regularization_factor in your calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVariableLinearHypothesis:\n",
    "    def __init__(self, n_vars, regularization_factor=0):\n",
    "        self.regularization_factor = regularization_factor\n",
    "        self.n_vars = n_vars\n",
    "        self.b = np.random.randn()\n",
    "        self.w = np.random.randn(n_vars)\n",
    "    def __call__(self, X): #input is of shape (n_datapoints, n_vars)\n",
    "        y_hat = np.matmul(X, self.w) + self.b\n",
    "        return y_hat #output is of shape (n_datapoints, 1)\n",
    "    def update_params(self, new_w, new_b):\n",
    "        self.w = new_w\n",
    "        self.b = new_b\n",
    "    def calc_deriv(self, X, y_hat, labels):\n",
    "        diffs = y_hat-labels\n",
    "        dLdw = ##\n",
    "        dLdb = 2*np.sum(diffs)/m\n",
    "        return dLdw, dLdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.03\n",
    "highest_order_power = 20\n",
    "regularization_factor = 0.1\n",
    "\n",
    "X_polynomial_augmented = create_polynomial_data(X, highest_order_power)\n",
    "X_normalized = normalize_data(X_polynomial_augmented)\n",
    "H = MultiVariableLinearHypothesis(n_vars=highest_order_power, regularization_factor=regularization_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_epochs, X_normalized, Y, H, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_h_vs_y(X, H(X_normalized), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_h_vs_y(X, H(X_test_normalized), Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
