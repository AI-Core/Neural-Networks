{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- [Fundamental Python]()\n",
    "- [Linear modeals and optimisation]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do the simple models struggle with meaningful tasks?\n",
    "\n",
    "Whereas the size of a house and its price might be linearly correlated, the pixel intensities of an image are certainly not linearly correlated with whether it contains a dog or a cat.\n",
    "\n",
    "![](./images/complex-fn.png)\n",
    "\n",
    "We need to build much more complex models to solve harder problems.\n",
    "\n",
    "## Can we build more complex models by combining many simple transformations?\n",
    "\n",
    "The models we have just seen apply a single transformation to the data. However most problems of practical interest can't be solved using such simple models. \n",
    "\n",
    "Models with greater **capacity** are those which are able to model more complicated functions.\n",
    "\n",
    "A single linear transformation (multiplication by weights of model) stretches the input space by a certain factor in some direction, and adding a constant (bias) shifts it. \n",
    "We call models which apply  \n",
    "What if we applied more than one **layer** of transformations to our inputs, to create a **deep model**. Would we be able to increase the capacity of our model and make it able to model more complex input-output relationships, particularly non-linear ones?\n",
    "\n",
    "![](./images/shallow-vs-deep.png)\n",
    "\n",
    "...well, not quite yet.\n",
    "\n",
    "...if we repeatedly apply a linear transformation, the input can be factorised out of the output, showing that many repeated linear functions are eventually equal to a single linear transformation.\n",
    "\n",
    "![](./images/factor-proof.png)\n",
    "\n",
    "## So how can we increase the capacity of our models?\n",
    "\n",
    "We want to be able to model non-linear functions, so let's try to throw in some non-linear transformations into our model.\n",
    "\n",
    "![](./images/activation.png)\n",
    "\n",
    "These non-linear functions prevent the input being able to be factorised out of the model. Hence the overall transformation can represent non-linear input-output relationships.\n",
    "\n",
    "We call these non-linear functions **activation functions**.\n",
    "\n",
    "However, It's not like we want to introduce really complicated functions into our model - ideally we wouldn't even have to and we could keep things simple. So let's try and complicate things only a minimal amount by keeping our activation functions very simple.\n",
    "\n",
    "Here are some common activation functions. ReLU (Rectified Linear Unit) is by far the most widely used.\n",
    "\n",
    "![](./images/activ-fns.png)\n",
    "\n",
    "Now we have all of the ingredients to fully understand how we can model more complicated functions. Let's look at that all together:\n",
    "\n",
    "![](./images/full-nn.png)\n",
    "\n",
    "Guess what? That is a **neural network**. Surprise.\n",
    "\n",
    "It's just repeated simple linear transformations followed by simple non-linear transformations (activation functions). Simple.\n",
    "\n",
    "Let's learn some jargon.\n",
    "\n",
    "![](./images/nn.png)\n",
    "\n",
    "Neural networks have additional hyperparameters of the depth of the model and the width of each layer. These \n",
    "\n",
    "## What can neural networks do?\n",
    "\n",
    "The motivation that led us to deriving neural networks was that we wanted to model more complex functions. But what functions can a neural network actually represent? Well, as we show below they can actually represent almost all continuous functions. Neural Networks are **general function approximators**.\n",
    "\n",
    "![](./images/univ-approx.png)\n",
    "\n",
    "## How can our neural networks learn to model some function?\n",
    "\n",
    "As we did in the optimisation notebook, we can adjust our model parameters using gradient descent as such:\n",
    "1. Pass input data forward through model to output a prediction\n",
    "2. Calculate loss between predicted output and output label\n",
    "3. Find direction that moving the model parameters in will reduce the error\n",
    "4. Move model weights (parameters) a small amount in that direction \n",
    "\n",
    "![](./images/backprop.png)\n",
    "\n",
    "Here you can see that many terms reappear when computing the gradients of preceeding layers. \n",
    "By caching those terms, we save having to recompute them for these layers nearer the input. This makes finding the gradients of the loss with respect to each weight in the model much more efficient both in terms of memory and speed. \n",
    "This process of computing these gradients effectively is called the **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prepare our data\n",
    "\n",
    "Today we are going to look at a dataset called MNIST (em-nist). It consists of 70,000 images of hand drawn digits from 0-9. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
      "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
      "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
      "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
      "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
      "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
      "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
      "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
      "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
      "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
      "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
      "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
      "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
      "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=transforms.ToTensor(),          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           train=False,\n",
    "                          )\n",
    "\n",
    "# PRINT THEIR LENGTHS AND VISUALISE AN EXAMPLE\n",
    "x = train_data[0][0]    # get the first example\n",
    "print(x)\n",
    "# tensor = # get the actual input data\n",
    "img = x\n",
    "t = transforms.ToPILImage() # create the transform that can be called to convert the tensor into a PIL Image\n",
    "img = t(img)    # call the transform on the tensor\n",
    "img.show()    # show the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Why should we not tune our hyperparameters based on our model's score on the test set?\n",
    "\n",
    "If we adjust our model's hyperparameters so that it performs well on the test set, then we are \n",
    "\n",
    "This is like training for a test and evaluating your performance based on how well you can answer the exact questions that come up.\n",
    "In real life you are unlikely to encounter exactly the same challenges, and so by training on them you will overfit, and not be able to generalise to *different* unseen answers.\n",
    "\n",
    "You may find that a certain set of hyperparameters perform well on the test set, but then fail to perform as well in the wild. \n",
    "Analagously, you may find that a particular \n",
    "\n",
    "## What else can we test them on? \n",
    "\n",
    "We can take some of the data that we plan to train the neural network's weights on and separate it from that main training set. \n",
    "We can then use this split-off data to validate that the current hyperparameters will make our model to perform well on unseen data (both the validation set and the test set are unseen).\n",
    "\n",
    "PyTorch has a utility method `torch.utils.data.random_split()` that makes it easy to randomly split a dataset. Check out the [docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ice/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-44fb3ce81f5a>\", line 2, in <module>\n",
      "    train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation\n",
      "  File \"/home/ice/.local/lib/python3.6/site-packages/torch/utils/data/dataset.py\", line 272, in random_split\n",
      "    raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n",
      "ValueError: Sum of input lengths does not equal the length of the input dataset!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ice/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ice/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ice/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ice/.local/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/usr/lib/python3/dist-packages/py/_vendored_packages/apipkg.py\", line 195, in __getattribute__\n",
      "    return getattr(getmod(), name)\n",
      "  File \"/usr/lib/python3/dist-packages/py/_vendored_packages/apipkg.py\", line 179, in getmod\n",
      "    x = importobj(modpath, None)\n",
      "  File \"/usr/lib/python3/dist-packages/py/_vendored_packages/apipkg.py\", line 69, in importobj\n",
      "    module = __import__(modpath, None, None, ['__doc__'])\n",
      "  File \"/usr/lib/python3/dist-packages/pytest.py\", line 13, in <module>\n",
      "    from _pytest.fixtures import fixture, yield_fixture\n",
      "  File \"/usr/lib/python3/dist-packages/_pytest/fixtures.py\", line 842, in <module>\n",
      "    class FixtureFunctionMarker(object):\n",
      "  File \"/usr/lib/python3/dist-packages/_pytest/fixtures.py\", line 844, in FixtureFunctionMarker\n",
      "    params = attr.ib(convert=attr.converters.optional(tuple))\n",
      "TypeError: attrib() got an unexpected keyword argument 'convert'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sum of input lengths does not equal the length of the input dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Why should we not pass the whole dataset through the model for each update?\n",
    "We know that to perform gradient based optimisation we need to pass inputs through the model (forward pass), and then compute the loss and find how it changes with respect to each of our model's parameters (backward pass). Modern datasets can be abslutely huge. This means that the forward pass can take a long time, as the function which our model represents has to be applied to each and every input given to it for a forward pass.\n",
    "\n",
    "## Why not just pass a single datapoint to the model for each update?\n",
    "We want our model to perform well on all examples, not just single examples. So we want to compute the loss and associated gradients over several examples to get an average\n",
    "\n",
    "## Mini-batch training\n",
    "The modern way to do training is neither full-batch (whole dataset) or fully stochastic (single datapoint). Instead we use mini-batch training, where we sample several (but not all) datapoints to compute a sample of the gradient, which we then use to update the model. The size of the mini-batch is called the **batch size**. Mini-batches are commonly incorrectly referred to as batches, but it's not that deep.\n",
    "\n",
    "We will experiment with the effect of batch size on the training later.\n",
    "\n",
    "## PyTorch's `DataLoader` \n",
    "PyTorch has a handy utility called a `DataLoader` which can pass us our data in mini-batches of a specified batch size. It can also shuffle them for us.\n",
    "\n",
    "Let's use `torch.data.DataLoader` to create data loaders from our train, validation and test datasets now. Hint: look at the [docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification vs multiclass classification\n",
    "\n",
    "In binary classification the output must be either true or false. Either the example falls into this class, or it doesn't. We have seen that we can represent this by our model having a single output node whose value is forced between 0 and 1, and as such represents a confidence in the fact that the example belongs to the positive class. Alternatively, still for binary classification, we could have two output nodes, where the value of the first represents the confidence that the input belongs to the positive class (true/class 1) and the value of the second represents the confidence that the input belongs to the negative class (false/class 2). In this case, the values of each output node must be positive and they must sum to 1, because this output layer represents a probability distribution over the output classes. \n",
    "\n",
    "# single vs double output\n",
    "\n",
    "In the case where we have two nodes to represent true and false, we can think about it as having trained two models, which have exactly the same weights in every layer except for the last. \n",
    "\n",
    "# shared weights diag\n",
    "\n",
    "Treating true and false as separate classes with separate output nodes shows us how we can extend this idea to do multiclass classification; we simply add more nodes and ensure that their values are positive and sum to one.\n",
    "\n",
    "# multiclass diagram\n",
    "\n",
    "### What function can we use to convert the output layer into a distribution over classes?\n",
    "\n",
    "The **softmax function** exponentiates each value in a vector to make it positive and then divides each of them by their sum to normalise them (make them sum to 1). This ensures that the vector then can be interpreted as a probability distribution.\n",
    "\n",
    "# softmax equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a neural network with PyTorch\n",
    "\n",
    "PyTorch makes it really easy for us to build complex models that can be improved via gradient based optimisation. It does this by providing a class named `torch.nn.Module`. Our model classes should inherit from this class because it does a few very useful things for us:\n",
    "\n",
    "1. `torch.nn.Module` keeps track of all `torch.nn.Parameters` that are created within it. So when we add a linear layer to our model, the parameters (matrix of weights) in that layer will be added to a list of our model's parameters. We can retrieve all parameters of our model using its `parameters()` method. We will later pass this (`mymodel.parameters()`) to our optimiser when we tell it that *this* is what it should be optimising.\n",
    "\n",
    "\n",
    "2. `torch.nn.Module` treats the `forward` method (function) of any child class specially by assigning it to the `__call__` method. That means that running `mymodel.forward(some_data)` is equal to `mymodel(some_data)`. \n",
    "\n",
    "\n",
    "It contains many more useful tools\n",
    "\n",
    "[More detail](https://pytorch.org/tutorials/beginner/nn_tutorial.html) on `torch.nn.Module`\n",
    "Check out the docs [here]()\n",
    "\n",
    "Once we have created a class to represent our model, we need to define how it performs the forward pass. What layers of transformations do we need to give it? \n",
    "Check out these [docs](https://pytorch.org/docs/stable/nn.html#linear-layers) to look at all the layers PyTorch provides.\n",
    "Hint: what layer have I linked to?\n",
    "\n",
    "After we've defined some layers for our model we should implement the forward function that will define what happens when we call an instance of our class. This should pass the argument (our input data) through each of the layers, and apply an activation function to them between each, before returning the transformed input as the output. The output should represent a categorical probability distribution over which class the input belongs to. What shape does it need to be? What function does it need to have applied to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(784, 1024)\n",
    "        self.layer2 = torch.nn.Linear(1024, 256)\n",
    "        self.layer3 = torch.nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = F.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "class NeuralNetworkClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()    # initialise parent module\n",
    "        self.layer1 = torch.nn.Linear(784, 1024)\n",
    "        self.layer2 = torch.nn.Linear(1024, 256)\n",
    "        self.layer3 = torch.nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network and visualising it's performance\n",
    "\n",
    "Now we've actually made a template for our model, we can actually\n",
    "- instantiate it by creating an instance of it from our class template\n",
    "- define how we will improve it by specifying an optimiser\n",
    "- define how we will measure its performance by specifying a criterion\n",
    "- train it\n",
    "- write its loss to a graph and see how this changes as it continues to train\n",
    "\n",
    "Let's code that up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 0 \tLoss: tensor(2.3024, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 1 \tLoss: tensor(2.3019, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 2 \tLoss: tensor(2.3005, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 3 \tLoss: tensor(2.2998, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 4 \tLoss: tensor(2.2987, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 5 \tLoss: tensor(2.2973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 6 \tLoss: tensor(2.2959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 7 \tLoss: tensor(2.2947, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 8 \tLoss: tensor(2.2942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 9 \tLoss: tensor(2.2913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 10 \tLoss: tensor(2.2926, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 11 \tLoss: tensor(2.2909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 12 \tLoss: tensor(2.2880, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 13 \tLoss: tensor(2.2884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 14 \tLoss: tensor(2.2863, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 15 \tLoss: tensor(2.2799, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 16 \tLoss: tensor(2.2835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 17 \tLoss: tensor(2.2791, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 18 \tLoss: tensor(2.2765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 19 \tLoss: tensor(2.2745, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 20 \tLoss: tensor(2.2691, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 21 \tLoss: tensor(2.2675, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 22 \tLoss: tensor(2.2627, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 23 \tLoss: tensor(2.2631, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 24 \tLoss: tensor(2.2596, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 25 \tLoss: tensor(2.2641, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 26 \tLoss: tensor(2.2527, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 27 \tLoss: tensor(2.2545, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 28 \tLoss: tensor(2.2488, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 29 \tLoss: tensor(2.2506, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 30 \tLoss: tensor(2.2345, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 31 \tLoss: tensor(2.2249, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 32 \tLoss: tensor(2.2252, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 33 \tLoss: tensor(2.2154, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 34 \tLoss: tensor(2.2086, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 35 \tLoss: tensor(2.2067, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 36 \tLoss: tensor(2.2016, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 37 \tLoss: tensor(2.1859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 38 \tLoss: tensor(2.1694, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 39 \tLoss: tensor(2.1691, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 40 \tLoss: tensor(2.1633, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 41 \tLoss: tensor(2.1665, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 42 \tLoss: tensor(2.1477, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 43 \tLoss: tensor(2.1474, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 44 \tLoss: tensor(2.1388, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 45 \tLoss: tensor(2.1275, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 46 \tLoss: tensor(2.1045, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 47 \tLoss: tensor(2.1137, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 48 \tLoss: tensor(2.0993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 49 \tLoss: tensor(2.1065, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 50 \tLoss: tensor(2.0717, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 51 \tLoss: tensor(2.0729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 52 \tLoss: tensor(2.0477, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 53 \tLoss: tensor(2.0532, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 54 \tLoss: tensor(2.0471, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 55 \tLoss: tensor(2.0297, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 56 \tLoss: tensor(2.0254, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 57 \tLoss: tensor(2.0309, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 58 \tLoss: tensor(1.9944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 59 \tLoss: tensor(2.0109, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 60 \tLoss: tensor(1.9705, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 61 \tLoss: tensor(1.9941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 62 \tLoss: tensor(1.9691, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 63 \tLoss: tensor(1.9461, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 64 \tLoss: tensor(1.9446, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 65 \tLoss: tensor(1.9738, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 66 \tLoss: tensor(1.9426, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 67 \tLoss: tensor(1.9438, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 68 \tLoss: tensor(1.9227, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 69 \tLoss: tensor(1.9306, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 70 \tLoss: tensor(1.8824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 71 \tLoss: tensor(1.8689, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 72 \tLoss: tensor(1.9028, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 73 \tLoss: tensor(1.9106, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 74 \tLoss: tensor(1.8575, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 75 \tLoss: tensor(1.9202, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 76 \tLoss: tensor(1.8722, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 77 \tLoss: tensor(1.8634, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 78 \tLoss: tensor(1.9010, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 79 \tLoss: tensor(1.9293, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 80 \tLoss: tensor(1.8951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 81 \tLoss: tensor(1.8748, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 82 \tLoss: tensor(1.8602, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 83 \tLoss: tensor(1.8503, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 84 \tLoss: tensor(1.8678, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 85 \tLoss: tensor(1.8329, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 86 \tLoss: tensor(1.8483, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 87 \tLoss: tensor(1.8536, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 88 \tLoss: tensor(1.8601, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 89 \tLoss: tensor(1.7765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 90 \tLoss: tensor(1.8103, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 91 \tLoss: tensor(1.8224, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 92 \tLoss: tensor(1.8648, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 93 \tLoss: tensor(1.8102, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 94 \tLoss: tensor(1.8379, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 95 \tLoss: tensor(1.8151, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 96 \tLoss: tensor(1.7596, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 97 \tLoss: tensor(1.8203, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 98 \tLoss: tensor(1.7954, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 99 \tLoss: tensor(1.7982, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 100 \tLoss: tensor(1.8234, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 101 \tLoss: tensor(1.7914, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 102 \tLoss: tensor(1.7863, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 103 \tLoss: tensor(1.7693, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 104 \tLoss: tensor(1.7999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 105 \tLoss: tensor(1.7894, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 106 \tLoss: tensor(1.7797, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 107 \tLoss: tensor(1.8093, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 108 \tLoss: tensor(1.7987, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 109 \tLoss: tensor(1.7531, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 110 \tLoss: tensor(1.7888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 111 \tLoss: tensor(1.8063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 112 \tLoss: tensor(1.7787, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 113 \tLoss: tensor(1.7435, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 114 \tLoss: tensor(1.8171, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 115 \tLoss: tensor(1.7671, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 116 \tLoss: tensor(1.7501, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 117 \tLoss: tensor(1.7911, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 118 \tLoss: tensor(1.7641, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 119 \tLoss: tensor(1.7646, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 120 \tLoss: tensor(1.7736, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 121 \tLoss: tensor(1.8058, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 122 \tLoss: tensor(1.7548, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 123 \tLoss: tensor(1.7686, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 124 \tLoss: tensor(1.7435, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 125 \tLoss: tensor(1.7681, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 126 \tLoss: tensor(1.7573, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 127 \tLoss: tensor(1.7509, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 128 \tLoss: tensor(1.7350, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 129 \tLoss: tensor(1.7267, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 130 \tLoss: tensor(1.7699, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 131 \tLoss: tensor(1.7198, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 132 \tLoss: tensor(1.7153, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 133 \tLoss: tensor(1.7384, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 134 \tLoss: tensor(1.7064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 135 \tLoss: tensor(1.6775, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 136 \tLoss: tensor(1.7664, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 137 \tLoss: tensor(1.6976, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 138 \tLoss: tensor(1.6870, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 139 \tLoss: tensor(1.7332, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 140 \tLoss: tensor(1.7122, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 141 \tLoss: tensor(1.7052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 142 \tLoss: tensor(1.6792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 143 \tLoss: tensor(1.7569, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 144 \tLoss: tensor(1.7040, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 145 \tLoss: tensor(1.7212, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 146 \tLoss: tensor(1.7177, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 147 \tLoss: tensor(1.6903, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 148 \tLoss: tensor(1.6769, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 149 \tLoss: tensor(1.6878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 150 \tLoss: tensor(1.7037, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 151 \tLoss: tensor(1.7053, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 152 \tLoss: tensor(1.7157, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 153 \tLoss: tensor(1.7138, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 154 \tLoss: tensor(1.6981, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 155 \tLoss: tensor(1.7004, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 156 \tLoss: tensor(1.6818, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 157 \tLoss: tensor(1.7015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 158 \tLoss: tensor(1.6704, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 159 \tLoss: tensor(1.6945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 160 \tLoss: tensor(1.7069, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 161 \tLoss: tensor(1.6865, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 162 \tLoss: tensor(1.6840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 163 \tLoss: tensor(1.6621, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 164 \tLoss: tensor(1.6404, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 165 \tLoss: tensor(1.6501, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 166 \tLoss: tensor(1.6729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 167 \tLoss: tensor(1.6482, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 168 \tLoss: tensor(1.6629, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 169 \tLoss: tensor(1.6727, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 170 \tLoss: tensor(1.6921, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 171 \tLoss: tensor(1.6519, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 172 \tLoss: tensor(1.6513, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 173 \tLoss: tensor(1.6835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 174 \tLoss: tensor(1.6713, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 175 \tLoss: tensor(1.6652, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 176 \tLoss: tensor(1.6813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 177 \tLoss: tensor(1.6314, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 178 \tLoss: tensor(1.6523, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 179 \tLoss: tensor(1.6574, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 180 \tLoss: tensor(1.6668, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 181 \tLoss: tensor(1.6504, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 182 \tLoss: tensor(1.6630, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 183 \tLoss: tensor(1.6525, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 184 \tLoss: tensor(1.6186, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 185 \tLoss: tensor(1.6193, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 186 \tLoss: tensor(1.6429, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 187 \tLoss: tensor(1.6437, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 188 \tLoss: tensor(1.6453, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 189 \tLoss: tensor(1.6593, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 190 \tLoss: tensor(1.6430, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 191 \tLoss: tensor(1.6003, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 192 \tLoss: tensor(1.6233, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 193 \tLoss: tensor(1.6328, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 194 \tLoss: tensor(1.6182, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 195 \tLoss: tensor(1.6147, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 0 \tLoss: tensor(1.6254, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 1 \tLoss: tensor(1.6327, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 2 \tLoss: tensor(1.6612, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 3 \tLoss: tensor(1.6321, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 4 \tLoss: tensor(1.6104, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 5 \tLoss: tensor(1.6302, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 6 \tLoss: tensor(1.6208, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 7 \tLoss: tensor(1.6059, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 8 \tLoss: tensor(1.6459, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 9 \tLoss: tensor(1.6340, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 10 \tLoss: tensor(1.6064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 11 \tLoss: tensor(1.6122, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 12 \tLoss: tensor(1.6214, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 13 \tLoss: tensor(1.6294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 14 \tLoss: tensor(1.6253, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 15 \tLoss: tensor(1.5965, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 16 \tLoss: tensor(1.6312, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 17 \tLoss: tensor(1.6151, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 18 \tLoss: tensor(1.6406, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 19 \tLoss: tensor(1.6145, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 20 \tLoss: tensor(1.6093, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 21 \tLoss: tensor(1.6238, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 22 \tLoss: tensor(1.6237, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 23 \tLoss: tensor(1.6383, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 24 \tLoss: tensor(1.6173, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 25 \tLoss: tensor(1.6209, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 26 \tLoss: tensor(1.6205, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 27 \tLoss: tensor(1.6176, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 28 \tLoss: tensor(1.6155, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 29 \tLoss: tensor(1.6231, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 30 \tLoss: tensor(1.6075, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 31 \tLoss: tensor(1.6060, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 32 \tLoss: tensor(1.6090, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 33 \tLoss: tensor(1.6118, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 34 \tLoss: tensor(1.6138, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 35 \tLoss: tensor(1.6042, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 36 \tLoss: tensor(1.6265, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 37 \tLoss: tensor(1.5884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 38 \tLoss: tensor(1.6267, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 39 \tLoss: tensor(1.6048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 40 \tLoss: tensor(1.6325, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 41 \tLoss: tensor(1.6201, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 42 \tLoss: tensor(1.5726, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 43 \tLoss: tensor(1.5929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 44 \tLoss: tensor(1.6125, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 45 \tLoss: tensor(1.6056, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 46 \tLoss: tensor(1.6026, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 47 \tLoss: tensor(1.6117, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 48 \tLoss: tensor(1.5969, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 49 \tLoss: tensor(1.5981, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 50 \tLoss: tensor(1.5878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 51 \tLoss: tensor(1.6286, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 52 \tLoss: tensor(1.5991, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 53 \tLoss: tensor(1.6057, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 54 \tLoss: tensor(1.5865, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 55 \tLoss: tensor(1.6172, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 56 \tLoss: tensor(1.5865, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 57 \tLoss: tensor(1.5975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 58 \tLoss: tensor(1.6041, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 59 \tLoss: tensor(1.5929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 60 \tLoss: tensor(1.5796, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 61 \tLoss: tensor(1.6156, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 62 \tLoss: tensor(1.6112, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 63 \tLoss: tensor(1.6067, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 64 \tLoss: tensor(1.5984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 65 \tLoss: tensor(1.5860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 66 \tLoss: tensor(1.6007, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 67 \tLoss: tensor(1.5669, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 68 \tLoss: tensor(1.5761, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 69 \tLoss: tensor(1.5751, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 70 \tLoss: tensor(1.6098, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 71 \tLoss: tensor(1.5844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 72 \tLoss: tensor(1.6234, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 73 \tLoss: tensor(1.5898, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 74 \tLoss: tensor(1.6000, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 75 \tLoss: tensor(1.6064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 76 \tLoss: tensor(1.5907, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 77 \tLoss: tensor(1.5901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 78 \tLoss: tensor(1.6042, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 79 \tLoss: tensor(1.5760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 80 \tLoss: tensor(1.6193, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 81 \tLoss: tensor(1.6187, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 82 \tLoss: tensor(1.5599, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 83 \tLoss: tensor(1.5592, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 84 \tLoss: tensor(1.6270, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 85 \tLoss: tensor(1.5933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 86 \tLoss: tensor(1.5655, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 87 \tLoss: tensor(1.6179, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 88 \tLoss: tensor(1.5792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 89 \tLoss: tensor(1.5868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 90 \tLoss: tensor(1.5625, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 91 \tLoss: tensor(1.5910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 92 \tLoss: tensor(1.6074, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 93 \tLoss: tensor(1.5642, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 94 \tLoss: tensor(1.5728, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 95 \tLoss: tensor(1.5968, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 96 \tLoss: tensor(1.5778, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 97 \tLoss: tensor(1.6178, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 98 \tLoss: tensor(1.5843, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 99 \tLoss: tensor(1.5961, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 100 \tLoss: tensor(1.5601, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 101 \tLoss: tensor(1.5778, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 102 \tLoss: tensor(1.5798, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 103 \tLoss: tensor(1.5676, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 104 \tLoss: tensor(1.5848, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 105 \tLoss: tensor(1.6064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 106 \tLoss: tensor(1.6057, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 107 \tLoss: tensor(1.5784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 108 \tLoss: tensor(1.5955, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 109 \tLoss: tensor(1.5796, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 110 \tLoss: tensor(1.6114, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 111 \tLoss: tensor(1.5642, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 112 \tLoss: tensor(1.5697, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 113 \tLoss: tensor(1.5937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 114 \tLoss: tensor(1.5816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 115 \tLoss: tensor(1.5821, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 116 \tLoss: tensor(1.5826, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 117 \tLoss: tensor(1.5781, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 118 \tLoss: tensor(1.5794, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 119 \tLoss: tensor(1.5976, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 120 \tLoss: tensor(1.5909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 121 \tLoss: tensor(1.5732, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 122 \tLoss: tensor(1.5682, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 123 \tLoss: tensor(1.5799, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 124 \tLoss: tensor(1.5808, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 125 \tLoss: tensor(1.5935, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 126 \tLoss: tensor(1.5670, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 127 \tLoss: tensor(1.5677, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 128 \tLoss: tensor(1.5918, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 129 \tLoss: tensor(1.5768, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 130 \tLoss: tensor(1.6005, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 131 \tLoss: tensor(1.5802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 132 \tLoss: tensor(1.5856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 133 \tLoss: tensor(1.6219, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 134 \tLoss: tensor(1.5723, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 135 \tLoss: tensor(1.5441, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 136 \tLoss: tensor(1.5772, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 137 \tLoss: tensor(1.5893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 138 \tLoss: tensor(1.5763, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 139 \tLoss: tensor(1.5691, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 140 \tLoss: tensor(1.5797, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 141 \tLoss: tensor(1.5972, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 142 \tLoss: tensor(1.5710, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 143 \tLoss: tensor(1.5831, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 144 \tLoss: tensor(1.5740, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 145 \tLoss: tensor(1.5681, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 146 \tLoss: tensor(1.5537, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 147 \tLoss: tensor(1.5793, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 148 \tLoss: tensor(1.5757, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 149 \tLoss: tensor(1.5592, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 150 \tLoss: tensor(1.5747, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 151 \tLoss: tensor(1.5729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 152 \tLoss: tensor(1.6076, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 153 \tLoss: tensor(1.5792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 154 \tLoss: tensor(1.5723, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 155 \tLoss: tensor(1.5956, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 156 \tLoss: tensor(1.5872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 157 \tLoss: tensor(1.5685, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 158 \tLoss: tensor(1.5951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 159 \tLoss: tensor(1.5984, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 160 \tLoss: tensor(1.5651, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 161 \tLoss: tensor(1.5874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 162 \tLoss: tensor(1.5763, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 163 \tLoss: tensor(1.5699, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 164 \tLoss: tensor(1.5762, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 165 \tLoss: tensor(1.5566, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 166 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 167 \tLoss: tensor(1.5776, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 168 \tLoss: tensor(1.5780, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 169 \tLoss: tensor(1.5788, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 170 \tLoss: tensor(1.5461, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 171 \tLoss: tensor(1.5542, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 172 \tLoss: tensor(1.5914, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 173 \tLoss: tensor(1.5886, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 174 \tLoss: tensor(1.5668, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 175 \tLoss: tensor(1.5554, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 176 \tLoss: tensor(1.5589, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 177 \tLoss: tensor(1.5842, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 178 \tLoss: tensor(1.5597, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 179 \tLoss: tensor(1.5928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 180 \tLoss: tensor(1.5495, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 181 \tLoss: tensor(1.5859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 182 \tLoss: tensor(1.5502, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 183 \tLoss: tensor(1.5782, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 184 \tLoss: tensor(1.5846, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 185 \tLoss: tensor(1.5669, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 186 \tLoss: tensor(1.5849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 187 \tLoss: tensor(1.5631, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 188 \tLoss: tensor(1.5833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 189 \tLoss: tensor(1.5531, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 190 \tLoss: tensor(1.5874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 191 \tLoss: tensor(1.5819, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 192 \tLoss: tensor(1.5638, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 193 \tLoss: tensor(1.5514, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 194 \tLoss: tensor(1.5575, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 195 \tLoss: tensor(1.5321, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 0 \tLoss: tensor(1.5812, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 1 \tLoss: tensor(1.5700, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 2 \tLoss: tensor(1.5676, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 3 \tLoss: tensor(1.5457, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 4 \tLoss: tensor(1.5614, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 5 \tLoss: tensor(1.5580, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 6 \tLoss: tensor(1.5719, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 7 \tLoss: tensor(1.5650, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 8 \tLoss: tensor(1.5728, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 9 \tLoss: tensor(1.5790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 10 \tLoss: tensor(1.5709, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 11 \tLoss: tensor(1.5961, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 12 \tLoss: tensor(1.5727, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 13 \tLoss: tensor(1.5624, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 14 \tLoss: tensor(1.5605, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 15 \tLoss: tensor(1.5454, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 16 \tLoss: tensor(1.5517, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 17 \tLoss: tensor(1.5560, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 18 \tLoss: tensor(1.5525, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 19 \tLoss: tensor(1.5622, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 20 \tLoss: tensor(1.5720, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 21 \tLoss: tensor(1.5687, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 22 \tLoss: tensor(1.5871, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 23 \tLoss: tensor(1.5663, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 24 \tLoss: tensor(1.5828, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 25 \tLoss: tensor(1.5479, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 26 \tLoss: tensor(1.5551, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 27 \tLoss: tensor(1.5589, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 28 \tLoss: tensor(1.5798, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 29 \tLoss: tensor(1.6016, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 30 \tLoss: tensor(1.5824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 31 \tLoss: tensor(1.5546, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 32 \tLoss: tensor(1.5845, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 33 \tLoss: tensor(1.5990, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 34 \tLoss: tensor(1.5581, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 35 \tLoss: tensor(1.5486, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 36 \tLoss: tensor(1.5763, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 37 \tLoss: tensor(1.5646, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 38 \tLoss: tensor(1.5634, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 39 \tLoss: tensor(1.5411, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 40 \tLoss: tensor(1.5553, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 41 \tLoss: tensor(1.5657, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 42 \tLoss: tensor(1.5622, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 43 \tLoss: tensor(1.5861, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 44 \tLoss: tensor(1.5669, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 45 \tLoss: tensor(1.5564, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 46 \tLoss: tensor(1.5575, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 47 \tLoss: tensor(1.5555, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 48 \tLoss: tensor(1.5786, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 49 \tLoss: tensor(1.5535, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 50 \tLoss: tensor(1.5644, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 51 \tLoss: tensor(1.5697, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 52 \tLoss: tensor(1.5773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 53 \tLoss: tensor(1.5668, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 54 \tLoss: tensor(1.5667, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 55 \tLoss: tensor(1.5719, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 56 \tLoss: tensor(1.5673, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 57 \tLoss: tensor(1.5638, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 58 \tLoss: tensor(1.5711, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 59 \tLoss: tensor(1.5398, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 60 \tLoss: tensor(1.5666, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 61 \tLoss: tensor(1.5583, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 62 \tLoss: tensor(1.5610, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 63 \tLoss: tensor(1.5521, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 64 \tLoss: tensor(1.5403, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 65 \tLoss: tensor(1.5527, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 66 \tLoss: tensor(1.5731, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 67 \tLoss: tensor(1.5413, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 68 \tLoss: tensor(1.5662, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 69 \tLoss: tensor(1.5686, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 70 \tLoss: tensor(1.5573, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 71 \tLoss: tensor(1.5568, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 72 \tLoss: tensor(1.5616, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 73 \tLoss: tensor(1.5590, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 74 \tLoss: tensor(1.5834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 75 \tLoss: tensor(1.5713, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 76 \tLoss: tensor(1.5648, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 77 \tLoss: tensor(1.5623, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 78 \tLoss: tensor(1.5522, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 79 \tLoss: tensor(1.5746, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 80 \tLoss: tensor(1.5732, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 81 \tLoss: tensor(1.5555, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 82 \tLoss: tensor(1.5619, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tBatch: 83 \tLoss: tensor(1.5542, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 84 \tLoss: tensor(1.5508, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 85 \tLoss: tensor(1.5699, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 86 \tLoss: tensor(1.5454, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 87 \tLoss: tensor(1.5842, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 88 \tLoss: tensor(1.5820, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 89 \tLoss: tensor(1.5707, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 90 \tLoss: tensor(1.5438, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 91 \tLoss: tensor(1.5476, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 92 \tLoss: tensor(1.5567, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 93 \tLoss: tensor(1.5603, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 94 \tLoss: tensor(1.5539, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 95 \tLoss: tensor(1.5445, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 96 \tLoss: tensor(1.5318, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 97 \tLoss: tensor(1.5683, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 98 \tLoss: tensor(1.5600, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 99 \tLoss: tensor(1.5519, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 100 \tLoss: tensor(1.5584, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 101 \tLoss: tensor(1.5729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 102 \tLoss: tensor(1.5671, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 103 \tLoss: tensor(1.5479, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 104 \tLoss: tensor(1.5689, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 105 \tLoss: tensor(1.5635, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 106 \tLoss: tensor(1.5535, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 107 \tLoss: tensor(1.5644, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 108 \tLoss: tensor(1.5246, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 109 \tLoss: tensor(1.5500, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 110 \tLoss: tensor(1.5608, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 111 \tLoss: tensor(1.5543, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 112 \tLoss: tensor(1.5603, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 113 \tLoss: tensor(1.5718, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 114 \tLoss: tensor(1.5613, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 115 \tLoss: tensor(1.5742, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 116 \tLoss: tensor(1.5423, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 117 \tLoss: tensor(1.5509, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 118 \tLoss: tensor(1.5667, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 119 \tLoss: tensor(1.5543, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 120 \tLoss: tensor(1.5526, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 121 \tLoss: tensor(1.5501, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 122 \tLoss: tensor(1.5809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 123 \tLoss: tensor(1.5488, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 124 \tLoss: tensor(1.5455, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 125 \tLoss: tensor(1.5618, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 126 \tLoss: tensor(1.5756, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 127 \tLoss: tensor(1.5694, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 128 \tLoss: tensor(1.5447, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 129 \tLoss: tensor(1.5670, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 130 \tLoss: tensor(1.5727, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 131 \tLoss: tensor(1.5716, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 132 \tLoss: tensor(1.5416, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 133 \tLoss: tensor(1.5689, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 134 \tLoss: tensor(1.5405, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 135 \tLoss: tensor(1.5576, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 136 \tLoss: tensor(1.5771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 137 \tLoss: tensor(1.5637, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 138 \tLoss: tensor(1.5549, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 139 \tLoss: tensor(1.5524, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 140 \tLoss: tensor(1.5269, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 141 \tLoss: tensor(1.5396, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 142 \tLoss: tensor(1.5519, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 143 \tLoss: tensor(1.5670, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 144 \tLoss: tensor(1.5754, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 145 \tLoss: tensor(1.5494, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 146 \tLoss: tensor(1.5443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 147 \tLoss: tensor(1.5613, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 148 \tLoss: tensor(1.5900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 149 \tLoss: tensor(1.5390, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 150 \tLoss: tensor(1.5647, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 151 \tLoss: tensor(1.5463, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 152 \tLoss: tensor(1.5678, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 153 \tLoss: tensor(1.5793, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 154 \tLoss: tensor(1.5705, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 155 \tLoss: tensor(1.5759, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 156 \tLoss: tensor(1.5580, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 157 \tLoss: tensor(1.5447, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 158 \tLoss: tensor(1.5665, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 159 \tLoss: tensor(1.5374, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 160 \tLoss: tensor(1.5648, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 161 \tLoss: tensor(1.5550, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 162 \tLoss: tensor(1.5747, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 163 \tLoss: tensor(1.5718, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 164 \tLoss: tensor(1.5442, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 165 \tLoss: tensor(1.5621, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 166 \tLoss: tensor(1.5513, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 167 \tLoss: tensor(1.5229, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 168 \tLoss: tensor(1.5553, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 169 \tLoss: tensor(1.5469, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 170 \tLoss: tensor(1.5460, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 171 \tLoss: tensor(1.5426, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 172 \tLoss: tensor(1.5619, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 173 \tLoss: tensor(1.5411, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 174 \tLoss: tensor(1.5444, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 175 \tLoss: tensor(1.5677, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 176 \tLoss: tensor(1.5504, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 177 \tLoss: tensor(1.5358, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 178 \tLoss: tensor(1.5305, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 179 \tLoss: tensor(1.5521, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 180 \tLoss: tensor(1.5593, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 181 \tLoss: tensor(1.5673, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 182 \tLoss: tensor(1.5464, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 183 \tLoss: tensor(1.5393, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 184 \tLoss: tensor(1.5384, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 185 \tLoss: tensor(1.5409, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 186 \tLoss: tensor(1.5230, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 187 \tLoss: tensor(1.5810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 188 \tLoss: tensor(1.5610, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 189 \tLoss: tensor(1.5468, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 190 \tLoss: tensor(1.5699, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 191 \tLoss: tensor(1.5576, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 192 \tLoss: tensor(1.5738, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 193 \tLoss: tensor(1.5554, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 194 \tLoss: tensor(1.5567, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 195 \tLoss: tensor(1.5739, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 0 \tLoss: tensor(1.5700, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 1 \tLoss: tensor(1.5486, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 2 \tLoss: tensor(1.5645, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 3 \tLoss: tensor(1.5723, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 4 \tLoss: tensor(1.5226, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 5 \tLoss: tensor(1.5643, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 6 \tLoss: tensor(1.5551, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 7 \tLoss: tensor(1.5542, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 8 \tLoss: tensor(1.5318, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 9 \tLoss: tensor(1.5496, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 10 \tLoss: tensor(1.5416, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 11 \tLoss: tensor(1.5529, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 12 \tLoss: tensor(1.5634, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 13 \tLoss: tensor(1.5461, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 14 \tLoss: tensor(1.5640, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 15 \tLoss: tensor(1.5402, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 16 \tLoss: tensor(1.5705, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 17 \tLoss: tensor(1.5383, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 18 \tLoss: tensor(1.5448, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 19 \tLoss: tensor(1.5566, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 20 \tLoss: tensor(1.5616, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 21 \tLoss: tensor(1.5528, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 22 \tLoss: tensor(1.5355, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 23 \tLoss: tensor(1.5323, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 24 \tLoss: tensor(1.5493, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 25 \tLoss: tensor(1.5854, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 26 \tLoss: tensor(1.5319, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 27 \tLoss: tensor(1.5421, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 28 \tLoss: tensor(1.5518, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 29 \tLoss: tensor(1.5542, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 30 \tLoss: tensor(1.5404, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 31 \tLoss: tensor(1.5201, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 32 \tLoss: tensor(1.5519, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 33 \tLoss: tensor(1.5497, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 34 \tLoss: tensor(1.5771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 35 \tLoss: tensor(1.5526, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 36 \tLoss: tensor(1.5587, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 37 \tLoss: tensor(1.5688, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 38 \tLoss: tensor(1.5395, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 39 \tLoss: tensor(1.5283, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 40 \tLoss: tensor(1.5718, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 41 \tLoss: tensor(1.5345, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 42 \tLoss: tensor(1.5574, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 43 \tLoss: tensor(1.5480, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 44 \tLoss: tensor(1.5659, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 45 \tLoss: tensor(1.5438, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 46 \tLoss: tensor(1.5429, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 47 \tLoss: tensor(1.5668, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 48 \tLoss: tensor(1.5326, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 49 \tLoss: tensor(1.5568, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 50 \tLoss: tensor(1.5443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 51 \tLoss: tensor(1.5326, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 52 \tLoss: tensor(1.5590, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 53 \tLoss: tensor(1.5694, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 54 \tLoss: tensor(1.5296, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 55 \tLoss: tensor(1.5432, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 56 \tLoss: tensor(1.5450, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 57 \tLoss: tensor(1.5343, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 58 \tLoss: tensor(1.5556, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 59 \tLoss: tensor(1.5597, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 60 \tLoss: tensor(1.5536, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 61 \tLoss: tensor(1.5359, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 62 \tLoss: tensor(1.5500, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 63 \tLoss: tensor(1.5488, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 64 \tLoss: tensor(1.5567, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 65 \tLoss: tensor(1.5720, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 66 \tLoss: tensor(1.5414, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 67 \tLoss: tensor(1.5342, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 68 \tLoss: tensor(1.5516, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 69 \tLoss: tensor(1.5545, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 70 \tLoss: tensor(1.5613, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 71 \tLoss: tensor(1.5465, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 72 \tLoss: tensor(1.5495, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 73 \tLoss: tensor(1.5501, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 74 \tLoss: tensor(1.5443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 75 \tLoss: tensor(1.5438, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 76 \tLoss: tensor(1.5669, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 77 \tLoss: tensor(1.5323, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 78 \tLoss: tensor(1.5539, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 79 \tLoss: tensor(1.5370, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 80 \tLoss: tensor(1.5368, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 81 \tLoss: tensor(1.5413, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 82 \tLoss: tensor(1.5417, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 83 \tLoss: tensor(1.5520, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 84 \tLoss: tensor(1.5407, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 85 \tLoss: tensor(1.5394, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 86 \tLoss: tensor(1.5924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 87 \tLoss: tensor(1.5692, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 88 \tLoss: tensor(1.5345, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 89 \tLoss: tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 90 \tLoss: tensor(1.5536, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 91 \tLoss: tensor(1.5709, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 92 \tLoss: tensor(1.5492, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 93 \tLoss: tensor(1.5743, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 94 \tLoss: tensor(1.5361, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 95 \tLoss: tensor(1.5598, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 96 \tLoss: tensor(1.5515, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 97 \tLoss: tensor(1.5464, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 98 \tLoss: tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 99 \tLoss: tensor(1.5305, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 100 \tLoss: tensor(1.5677, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 101 \tLoss: tensor(1.5349, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 102 \tLoss: tensor(1.5315, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 103 \tLoss: tensor(1.5500, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 104 \tLoss: tensor(1.5462, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 105 \tLoss: tensor(1.5470, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 106 \tLoss: tensor(1.5312, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 107 \tLoss: tensor(1.5518, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 108 \tLoss: tensor(1.5733, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 109 \tLoss: tensor(1.5562, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 110 \tLoss: tensor(1.5534, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 111 \tLoss: tensor(1.5188, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 112 \tLoss: tensor(1.5468, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 113 \tLoss: tensor(1.5213, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 114 \tLoss: tensor(1.5605, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 115 \tLoss: tensor(1.5573, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 116 \tLoss: tensor(1.5268, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 117 \tLoss: tensor(1.5425, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 118 \tLoss: tensor(1.5472, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 119 \tLoss: tensor(1.5542, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 120 \tLoss: tensor(1.5533, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 121 \tLoss: tensor(1.5762, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 122 \tLoss: tensor(1.5390, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 123 \tLoss: tensor(1.5460, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 124 \tLoss: tensor(1.5597, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 125 \tLoss: tensor(1.5569, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 126 \tLoss: tensor(1.5445, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 127 \tLoss: tensor(1.5564, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 128 \tLoss: tensor(1.5403, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 129 \tLoss: tensor(1.5451, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 130 \tLoss: tensor(1.5709, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 131 \tLoss: tensor(1.5106, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 132 \tLoss: tensor(1.5302, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 133 \tLoss: tensor(1.5404, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 134 \tLoss: tensor(1.5215, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 135 \tLoss: tensor(1.5423, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 136 \tLoss: tensor(1.5393, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 137 \tLoss: tensor(1.5244, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 138 \tLoss: tensor(1.5755, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 139 \tLoss: tensor(1.5533, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 140 \tLoss: tensor(1.5334, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 141 \tLoss: tensor(1.5238, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 142 \tLoss: tensor(1.5565, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 143 \tLoss: tensor(1.5884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 144 \tLoss: tensor(1.5440, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 145 \tLoss: tensor(1.5166, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 146 \tLoss: tensor(1.5351, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 147 \tLoss: tensor(1.5384, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 148 \tLoss: tensor(1.5419, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 149 \tLoss: tensor(1.5297, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 150 \tLoss: tensor(1.5643, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 151 \tLoss: tensor(1.5252, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 152 \tLoss: tensor(1.5390, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 153 \tLoss: tensor(1.5342, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 154 \tLoss: tensor(1.5600, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 155 \tLoss: tensor(1.5505, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 156 \tLoss: tensor(1.5511, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 157 \tLoss: tensor(1.5658, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 158 \tLoss: tensor(1.5360, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 159 \tLoss: tensor(1.5496, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 160 \tLoss: tensor(1.5545, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 161 \tLoss: tensor(1.5390, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 162 \tLoss: tensor(1.5363, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 163 \tLoss: tensor(1.5431, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 164 \tLoss: tensor(1.5339, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 165 \tLoss: tensor(1.5478, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 166 \tLoss: tensor(1.5238, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 167 \tLoss: tensor(1.5574, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 168 \tLoss: tensor(1.5443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 169 \tLoss: tensor(1.5390, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 170 \tLoss: tensor(1.5514, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 171 \tLoss: tensor(1.5411, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 172 \tLoss: tensor(1.5314, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 173 \tLoss: tensor(1.5424, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 174 \tLoss: tensor(1.5477, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 175 \tLoss: tensor(1.5479, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 176 \tLoss: tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 177 \tLoss: tensor(1.5535, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 178 \tLoss: tensor(1.5272, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 179 \tLoss: tensor(1.5350, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 180 \tLoss: tensor(1.5392, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 181 \tLoss: tensor(1.5449, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 182 \tLoss: tensor(1.5481, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 183 \tLoss: tensor(1.5358, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 184 \tLoss: tensor(1.5713, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 185 \tLoss: tensor(1.5275, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 186 \tLoss: tensor(1.5580, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 187 \tLoss: tensor(1.5472, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 188 \tLoss: tensor(1.5023, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 189 \tLoss: tensor(1.5474, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 190 \tLoss: tensor(1.5416, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 191 \tLoss: tensor(1.5643, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 192 \tLoss: tensor(1.5440, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 193 \tLoss: tensor(1.5483, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 194 \tLoss: tensor(1.5258, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 195 \tLoss: tensor(1.5026, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 0 \tLoss: tensor(1.5340, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 1 \tLoss: tensor(1.5609, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 2 \tLoss: tensor(1.5387, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 3 \tLoss: tensor(1.5242, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 4 \tLoss: tensor(1.5193, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 5 \tLoss: tensor(1.5467, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 6 \tLoss: tensor(1.5290, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 7 \tLoss: tensor(1.5404, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 8 \tLoss: tensor(1.5342, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 9 \tLoss: tensor(1.5376, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 10 \tLoss: tensor(1.5524, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 11 \tLoss: tensor(1.5442, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 12 \tLoss: tensor(1.5305, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 13 \tLoss: tensor(1.5419, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 14 \tLoss: tensor(1.5314, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 15 \tLoss: tensor(1.5365, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 16 \tLoss: tensor(1.5184, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 17 \tLoss: tensor(1.5472, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 18 \tLoss: tensor(1.5675, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 19 \tLoss: tensor(1.5487, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 20 \tLoss: tensor(1.5435, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 21 \tLoss: tensor(1.5348, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 22 \tLoss: tensor(1.5412, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 23 \tLoss: tensor(1.5420, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 24 \tLoss: tensor(1.5452, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 25 \tLoss: tensor(1.5423, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 26 \tLoss: tensor(1.5536, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 27 \tLoss: tensor(1.5318, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 28 \tLoss: tensor(1.5472, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 29 \tLoss: tensor(1.5306, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 30 \tLoss: tensor(1.5536, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 31 \tLoss: tensor(1.5563, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 32 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 33 \tLoss: tensor(1.5459, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 34 \tLoss: tensor(1.5299, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 35 \tLoss: tensor(1.5348, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 36 \tLoss: tensor(1.5353, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 37 \tLoss: tensor(1.5438, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 38 \tLoss: tensor(1.5098, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 39 \tLoss: tensor(1.5358, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 40 \tLoss: tensor(1.5294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 41 \tLoss: tensor(1.5417, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 42 \tLoss: tensor(1.5693, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 43 \tLoss: tensor(1.5423, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 44 \tLoss: tensor(1.5272, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 45 \tLoss: tensor(1.5387, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 46 \tLoss: tensor(1.5360, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 47 \tLoss: tensor(1.5362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 48 \tLoss: tensor(1.5480, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 49 \tLoss: tensor(1.5254, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 50 \tLoss: tensor(1.5455, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 51 \tLoss: tensor(1.5444, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 52 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 53 \tLoss: tensor(1.5682, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 54 \tLoss: tensor(1.5390, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 55 \tLoss: tensor(1.5506, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 56 \tLoss: tensor(1.5608, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 57 \tLoss: tensor(1.5412, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 58 \tLoss: tensor(1.5519, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 59 \tLoss: tensor(1.5630, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 60 \tLoss: tensor(1.5182, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 61 \tLoss: tensor(1.5394, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 62 \tLoss: tensor(1.5489, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 63 \tLoss: tensor(1.5481, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 64 \tLoss: tensor(1.5400, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 65 \tLoss: tensor(1.5476, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 66 \tLoss: tensor(1.5055, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 67 \tLoss: tensor(1.5464, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 68 \tLoss: tensor(1.5276, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 69 \tLoss: tensor(1.5553, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 70 \tLoss: tensor(1.5314, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 71 \tLoss: tensor(1.5481, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 72 \tLoss: tensor(1.5695, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 73 \tLoss: tensor(1.5396, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 74 \tLoss: tensor(1.5505, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 75 \tLoss: tensor(1.5433, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 76 \tLoss: tensor(1.5619, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 77 \tLoss: tensor(1.5347, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 78 \tLoss: tensor(1.5546, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 79 \tLoss: tensor(1.5415, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 80 \tLoss: tensor(1.5073, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 81 \tLoss: tensor(1.5319, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 82 \tLoss: tensor(1.5276, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 83 \tLoss: tensor(1.5310, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 84 \tLoss: tensor(1.5603, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 85 \tLoss: tensor(1.5456, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 86 \tLoss: tensor(1.5372, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 87 \tLoss: tensor(1.5495, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 88 \tLoss: tensor(1.5550, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 89 \tLoss: tensor(1.5296, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 90 \tLoss: tensor(1.5315, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 91 \tLoss: tensor(1.5237, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 92 \tLoss: tensor(1.5352, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 93 \tLoss: tensor(1.5418, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 94 \tLoss: tensor(1.5368, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 95 \tLoss: tensor(1.5481, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 96 \tLoss: tensor(1.5362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 97 \tLoss: tensor(1.5581, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 98 \tLoss: tensor(1.5316, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 99 \tLoss: tensor(1.5355, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 100 \tLoss: tensor(1.5288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 101 \tLoss: tensor(1.5015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 102 \tLoss: tensor(1.5405, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 103 \tLoss: tensor(1.5270, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 104 \tLoss: tensor(1.5655, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 105 \tLoss: tensor(1.5350, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 106 \tLoss: tensor(1.5585, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 107 \tLoss: tensor(1.5445, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 108 \tLoss: tensor(1.5136, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 109 \tLoss: tensor(1.5136, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 110 \tLoss: tensor(1.5663, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 111 \tLoss: tensor(1.5472, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 112 \tLoss: tensor(1.5072, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 113 \tLoss: tensor(1.5412, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 114 \tLoss: tensor(1.5420, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 115 \tLoss: tensor(1.5648, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 116 \tLoss: tensor(1.5509, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 117 \tLoss: tensor(1.5276, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 118 \tLoss: tensor(1.5268, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 119 \tLoss: tensor(1.5550, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 120 \tLoss: tensor(1.5258, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 121 \tLoss: tensor(1.5330, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 122 \tLoss: tensor(1.5456, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 123 \tLoss: tensor(1.5293, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 124 \tLoss: tensor(1.5188, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 125 \tLoss: tensor(1.5305, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 126 \tLoss: tensor(1.5394, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 127 \tLoss: tensor(1.5355, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 128 \tLoss: tensor(1.5566, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 129 \tLoss: tensor(1.5351, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 130 \tLoss: tensor(1.5459, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 131 \tLoss: tensor(1.5432, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 132 \tLoss: tensor(1.5221, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 133 \tLoss: tensor(1.5322, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 134 \tLoss: tensor(1.5452, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 135 \tLoss: tensor(1.5404, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 136 \tLoss: tensor(1.5652, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 137 \tLoss: tensor(1.5889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 138 \tLoss: tensor(1.5134, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 139 \tLoss: tensor(1.5265, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 140 \tLoss: tensor(1.5158, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 141 \tLoss: tensor(1.5584, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 142 \tLoss: tensor(1.5432, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 143 \tLoss: tensor(1.5339, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 144 \tLoss: tensor(1.5462, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 145 \tLoss: tensor(1.5178, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 146 \tLoss: tensor(1.5368, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 147 \tLoss: tensor(1.5507, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 148 \tLoss: tensor(1.5441, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 149 \tLoss: tensor(1.5152, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 150 \tLoss: tensor(1.5446, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 151 \tLoss: tensor(1.5236, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 152 \tLoss: tensor(1.5133, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 153 \tLoss: tensor(1.5462, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 154 \tLoss: tensor(1.5407, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 155 \tLoss: tensor(1.5192, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 156 \tLoss: tensor(1.5194, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 157 \tLoss: tensor(1.5326, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 158 \tLoss: tensor(1.5464, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 159 \tLoss: tensor(1.5192, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 160 \tLoss: tensor(1.5324, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 161 \tLoss: tensor(1.5231, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 162 \tLoss: tensor(1.5154, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 163 \tLoss: tensor(1.5382, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 164 \tLoss: tensor(1.5188, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 165 \tLoss: tensor(1.5614, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 166 \tLoss: tensor(1.5547, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 167 \tLoss: tensor(1.5322, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 168 \tLoss: tensor(1.5522, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 169 \tLoss: tensor(1.5434, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 170 \tLoss: tensor(1.5430, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 171 \tLoss: tensor(1.5197, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 172 \tLoss: tensor(1.5182, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 173 \tLoss: tensor(1.5348, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 174 \tLoss: tensor(1.5293, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 175 \tLoss: tensor(1.5136, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 176 \tLoss: tensor(1.5626, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 177 \tLoss: tensor(1.5472, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 178 \tLoss: tensor(1.5387, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 179 \tLoss: tensor(1.5347, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 180 \tLoss: tensor(1.5466, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 181 \tLoss: tensor(1.5467, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 182 \tLoss: tensor(1.5324, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 183 \tLoss: tensor(1.5463, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 184 \tLoss: tensor(1.5260, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 185 \tLoss: tensor(1.5491, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 186 \tLoss: tensor(1.5230, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 187 \tLoss: tensor(1.5114, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 188 \tLoss: tensor(1.5280, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 189 \tLoss: tensor(1.5356, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 190 \tLoss: tensor(1.5551, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 191 \tLoss: tensor(1.5460, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 192 \tLoss: tensor(1.5413, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 193 \tLoss: tensor(1.5501, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 194 \tLoss: tensor(1.5436, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 195 \tLoss: tensor(1.5466, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 0 \tLoss: tensor(1.5273, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 1 \tLoss: tensor(1.5190, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 2 \tLoss: tensor(1.5278, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 3 \tLoss: tensor(1.5160, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 4 \tLoss: tensor(1.5433, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 5 \tLoss: tensor(1.5191, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 6 \tLoss: tensor(1.5537, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 7 \tLoss: tensor(1.5355, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 8 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 9 \tLoss: tensor(1.5062, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 10 \tLoss: tensor(1.5516, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 11 \tLoss: tensor(1.5020, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 12 \tLoss: tensor(1.5333, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 13 \tLoss: tensor(1.5089, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 14 \tLoss: tensor(1.5275, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 15 \tLoss: tensor(1.5440, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 16 \tLoss: tensor(1.5310, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 17 \tLoss: tensor(1.5508, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 18 \tLoss: tensor(1.5353, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 19 \tLoss: tensor(1.5262, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 20 \tLoss: tensor(1.5390, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 21 \tLoss: tensor(1.5284, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 22 \tLoss: tensor(1.5343, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 23 \tLoss: tensor(1.5315, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 24 \tLoss: tensor(1.5451, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 25 \tLoss: tensor(1.5191, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 26 \tLoss: tensor(1.5216, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 27 \tLoss: tensor(1.5291, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 28 \tLoss: tensor(1.5358, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 29 \tLoss: tensor(1.5445, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 30 \tLoss: tensor(1.5045, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 31 \tLoss: tensor(1.5410, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 32 \tLoss: tensor(1.5422, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 33 \tLoss: tensor(1.5267, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 34 \tLoss: tensor(1.5248, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 35 \tLoss: tensor(1.5357, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 36 \tLoss: tensor(1.5351, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 37 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 38 \tLoss: tensor(1.5448, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 39 \tLoss: tensor(1.5194, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 40 \tLoss: tensor(1.5370, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 41 \tLoss: tensor(1.5181, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 42 \tLoss: tensor(1.5367, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 43 \tLoss: tensor(1.5259, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 44 \tLoss: tensor(1.5379, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 45 \tLoss: tensor(1.5501, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 46 \tLoss: tensor(1.5070, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 47 \tLoss: tensor(1.5403, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 48 \tLoss: tensor(1.5495, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 49 \tLoss: tensor(1.5525, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 50 \tLoss: tensor(1.5520, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 51 \tLoss: tensor(1.5190, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 52 \tLoss: tensor(1.4986, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 53 \tLoss: tensor(1.5338, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 54 \tLoss: tensor(1.5217, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 55 \tLoss: tensor(1.5556, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 56 \tLoss: tensor(1.5225, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 57 \tLoss: tensor(1.5363, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 58 \tLoss: tensor(1.5296, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 59 \tLoss: tensor(1.5352, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 60 \tLoss: tensor(1.5300, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 61 \tLoss: tensor(1.5502, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 62 \tLoss: tensor(1.5277, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 63 \tLoss: tensor(1.5584, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 64 \tLoss: tensor(1.5126, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 65 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 66 \tLoss: tensor(1.5377, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 67 \tLoss: tensor(1.5711, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 68 \tLoss: tensor(1.5497, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 69 \tLoss: tensor(1.5388, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 70 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 71 \tLoss: tensor(1.5303, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 72 \tLoss: tensor(1.5108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 73 \tLoss: tensor(1.5678, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 74 \tLoss: tensor(1.5273, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 75 \tLoss: tensor(1.5499, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 76 \tLoss: tensor(1.5500, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 77 \tLoss: tensor(1.5270, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 78 \tLoss: tensor(1.5379, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 79 \tLoss: tensor(1.5298, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 80 \tLoss: tensor(1.5185, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 81 \tLoss: tensor(1.5305, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 82 \tLoss: tensor(1.5494, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 83 \tLoss: tensor(1.5424, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 84 \tLoss: tensor(1.5287, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 85 \tLoss: tensor(1.5187, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 86 \tLoss: tensor(1.5247, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 87 \tLoss: tensor(1.5334, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 88 \tLoss: tensor(1.5398, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 89 \tLoss: tensor(1.5383, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 90 \tLoss: tensor(1.5498, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 91 \tLoss: tensor(1.5269, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 92 \tLoss: tensor(1.5098, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 93 \tLoss: tensor(1.5237, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 94 \tLoss: tensor(1.5336, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 95 \tLoss: tensor(1.5319, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 96 \tLoss: tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 97 \tLoss: tensor(1.5371, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 98 \tLoss: tensor(1.5357, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 99 \tLoss: tensor(1.5330, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 100 \tLoss: tensor(1.5468, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 101 \tLoss: tensor(1.5294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 102 \tLoss: tensor(1.5110, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 103 \tLoss: tensor(1.5385, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 104 \tLoss: tensor(1.5400, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 105 \tLoss: tensor(1.5285, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 106 \tLoss: tensor(1.5226, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 107 \tLoss: tensor(1.5598, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tBatch: 108 \tLoss: tensor(1.5171, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 109 \tLoss: tensor(1.5344, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 110 \tLoss: tensor(1.5488, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 111 \tLoss: tensor(1.5362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 112 \tLoss: tensor(1.5528, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 113 \tLoss: tensor(1.5384, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 114 \tLoss: tensor(1.5393, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 115 \tLoss: tensor(1.5236, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 116 \tLoss: tensor(1.5193, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 117 \tLoss: tensor(1.5249, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 118 \tLoss: tensor(1.5244, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 119 \tLoss: tensor(1.5093, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 120 \tLoss: tensor(1.5362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 121 \tLoss: tensor(1.5257, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 122 \tLoss: tensor(1.5369, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 123 \tLoss: tensor(1.5451, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 124 \tLoss: tensor(1.5411, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 125 \tLoss: tensor(1.5276, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 126 \tLoss: tensor(1.5585, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 127 \tLoss: tensor(1.5310, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 128 \tLoss: tensor(1.5094, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 129 \tLoss: tensor(1.5310, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 130 \tLoss: tensor(1.5058, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 131 \tLoss: tensor(1.5288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 132 \tLoss: tensor(1.5213, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 133 \tLoss: tensor(1.5323, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 134 \tLoss: tensor(1.5325, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 135 \tLoss: tensor(1.5617, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 136 \tLoss: tensor(1.5244, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 137 \tLoss: tensor(1.5343, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 138 \tLoss: tensor(1.5201, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 139 \tLoss: tensor(1.5341, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 140 \tLoss: tensor(1.5414, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 141 \tLoss: tensor(1.5356, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 142 \tLoss: tensor(1.5373, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 143 \tLoss: tensor(1.5330, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 144 \tLoss: tensor(1.5168, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 145 \tLoss: tensor(1.5315, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 146 \tLoss: tensor(1.5202, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 147 \tLoss: tensor(1.5379, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 148 \tLoss: tensor(1.5211, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 149 \tLoss: tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 150 \tLoss: tensor(1.5106, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 151 \tLoss: tensor(1.5262, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 152 \tLoss: tensor(1.5548, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 153 \tLoss: tensor(1.5524, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 154 \tLoss: tensor(1.5216, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 155 \tLoss: tensor(1.5367, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 156 \tLoss: tensor(1.5064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 157 \tLoss: tensor(1.5450, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 158 \tLoss: tensor(1.5443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 159 \tLoss: tensor(1.5805, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 160 \tLoss: tensor(1.5366, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 161 \tLoss: tensor(1.5297, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 162 \tLoss: tensor(1.5411, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 163 \tLoss: tensor(1.5441, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 164 \tLoss: tensor(1.5385, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 165 \tLoss: tensor(1.5277, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 166 \tLoss: tensor(1.5144, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 167 \tLoss: tensor(1.5274, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 168 \tLoss: tensor(1.5368, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 169 \tLoss: tensor(1.5395, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 170 \tLoss: tensor(1.5331, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 171 \tLoss: tensor(1.5290, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 172 \tLoss: tensor(1.5528, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 173 \tLoss: tensor(1.5190, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 174 \tLoss: tensor(1.5446, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 175 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 176 \tLoss: tensor(1.5240, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 177 \tLoss: tensor(1.5317, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 178 \tLoss: tensor(1.5410, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 179 \tLoss: tensor(1.5122, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 180 \tLoss: tensor(1.5256, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 181 \tLoss: tensor(1.5206, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 182 \tLoss: tensor(1.5349, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 183 \tLoss: tensor(1.5419, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 184 \tLoss: tensor(1.5442, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 185 \tLoss: tensor(1.5262, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 186 \tLoss: tensor(1.5322, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 187 \tLoss: tensor(1.5137, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 188 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 189 \tLoss: tensor(1.5219, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 190 \tLoss: tensor(1.5656, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 191 \tLoss: tensor(1.5258, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 192 \tLoss: tensor(1.5306, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 193 \tLoss: tensor(1.5356, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 194 \tLoss: tensor(1.5415, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 195 \tLoss: tensor(1.5069, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 0 \tLoss: tensor(1.5456, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 1 \tLoss: tensor(1.5112, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 2 \tLoss: tensor(1.5313, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 3 \tLoss: tensor(1.5258, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 4 \tLoss: tensor(1.5448, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 5 \tLoss: tensor(1.5321, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 6 \tLoss: tensor(1.5138, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 7 \tLoss: tensor(1.5203, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 8 \tLoss: tensor(1.5244, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 9 \tLoss: tensor(1.5266, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 10 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 11 \tLoss: tensor(1.5426, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 12 \tLoss: tensor(1.5343, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 13 \tLoss: tensor(1.5470, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 14 \tLoss: tensor(1.5345, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 15 \tLoss: tensor(1.5226, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 16 \tLoss: tensor(1.5307, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 17 \tLoss: tensor(1.5320, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 18 \tLoss: tensor(1.5368, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 19 \tLoss: tensor(1.5038, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 20 \tLoss: tensor(1.5369, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 21 \tLoss: tensor(1.5222, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 22 \tLoss: tensor(1.5615, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 23 \tLoss: tensor(1.5321, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 24 \tLoss: tensor(1.5130, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 25 \tLoss: tensor(1.5341, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 26 \tLoss: tensor(1.5420, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 27 \tLoss: tensor(1.5207, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 28 \tLoss: tensor(1.5354, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 29 \tLoss: tensor(1.5259, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tBatch: 30 \tLoss: tensor(1.5140, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 31 \tLoss: tensor(1.5417, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 32 \tLoss: tensor(1.5126, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 33 \tLoss: tensor(1.5192, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 34 \tLoss: tensor(1.5056, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 35 \tLoss: tensor(1.5369, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 36 \tLoss: tensor(1.5215, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 37 \tLoss: tensor(1.5211, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 38 \tLoss: tensor(1.5541, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 39 \tLoss: tensor(1.5482, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 40 \tLoss: tensor(1.5497, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 41 \tLoss: tensor(1.5212, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 42 \tLoss: tensor(1.5191, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 43 \tLoss: tensor(1.5210, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 44 \tLoss: tensor(1.5314, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 45 \tLoss: tensor(1.5468, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 46 \tLoss: tensor(1.5136, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 47 \tLoss: tensor(1.5250, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 48 \tLoss: tensor(1.5178, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 49 \tLoss: tensor(1.5187, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 50 \tLoss: tensor(1.5102, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 51 \tLoss: tensor(1.5307, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 52 \tLoss: tensor(1.5520, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 53 \tLoss: tensor(1.5227, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 54 \tLoss: tensor(1.5320, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 55 \tLoss: tensor(1.5588, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 56 \tLoss: tensor(1.5279, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 57 \tLoss: tensor(1.5424, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 58 \tLoss: tensor(1.5479, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 59 \tLoss: tensor(1.5159, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 60 \tLoss: tensor(1.5474, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 61 \tLoss: tensor(1.5098, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 62 \tLoss: tensor(1.5317, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 63 \tLoss: tensor(1.5063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 64 \tLoss: tensor(1.5145, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 65 \tLoss: tensor(1.5149, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 66 \tLoss: tensor(1.5517, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 67 \tLoss: tensor(1.5297, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 68 \tLoss: tensor(1.5379, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 69 \tLoss: tensor(1.5424, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 70 \tLoss: tensor(1.5237, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 71 \tLoss: tensor(1.5398, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 72 \tLoss: tensor(1.5152, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 73 \tLoss: tensor(1.5437, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 74 \tLoss: tensor(1.5349, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 75 \tLoss: tensor(1.5309, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 76 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 77 \tLoss: tensor(1.4980, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 78 \tLoss: tensor(1.5240, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 79 \tLoss: tensor(1.5221, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 80 \tLoss: tensor(1.5089, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 81 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 82 \tLoss: tensor(1.5144, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 83 \tLoss: tensor(1.5332, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 84 \tLoss: tensor(1.5022, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 85 \tLoss: tensor(1.5503, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 86 \tLoss: tensor(1.5132, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 87 \tLoss: tensor(1.5055, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 88 \tLoss: tensor(1.5223, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 89 \tLoss: tensor(1.5503, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 90 \tLoss: tensor(1.5297, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 91 \tLoss: tensor(1.5368, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 92 \tLoss: tensor(1.5465, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 93 \tLoss: tensor(1.5168, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 94 \tLoss: tensor(1.5396, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 95 \tLoss: tensor(1.5269, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 96 \tLoss: tensor(1.5399, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 97 \tLoss: tensor(1.5293, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 98 \tLoss: tensor(1.5209, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 99 \tLoss: tensor(1.5209, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 100 \tLoss: tensor(1.5037, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 101 \tLoss: tensor(1.5217, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 102 \tLoss: tensor(1.5209, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 103 \tLoss: tensor(1.5393, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 104 \tLoss: tensor(1.5120, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 105 \tLoss: tensor(1.5109, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 106 \tLoss: tensor(1.5284, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 107 \tLoss: tensor(1.5392, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 108 \tLoss: tensor(1.5395, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 109 \tLoss: tensor(1.5199, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 110 \tLoss: tensor(1.5183, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 111 \tLoss: tensor(1.5284, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 112 \tLoss: tensor(1.5143, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 113 \tLoss: tensor(1.5121, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 114 \tLoss: tensor(1.5106, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 115 \tLoss: tensor(1.5435, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 116 \tLoss: tensor(1.5049, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 117 \tLoss: tensor(1.5402, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 118 \tLoss: tensor(1.5164, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 119 \tLoss: tensor(1.5302, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 120 \tLoss: tensor(1.5376, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 121 \tLoss: tensor(1.5226, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 122 \tLoss: tensor(1.5392, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 123 \tLoss: tensor(1.5130, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 124 \tLoss: tensor(1.5443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 125 \tLoss: tensor(1.5419, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 126 \tLoss: tensor(1.5252, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 127 \tLoss: tensor(1.5315, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 128 \tLoss: tensor(1.5388, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 129 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 130 \tLoss: tensor(1.5288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 131 \tLoss: tensor(1.5128, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 132 \tLoss: tensor(1.5046, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 133 \tLoss: tensor(1.5174, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 134 \tLoss: tensor(1.5387, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 135 \tLoss: tensor(1.5222, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 136 \tLoss: tensor(1.5294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 137 \tLoss: tensor(1.5286, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 138 \tLoss: tensor(1.5144, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 139 \tLoss: tensor(1.5178, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 140 \tLoss: tensor(1.5147, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 141 \tLoss: tensor(1.5295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 142 \tLoss: tensor(1.5292, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 143 \tLoss: tensor(1.5339, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 144 \tLoss: tensor(1.5313, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 145 \tLoss: tensor(1.5264, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 146 \tLoss: tensor(1.5517, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 147 \tLoss: tensor(1.5025, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 148 \tLoss: tensor(1.5284, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tBatch: 149 \tLoss: tensor(1.5197, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 150 \tLoss: tensor(1.5053, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 151 \tLoss: tensor(1.5333, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 152 \tLoss: tensor(1.5130, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 153 \tLoss: tensor(1.5340, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 154 \tLoss: tensor(1.5122, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 155 \tLoss: tensor(1.5325, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 156 \tLoss: tensor(1.5366, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 157 \tLoss: tensor(1.5429, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 158 \tLoss: tensor(1.5269, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 159 \tLoss: tensor(1.5405, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 160 \tLoss: tensor(1.5218, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 161 \tLoss: tensor(1.5406, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 162 \tLoss: tensor(1.5558, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 163 \tLoss: tensor(1.5362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 164 \tLoss: tensor(1.5114, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 165 \tLoss: tensor(1.5363, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 166 \tLoss: tensor(1.5241, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 167 \tLoss: tensor(1.5213, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 168 \tLoss: tensor(1.5302, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 169 \tLoss: tensor(1.5330, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 170 \tLoss: tensor(1.5350, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 171 \tLoss: tensor(1.5318, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 172 \tLoss: tensor(1.5430, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 173 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 174 \tLoss: tensor(1.5274, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 175 \tLoss: tensor(1.5220, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 176 \tLoss: tensor(1.5218, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 177 \tLoss: tensor(1.5053, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 178 \tLoss: tensor(1.5373, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 179 \tLoss: tensor(1.5394, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 180 \tLoss: tensor(1.4989, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 181 \tLoss: tensor(1.5120, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 182 \tLoss: tensor(1.5458, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 183 \tLoss: tensor(1.5406, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 184 \tLoss: tensor(1.5126, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 185 \tLoss: tensor(1.5055, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 186 \tLoss: tensor(1.5379, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 187 \tLoss: tensor(1.5256, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 188 \tLoss: tensor(1.5208, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 189 \tLoss: tensor(1.5401, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 190 \tLoss: tensor(1.5297, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 191 \tLoss: tensor(1.5120, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 192 \tLoss: tensor(1.5235, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 193 \tLoss: tensor(1.5123, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 194 \tLoss: tensor(1.5330, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 195 \tLoss: tensor(1.5774, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 0 \tLoss: tensor(1.5132, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 1 \tLoss: tensor(1.5263, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 2 \tLoss: tensor(1.5106, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 3 \tLoss: tensor(1.5121, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 4 \tLoss: tensor(1.5294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 5 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 6 \tLoss: tensor(1.5226, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 7 \tLoss: tensor(1.5090, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 8 \tLoss: tensor(1.5178, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 9 \tLoss: tensor(1.5117, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 10 \tLoss: tensor(1.5251, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 11 \tLoss: tensor(1.5132, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 12 \tLoss: tensor(1.5285, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 13 \tLoss: tensor(1.5416, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 14 \tLoss: tensor(1.5263, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 15 \tLoss: tensor(1.5149, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 16 \tLoss: tensor(1.5226, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 17 \tLoss: tensor(1.5217, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 18 \tLoss: tensor(1.5162, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 19 \tLoss: tensor(1.5126, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 20 \tLoss: tensor(1.5362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 21 \tLoss: tensor(1.5276, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 22 \tLoss: tensor(1.5456, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 23 \tLoss: tensor(1.5228, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 24 \tLoss: tensor(1.5120, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 25 \tLoss: tensor(1.5265, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 26 \tLoss: tensor(1.5053, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 27 \tLoss: tensor(1.5059, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 28 \tLoss: tensor(1.5270, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 29 \tLoss: tensor(1.5446, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 30 \tLoss: tensor(1.5016, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 31 \tLoss: tensor(1.5233, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 32 \tLoss: tensor(1.5380, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 33 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 34 \tLoss: tensor(1.5196, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 35 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 36 \tLoss: tensor(1.5162, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 37 \tLoss: tensor(1.5374, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 38 \tLoss: tensor(1.5046, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 39 \tLoss: tensor(1.5203, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 40 \tLoss: tensor(1.5286, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 41 \tLoss: tensor(1.5267, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 42 \tLoss: tensor(1.5172, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 43 \tLoss: tensor(1.5486, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 44 \tLoss: tensor(1.5274, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 45 \tLoss: tensor(1.5415, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 46 \tLoss: tensor(1.5380, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 47 \tLoss: tensor(1.5188, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 48 \tLoss: tensor(1.5188, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 49 \tLoss: tensor(1.5163, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 50 \tLoss: tensor(1.5512, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 51 \tLoss: tensor(1.5301, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 52 \tLoss: tensor(1.5255, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 53 \tLoss: tensor(1.5215, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 54 \tLoss: tensor(1.5378, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 55 \tLoss: tensor(1.5141, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 56 \tLoss: tensor(1.5009, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 57 \tLoss: tensor(1.5358, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 58 \tLoss: tensor(1.5139, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 59 \tLoss: tensor(1.5377, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 60 \tLoss: tensor(1.5283, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 61 \tLoss: tensor(1.5289, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 62 \tLoss: tensor(1.5234, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 63 \tLoss: tensor(1.5294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 64 \tLoss: tensor(1.5267, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 65 \tLoss: tensor(1.5349, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 66 \tLoss: tensor(1.5314, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 67 \tLoss: tensor(1.5158, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 68 \tLoss: tensor(1.5261, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 69 \tLoss: tensor(1.5170, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 70 \tLoss: tensor(1.5260, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 71 \tLoss: tensor(1.5338, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 72 \tLoss: tensor(1.5354, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 73 \tLoss: tensor(1.4967, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tBatch: 74 \tLoss: tensor(1.5061, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 75 \tLoss: tensor(1.5064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 76 \tLoss: tensor(1.5196, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 77 \tLoss: tensor(1.5426, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 78 \tLoss: tensor(1.5228, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 79 \tLoss: tensor(1.5408, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 80 \tLoss: tensor(1.5022, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 81 \tLoss: tensor(1.5101, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 82 \tLoss: tensor(1.5215, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 83 \tLoss: tensor(1.5063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 84 \tLoss: tensor(1.5189, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 85 \tLoss: tensor(1.5174, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 86 \tLoss: tensor(1.5126, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 87 \tLoss: tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 88 \tLoss: tensor(1.5094, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 89 \tLoss: tensor(1.5333, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 90 \tLoss: tensor(1.5268, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 91 \tLoss: tensor(1.5206, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 92 \tLoss: tensor(1.5120, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 93 \tLoss: tensor(1.5111, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 94 \tLoss: tensor(1.5367, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 95 \tLoss: tensor(1.5246, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 96 \tLoss: tensor(1.5093, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 97 \tLoss: tensor(1.5409, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 98 \tLoss: tensor(1.5388, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 99 \tLoss: tensor(1.5158, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 100 \tLoss: tensor(1.5237, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 101 \tLoss: tensor(1.5176, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 102 \tLoss: tensor(1.5060, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 103 \tLoss: tensor(1.5175, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 104 \tLoss: tensor(1.5114, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 105 \tLoss: tensor(1.5102, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 106 \tLoss: tensor(1.5491, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 107 \tLoss: tensor(1.5221, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 108 \tLoss: tensor(1.5245, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 109 \tLoss: tensor(1.5200, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 110 \tLoss: tensor(1.5121, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 111 \tLoss: tensor(1.5074, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 112 \tLoss: tensor(1.5480, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 113 \tLoss: tensor(1.5279, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 114 \tLoss: tensor(1.5237, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 115 \tLoss: tensor(1.5136, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 116 \tLoss: tensor(1.5290, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 117 \tLoss: tensor(1.5064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 118 \tLoss: tensor(1.5225, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 119 \tLoss: tensor(1.5244, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 120 \tLoss: tensor(1.5328, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 121 \tLoss: tensor(1.5336, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 122 \tLoss: tensor(1.5313, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 123 \tLoss: tensor(1.5217, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 124 \tLoss: tensor(1.5193, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 125 \tLoss: tensor(1.5192, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 126 \tLoss: tensor(1.5131, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 127 \tLoss: tensor(1.5101, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 128 \tLoss: tensor(1.5241, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 129 \tLoss: tensor(1.5207, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 130 \tLoss: tensor(1.5381, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 131 \tLoss: tensor(1.5192, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 132 \tLoss: tensor(1.5128, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 133 \tLoss: tensor(1.5193, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 134 \tLoss: tensor(1.5196, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 135 \tLoss: tensor(1.5410, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 136 \tLoss: tensor(1.5237, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 137 \tLoss: tensor(1.5218, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 138 \tLoss: tensor(1.5371, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 139 \tLoss: tensor(1.5473, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 140 \tLoss: tensor(1.5150, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 141 \tLoss: tensor(1.5390, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 142 \tLoss: tensor(1.5268, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 143 \tLoss: tensor(1.5181, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 144 \tLoss: tensor(1.5072, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 145 \tLoss: tensor(1.5090, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 146 \tLoss: tensor(1.5223, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 147 \tLoss: tensor(1.5128, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 148 \tLoss: tensor(1.5263, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 149 \tLoss: tensor(1.5229, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 150 \tLoss: tensor(1.5280, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 151 \tLoss: tensor(1.5256, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 152 \tLoss: tensor(1.5476, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 153 \tLoss: tensor(1.5289, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 154 \tLoss: tensor(1.5176, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 155 \tLoss: tensor(1.5305, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 156 \tLoss: tensor(1.5420, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 157 \tLoss: tensor(1.5438, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 158 \tLoss: tensor(1.5110, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 159 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 160 \tLoss: tensor(1.5061, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 161 \tLoss: tensor(1.5165, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 162 \tLoss: tensor(1.5245, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 163 \tLoss: tensor(1.5046, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 164 \tLoss: tensor(1.5188, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 165 \tLoss: tensor(1.5272, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 166 \tLoss: tensor(1.5018, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 167 \tLoss: tensor(1.5115, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 168 \tLoss: tensor(1.5140, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 169 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 170 \tLoss: tensor(1.5218, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 171 \tLoss: tensor(1.5039, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 172 \tLoss: tensor(1.5273, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 173 \tLoss: tensor(1.5244, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 174 \tLoss: tensor(1.5279, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 175 \tLoss: tensor(1.5227, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 176 \tLoss: tensor(1.5253, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 177 \tLoss: tensor(1.5152, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 178 \tLoss: tensor(1.5229, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 179 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 180 \tLoss: tensor(1.5453, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 181 \tLoss: tensor(1.5187, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 182 \tLoss: tensor(1.5098, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 183 \tLoss: tensor(1.5303, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 184 \tLoss: tensor(1.5128, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 185 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 186 \tLoss: tensor(1.5231, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 187 \tLoss: tensor(1.5159, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 188 \tLoss: tensor(1.5338, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 189 \tLoss: tensor(1.5191, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 190 \tLoss: tensor(1.5237, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 191 \tLoss: tensor(1.5252, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 192 \tLoss: tensor(1.5159, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 193 \tLoss: tensor(1.5037, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 194 \tLoss: tensor(1.5288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 195 \tLoss: tensor(1.5106, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "myNeuralNetwork = NeuralNetworkClass()\n",
    "\n",
    "# CREATE OUR OPTIMISER\n",
    "optimiser = torch.optim.Adam(              # what optimiser should we use?\n",
    "    myNeuralNetwork.parameters(),          # what should it optimise?\n",
    "#     lr=learning_rate                       # using what learning rate?\n",
    ")\n",
    "\n",
    "class Adam():\n",
    "    def __init__(self, params, lr=0.1):\n",
    "        # init\n",
    "        self.params = params\n",
    "        \n",
    "    def step(self):\n",
    "        self.params +1 \n",
    "        \n",
    "    def get_next_update():\n",
    "        rerturn grad\n",
    "        \n",
    "myadam = Adam()\n",
    "myadam.step()\n",
    "the_grad = myadam.get_next_update()\n",
    "        \n",
    "        \n",
    "# CREATE OUR CRITERION\n",
    "criterion = torch.nn.CrossEntropyLoss()             # callable class that compares our predictions to our labels and returns our loss\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()                            # we will use this to show our models performance on a graph\n",
    "    \n",
    "# TRAINING LOOP\n",
    "def train(model, epochs):\n",
    "    model.train()                                  # put the model into training mode (more on this later)\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader):\n",
    "            inputs, labels = minibatch\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            writer.add_scalar('Loss/Train', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "            \n",
    "            \n",
    "train(myNeuralNetwork, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the loss actually mean practically?\n",
    "\n",
    "The absolute value of the loss doesn't really mean much, it's just a way of continuously evaluating the relative performance of the model whilst it trains. The real metric of performance that we care about is the proportion of ***unseen*** examples that our neural network can correctly classify. These unseen examples are what the test loader consists of.\n",
    "\n",
    "Let's write the code to calculate that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.39999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def test(model):\n",
    "    num_correct = 0\n",
    "    num_examples = len(test_data)                       # test DATA not test LOADER\n",
    "    for inputs, labels in test_loader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    print('Accuracy:', percent_correct)\n",
    "    \n",
    "test(myNeuralNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. Compare the loss curves generated by using different batch sizes. What's the best? As you change the batch size, what variable do you need to change to give those curves the same domain over the x-axis (num writes to summary writer)\n",
    "2. It would be good to validate our model as we go along to ensure that we don't overfit. Let's write a training loop that tests the loss on the validation set after each epoch. Plot the validation error alongside What can you see on the graphs that indicates overfitting?\n",
    "3. What is the best accuracy you can achieve? Can you implement a grid search and a random search to try them automatically. Record all permutations that you try.\n",
    "4. What feature of the input data is our standard neural network not taking advantage of? Hint: '************* neural networks' take this into account.\n",
    "\n",
    "## Congratulations you boss, you've finished the notebook!\n",
    "\n",
    "Please provide your feedback [here](https://docs.google.com/forms/d/e/1FAIpQLSdZSxvkAE19vjDN4jpp0VvUBPGr_wdtayGAcRNfFGH7e7jQDQ/viewform?usp=sf_link). It means a lot to us.\n",
    "\n",
    "Next, you might want to check out:\n",
    "- [Convolutional Neural Networks](https://github.com/AI-Core/Convolutional-Neural-Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
