{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to PyTorch\n",
    "\n",
    "Torch is a numerical computation library like Numpy but instead of arrays, it uses tensors. \n",
    "\n",
    "Tensors behave the same way as arrays but PyTorch has extra functionality under the hood which builds a computation graph as you perform calculations on the tensors and calculate gradients with one line of code.\n",
    "\n",
    "PyTorch was made and is maintained by Facebook.\n",
    "\n",
    "Below is a diagram of a computational graph. This one represents the computation of the loss (error) used in linear regression. Each node represents values (tensors usually), which are computed from all of, and only, the data nodes at the start of the incoming arcs (arrows). It displays the relationship between computed values, and crucially what data and operations they were computed from.\n",
    "\n",
    "![title](images/NN1_computation_graph.JPG)\n",
    "\n",
    "We could easily write functions in code to implement the mathematical functions that each node performs on its incoming data to compute it's values. Because we know what these functions are, we can also write functions to compute their rate of change (gradient) at any particular input values. That is, we can write functions that compute the derivative of the function.\n",
    "\n",
    "# grad img\n",
    "\n",
    "Thankfully, we don't have to write all of these functions and their derivatives that make up the layers of successive transformations in our model. That's what pytorch does.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   # for tensor math\n",
    "import torch    # our deep learning framework\n",
    "import torch.nn.functional as F    # contains functions like mean squared error that dont have parameters and can be called functionally\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_linear_data(m=20):\n",
    "    X = np.linspace(0, 1, 10)\n",
    "    Y = 2*X + 0.7\n",
    "    return X, Y #returns X (the input) which should have shape (m, 2) and Y (labels) which should have shape (m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556\n",
      " 0.66666667 0.77777778 0.88888889 1.        ] \n",
      "\n",
      "Y: [0.7        0.92222222 1.14444444 1.36666667 1.58888889 1.81111111\n",
      " 2.03333333 2.25555556 2.47777778 2.7       ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, Y = sample_linear_data()\n",
    "print('X:',X, '\\n')\n",
    "print('Y:',Y, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making models in PyTorch\n",
    "\n",
    "PyTorch makes it really easy for us to build complex models that can be improved via gradient based optimisation. It does this by providing a class named `torch.nn.Module`. Our model classes should inherit from this class because it does a few very useful things for us:\n",
    "\n",
    "1. `torch.nn.Module` keeps track of all `torch.nn.Parameters` that are created within it. So when we add a linear layer to our model, the parameters (matrix of weights) in that layer will be added to a list of our model's parameters. We can retrieve all parameters of our model using its `parameters()` method. We will later pass this (`mymodel.parameters()`) to our optimiser when we tell it that *this* is what it should be optimising.\n",
    "\n",
    "\n",
    "2. `torch.nn.Module` treats the `forward` method (function) of any child class specially by assigning it to the `__call__` method. That means that running `mymodel.forward(some_data)` is equal to `mymodel(some_data)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()    # initialise parent module\n",
    "        self.linear_layer = torch.nn.Linear(1, 1)    # we want a linear transformation that maps from a scalar X to a scalar Y\n",
    "     \n",
    "    def forward(self, x):\n",
    "        x = self.linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What data can a PyTorch model process?\n",
    "\n",
    "### What type should it be?\n",
    "Most models will have parameters that control what transformations are applied to the data they process. The parameters which are initialised in each layer of a PyTorch model are of type `torch.tensor`. Many of these transformations, like matrix multiplication between two tensors, won't work between data of different types.\n",
    "\n",
    "We have created our fake data as type `numpy.ndarray`. So let's convert it into a torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type((X)))\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In what format does a layer expect to recieve an input?\n",
    "\n",
    "PyTorch models expect to be fed data of shape `(batch_size, num_example_features)`. That is, it expects a of examples, and each example should also be a list. All of that is still required to be of type `torch tensor`. See more [here](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000],\n",
      "        [0.1111],\n",
      "        [0.2222],\n",
      "        [0.3333],\n",
      "        [0.4444],\n",
      "        [0.5556],\n",
      "        [0.6667],\n",
      "        [0.7778],\n",
      "        [0.8889],\n",
      "        [1.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "X = X.view(-1, 1)\n",
    "print(X)\n",
    "Y = Y.view(-1, 1)\n",
    "\n",
    "data = zip(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifically what kind of tensor?\n",
    "\n",
    "On a computer, numbers can be stored with varying levels of precision. By default, the model parameters are stored as 32 bit floating point numbers (called type `Float`). But our fake datapoints that were transformed into a torch tensor are all 64 but floating point (called type `Double`). So we need to convert either the model parameters or the data into the other type so that they match and we can perform computations between them. \n",
    "\n",
    "64 bit floats are twice as accurate as, but take up twice the memory of 32 bit floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = LinearModel().double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation in PyTorch and more about `torch.tensors`\n",
    "\n",
    "In the linear regression notebook, we wrote our own functions to compute the rate of change (gradients) or our model with respect to its parameters. We then used these gradients to iteratively improve the model through gradient descent (making small updates to the parameters in the direction that decreases the error).\n",
    "\n",
    "The gradient computing functions that we wrote in that notebook depended on the specific transformations (functions) that were applied on data passing through our model. If we changed the model, we would have to make sure that \n",
    "\n",
    "As illustrated in the computational graph, the relationship between our objective (loss) and the parameters which we want to optimise is defined by the layers of transformations that are used to compute the loss from these parameters.\n",
    "\n",
    "**The chain rule** shows us that we can compute a derivative of a function by chaining together the derivatives of the functions that connect those variables.\n",
    "\n",
    "# chain rule img\n",
    "\n",
    "By looking at our computational graph, we can see which functions connect any variables by following the arcs backward from the furthest point forward. PyTorch has many useful functions that can be used as layers in our models. Because PyTorch expects us to be differentiating (it is an automatic differentiation library), it also has associated gradient computing functions for each of the layers it provides. So by evaluating these gradient functions of the functions that relate two variables that we care about and multiplying them together, we can compute the derivative of one with respect to another. \n",
    "\n",
    "We can do this by calling the `.backward()` method on any tensorThis happens without us having to write the gradient functions, or perform their multiplication.\n",
    "\n",
    "### Computing the gradient automatically - `.backward()`\n",
    "\n",
    "\n",
    "### `.grad_fn`\n",
    "Every tensor that is created in PyTorch has an attribute called `grad_fn` (gradient function). This attribute specifies what operation was this value output by. Remember the nodes in our computation graph? Well the grad_fn is the function that was performed on the incoming values to compute the value at a particular node.\n",
    "\n",
    "# grad_fn img\n",
    "\n",
    "### `.requires_grad`\n",
    "We don't need to track the gradients of our loss with respect to some variables (like our input data or our learning rate for example).\n",
    "PyTorch tensors will only have a `grad_fn` if their **`requires_grad`** attribute is true.\n",
    "\n",
    "Let's play around with `grad_fn` and `requires_grad` now.\n",
    "\n",
    "### Where does a tenso's gradient get stored? `.grad`\n",
    "Not only do torch tensors store their current values, but they can also store computed gradients.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "requires_grad: False\n",
      "grad_fn: None\n",
      "x grad: None\n",
      "\n",
      "weight requires_grad: True\n",
      "weight grad_fn: None\n",
      "weight grad: None\n",
      "\n",
      "hypothesis requires_grad: True\n",
      "hypothesis grad_fn: <MulBackward0 object at 0x7fa5cf681da0>\n",
      "\n",
      "x.grad: None\n",
      "w.grad: tensor([0.6551])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1)\n",
    "print(x.shape)\n",
    "print('requires_grad:', x.requires_grad)\n",
    "print('grad_fn:', x.grad_fn)\n",
    "print('x grad:', x.grad)\n",
    "\n",
    "w = torch.rand(1, requires_grad=True)    # use the keyword argument to track \n",
    "# w.requires_grad=True    # we could also have done this instead of using the keyword argument\n",
    "print()\n",
    "print('weight requires_grad:', w.requires_grad)\n",
    "print('weight grad_fn:', w.grad_fn)\n",
    "print('weight grad:', w.grad)\n",
    "\n",
    "h = w*x\n",
    "print()\n",
    "print('hypothesis requires_grad:', h.requires_grad)\n",
    "print('hypothesis grad_fn:', h.grad_fn)\n",
    "\n",
    "h.backward()\n",
    "\n",
    "print()\n",
    "print('x.grad:', x.grad)\n",
    "print('w.grad:', w.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation in PyTorch\n",
    "\n",
    "As well as providing tensors and layers, PyTorch provides optimisers. This means that we don't have to write our own optimisers which again are likely to be slower and less efficient. We also don't have to stay up to date with  the latest state of the art optimisers. We don't even need to know how they work to be honest. We can just let Facebook do the hard work for us!\n",
    "\n",
    "A module named `torch.optim` contains many classes that are templates for common optimisers. They include SGD, Adam, RMSProp, Ada and many more optimisers. Most of the time we should just use SGD or Adam.\n",
    "\n",
    "### `.parameters()`\n",
    "When we initialise our PyTorch optimisers, we need to tell it what it will be optimising. What should it be optimising? When we create our model that inherits from `torch.nn.Module`, PyTorch looks out for any layers that also inherit from `torch.nn.Module` (like our `Linear` layer). It knows that the weights of these layers are parameters of our model, and they are automatically added to the model's list of parameters (see more [here](https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter)). We can get the parameters of our model at any point by using it's `.parameters()` method (thanks to the fact that the model inherited from `torch.nn.Module`).\n",
    "\n",
    "Most optimisers also require an initial learning rate, so we should pass that in too.\n",
    "\n",
    "### `.step()`\n",
    "The optimisers that PyTorch provides all use the gradient of the loss (with respect to the parameters that they optimise) to compute how much they should update each weight and in what direction. They access this gradient by looking at the `.grad` attribute for all the parameters that they optimise, which is computed when we call `.backward` on any variable that these parameters affect. We can use the optimiser's `.step()` method  to actually perform the parameter updates - this is the ***learning*** in machine learning.\n",
    "\n",
    "### `.zero_grad()`\n",
    "Calling `.backward()` **does not reset** the existing parameter values of `.grad` for the tensore which gradients are computed for. Instead it **accumulates** them, adding to what was previously there. We may not want this, and we can use out optimiser's `.zero_grad()` method to reset the `.grad` values of the parameters which it's optimising back to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's combine all of that to train a simple linear model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.03427802165094261\n",
      "Epoch: 1 \tLoss: 0.020497414338046714\n",
      "Epoch: 2 \tLoss: 0.0148282668315418\n",
      "Epoch: 3 \tLoss: 0.010735872505920778\n",
      "Epoch: 4 \tLoss: 0.007772946622934147\n",
      "Epoch: 5 \tLoss: 0.0056277400762725725\n",
      "Epoch: 6 \tLoss: 0.004074575563683276\n",
      "Epoch: 7 \tLoss: 0.0029500591354894605\n",
      "Epoch: 8 \tLoss: 0.0021358909086024534\n",
      "Epoch: 9 \tLoss: 0.0015464198390361045\n",
      "learnt weight: Parameter containing:\n",
      "tensor([[1.8799]], dtype=torch.float64, requires_grad=True) \tlearnt bias: tensor([0.7653], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-9e24b0b27fd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ground truth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1668\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1669\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoscale_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36madd_line\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_line%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   1922\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m         \"\"\"\n\u001b[0;32m-> 1924\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mget_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \"\"\"\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mrecache\u001b[0;34m(self, always)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0myconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_yorig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "H = LinearModel()   # instantiate our model\n",
    "H = H.double()\n",
    "optimiser = torch.optim.SGD(H.parameters(), lr=0.1)    # use PyTorch's Stochastic Gradient Descent optimiser\n",
    "L = F.mse_loss    # use a pytorch function that computes the mean squared error (mse)\n",
    "\n",
    "epochs = 10\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    data = list(zip(X, Y))\n",
    "    shuffle(data)\n",
    "    for x, y in data:    # iterate through batches\n",
    "        prediction = H(X)    #make a prediction using our model\n",
    "        loss = L(prediction, Y)    # compute the loss\n",
    "        loss.backward()    # differentiate the loss with respect to all variables that contribute to it in the computational graph and have requires_grad=True\n",
    "        optimiser.step()    # take an optimisation step (update weights)\n",
    "        optimiser.zero_grad()    # reset gradients to zero (otherwise they accumulate)\n",
    "        losses.append(loss)\n",
    "    print(f'Epoch: {epoch} \\tLoss: {loss}')\n",
    "        \n",
    "# GET THE LEARNT PARAMETERS\n",
    "w = H.linear_layer.weight.detach()    # detach removes the tensor from the graph so we can do numpy computation on it\n",
    "b = H.linear_layer.bias.detach()\n",
    "print('learnt weight:', w, '\\tlearnt bias:', b)\n",
    "    \n",
    "# PLOT THE LOSS CURVE AND OUR HYPOTHESIS\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.plot(losses)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.set_ylabel('Label')\n",
    "ax2.set_xlabel('Features')\n",
    "ax2.plot(X, Y, c='r', label='ground truth')\n",
    "ax2.plot(X, w*X+b, label='predictions')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
