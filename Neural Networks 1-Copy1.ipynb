{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- [Fundamental Python]()\n",
    "- [Linear modeals and optimisation]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do the simple models struggle with meaningful tasks?\n",
    "\n",
    "Whereas the size of a house and its price might be linearly correlated, the pixel intensities of an image are certainly not linearly correlated with whether it contains a dog or a cat.\n",
    "\n",
    "![](./images/complex-fn.png)\n",
    "\n",
    "We need to build much more complex models to solve harder problems.\n",
    "\n",
    "## Can we build more complex models by combining many simple transformations?\n",
    "\n",
    "The models we have just seen apply a single transformation to the data. However most problems of practical interest can't be solved using such simple models. \n",
    "\n",
    "Models with greater **capacity** are those which are able to model more complicated functions.\n",
    "\n",
    "A single linear transformation (multiplication by weights of model) stretches the input space by a certain factor in some direction, and adding a constant (bias) shifts it. \n",
    "We call models which apply  \n",
    "What if we applied more than one **layer** of transformations to our inputs, to create a **deep model**. Would we be able to increase the capacity of our model and make it able to model more complex input-output relationships, particularly non-linear ones?\n",
    "\n",
    "![](./images/shallow-vs-deep.png)\n",
    "\n",
    "...well, not quite yet.\n",
    "\n",
    "...if we repeatedly apply a linear transformation, the input can be factorised out of the output, showing that many repeated linear functions are eventually equal to a single linear transformation.\n",
    "\n",
    "![](./images/factor-proof.png)\n",
    "\n",
    "## So how can we increase the capacity of our models?\n",
    "\n",
    "We want to be able to model non-linear functions, so let's try to throw in some non-linear transformations into our model.\n",
    "\n",
    "![](./images/activation.png)\n",
    "\n",
    "These non-linear functions prevent the input being able to be factorised out of the model. Hence the overall transformation can represent non-linear input-output relationships.\n",
    "\n",
    "We call these non-linear functions **activation functions**.\n",
    "\n",
    "However, It's not like we want to introduce really complicated functions into our model - ideally we wouldn't even have to and we could keep things simple. So let's try and complicate things only a minimal amount by keeping our activation functions very simple.\n",
    "\n",
    "Here are some common activation functions. ReLU (Rectified Linear Unit) is by far the most widely used.\n",
    "\n",
    "![](./images/activ-fns.png)\n",
    "\n",
    "Now we have all of the ingredients to fully understand how we can model more complicated functions. Let's look at that all together:\n",
    "\n",
    "![](./images/full-nn.png)\n",
    "\n",
    "Guess what? That is a **neural network**. Surprise.\n",
    "\n",
    "It's just repeated simple linear transformations followed by simple non-linear transformations (activation functions). Simple.\n",
    "\n",
    "Let's learn some jargon.\n",
    "\n",
    "![](./images/nn.png)\n",
    "\n",
    "Neural networks have additional hyperparameters of the depth of the model and the width of each layer. These \n",
    "\n",
    "## What can neural networks do?\n",
    "\n",
    "The motivation that led us to deriving neural networks was that we wanted to model more complex functions. But what functions can a neural network actually represent? Well, as we show below they can actually represent almost all continuous functions. Neural Networks are **general function approximators**.\n",
    "\n",
    "![](./images/univ-approx.png)\n",
    "\n",
    "## How can our neural networks learn to model some function?\n",
    "\n",
    "As we did in the optimisation notebook, we can adjust our model parameters using gradient descent as such:\n",
    "1. Pass input data forward through model to output a prediction\n",
    "2. Calculate loss between predicted output and output label\n",
    "3. Find direction that moving the model parameters in will reduce the error\n",
    "4. Move model weights (parameters) a small amount in that direction \n",
    "\n",
    "![](./images/backprop.png)\n",
    "\n",
    "Here you can see that many terms reappear when computing the gradients of preceeding layers. \n",
    "By caching those terms, we save having to recompute them for these layers nearer the input. This makes finding the gradients of the loss with respect to each weight in the model much more efficient both in terms of memory and speed. \n",
    "This process of computing these gradients effectively is called the **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prepare our data\n",
    "\n",
    "Today we are going to look at a dataset called MNIST (em-nist). It consists of 70,000 images of hand drawn digits from 0-9. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "crop_size = 22\n",
    "\n",
    "traintransforms = []\n",
    "traintransforms.append(transforms.CenterCrop(crop_size))\n",
    "traintransforms.append(transforms.ToTensor())\n",
    "traintransforms = transforms.Compose(traintransforms)\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=traintransforms,          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "testtransforms = []\n",
    "testtransforms.append(transforms.RandomCrop(crop_size))\n",
    "testtransforms.append(transforms.ToTensor())\n",
    "testtransforms = transforms.Compose(testtransforms)\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=testtransforms,\n",
    "                           train=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# PRINT THEIR LENGTHS AND VISUALISE AN EXAMPLE\n",
    "x = train_data[np.random.randint(0, 300)][0]    # get the first example\n",
    "plt.imshow(x[0].numpy())\n",
    "plt.show()\n",
    "x = test_data[np.random.randint(0, 300)][0]    # get the first example\n",
    "plt.imshow(x[0].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Why should we not tune our hyperparameters based on our model's score on the test set?\n",
    "\n",
    "If we adjust our model's hyperparameters so that it performs well on the test set, then we are \n",
    "\n",
    "This is like training for a test and evaluating your performance based on how well you can answer the exact questions that come up.\n",
    "In real life you are unlikely to encounter exactly the same challenges, and so by training on them you will overfit, and not be able to generalise to *different* unseen answers.\n",
    "\n",
    "You may find that a certain set of hyperparameters perform well on the test set, but then fail to perform as well in the wild. \n",
    "Analagously, you may find that a particular \n",
    "\n",
    "## What else can we test them on? \n",
    "\n",
    "We can take some of the data that we plan to train the neural network's weights on and separate it from that main training set. \n",
    "We can then use this split-off data to validate that the current hyperparameters will make our model to perform well on unseen data (both the validation set and the test set are unseen).\n",
    "\n",
    "PyTorch has a utility method `torch.utils.data.random_split()` that makes it easy to randomly split a dataset. Check out the [docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Why should we not pass the whole dataset through the model for each update?\n",
    "We know that to perform gradient based optimisation we need to pass inputs through the model (forward pass), and then compute the loss and find how it changes with respect to each of our model's parameters (backward pass). Modern datasets can be abslutely huge. This means that the forward pass can take a long time, as the function which our model represents has to be applied to each and every input given to it for a forward pass.\n",
    "\n",
    "## Why not just pass a single datapoint to the model for each update?\n",
    "We want our model to perform well on all examples, not just single examples. So we want to compute the loss and associated gradients over several examples to get an average\n",
    "\n",
    "## Mini-batch training\n",
    "The modern way to do training is neither full-batch (whole dataset) or fully stochastic (single datapoint). Instead we use mini-batch training, where we sample several (but not all) datapoints to compute a sample of the gradient, which we then use to update the model. The size of the mini-batch is called the **batch size**. Mini-batches are commonly incorrectly referred to as batches, but it's not that deep.\n",
    "\n",
    "We will experiment with the effect of batch size on the training later.\n",
    "\n",
    "## PyTorch's `DataLoader` \n",
    "PyTorch has a handy utility called a `DataLoader` which can pass us our data in mini-batches of a specified batch size. It can also shuffle them for us.\n",
    "\n",
    "Let's use `torch.data.DataLoader` to create data loaders from our train, validation and test datasets now. Hint: look at the [docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification vs multiclass classification\n",
    "\n",
    "In binary classification the output must be either true or false. Either the example falls into this class, or it doesn't. We have seen that we can represent this by our model having a single output node whose value is forced between 0 and 1, and as such represents a confidence in the fact that the example belongs to the positive class. Alternatively, still for binary classification, we could have two output nodes, where the value of the first represents the confidence that the input belongs to the positive class (true/class 1) and the value of the second represents the confidence that the input belongs to the negative class (false/class 2). In this case, the values of each output node must be positive and they must sum to 1, because this output layer represents a probability distribution over the output classes. Treating true and false as separate classes with separate output nodes shows us how we can extend this idea to do multiclass classification; we simply add more nodes and ensure that their values are positive and sum to one.\n",
    "\n",
    "# multiclass diagram\n",
    "\n",
    "### What function can we use to convert the output layer into a distribution over classes?\n",
    "\n",
    "The **softmax function** exponentiates each value in a vector to make it positive and then divides each of them by their sum to normalise them (make them sum to 1). This ensures that the vector then can be interpreted as a probability distribution.\n",
    "\n",
    "# softmax equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a neural network with PyTorch\n",
    "\n",
    "PyTorch makes it really easy for us to build complex models that can be improved via gradient based optimisation. It does this by providing a class named `torch.nn.Module`. Our model classes should inherit from this class because it does a few very useful things for us:\n",
    "\n",
    "1. `torch.nn.Module` keeps track of all `torch.nn.Parameters` that are created within it. So when we add a linear layer to our model, the parameters (matrix of weights) in that layer will be added to a list of our model's parameters. We can retrieve all parameters of our model using its `parameters()` method. We will later pass this (`mymodel.parameters()`) to our optimiser when we tell it that *this* is what it should be optimising.\n",
    "\n",
    "\n",
    "2. `torch.nn.Module` treats the `forward` method (function) of any child class specially by assigning it to the `__call__` method. That means that running `mymodel.forward(some_data)` is equal to `mymodel(some_data)`. \n",
    "\n",
    "\n",
    "It contains many more useful tools\n",
    "\n",
    "[More detail](https://pytorch.org/tutorials/beginner/nn_tutorial.html) on `torch.nn.Module`\n",
    "Check out the docs [here]()\n",
    "\n",
    "Once we have created a class to represent our model, we need to define how it performs the forward pass. What layers of transformations do we need to give it? \n",
    "Check out these [docs](https://pytorch.org/docs/stable/nn.html#linear-layers) to look at all the layers PyTorch provides.\n",
    "Hint: what layer have I linked to?\n",
    "\n",
    "After we've defined some layers for our model we should implement the forward function that will define what happens when we call an instance of our class. This should pass the argument (our input data) through each of the layers, and apply an activation function to them between each, before returning the transformed input as the output. The output should represent a categorical probability distribution over which class the input belongs to. What shape does it need to be? What function does it need to have applied to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "class NeuralNetworkClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()    # initialise parent module\n",
    "        self.layer1 = torch.nn.Linear(crop_size*crop_size, 225)\n",
    "        self.layer2 = torch.nn.Linear(225, 100)\n",
    "        self.layer3 = torch.nn.Linear(100, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, crop_size*crop_size)\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "def get_n_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return n_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network and visualising it's performance\n",
    "\n",
    "Now we've actually made a template for our model, we can actually\n",
    "- instantiate it by creating an instance of it from our class template\n",
    "- define how we will improve it by specifying an optimiser\n",
    "- define how we will measure its performance by specifying a criterion\n",
    "- train it\n",
    "- write its loss to a graph and see how this changes as it continues to train\n",
    "\n",
    "Let's code that up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 132735\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0005\n",
    "myNeuralNetwork = NeuralNetworkClass()\n",
    "print('Number of parameters in model:', get_n_params(myNeuralNetwork))\n",
    "\n",
    "# CREATE OUR OPTIMISER\n",
    "optimiser = torch.optim.Adam(              # what optimiser should we use?\n",
    "    myNeuralNetwork.parameters(),          # what should it optimise?\n",
    "    lr=learning_rate                       # using what learning rate?\n",
    ")\n",
    "\n",
    "# CREATE OUR CRITERION\n",
    "criterion = torch.nn.CrossEntropyLoss()             # callable class that compares our predictions to our labels and returns our loss\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()                            # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 0 \tLoss: tensor(2.3034, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 1 \tLoss: tensor(2.3028, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 2 \tLoss: tensor(2.2999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 3 \tLoss: tensor(2.2984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 4 \tLoss: tensor(2.2986, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 5 \tLoss: tensor(2.2969, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 6 \tLoss: tensor(2.2923, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 7 \tLoss: tensor(2.2916, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 8 \tLoss: tensor(2.2891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 9 \tLoss: tensor(2.2869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 10 \tLoss: tensor(2.2853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 11 \tLoss: tensor(2.2811, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 12 \tLoss: tensor(2.2771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 13 \tLoss: tensor(2.2696, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 14 \tLoss: tensor(2.2661, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 15 \tLoss: tensor(2.2651, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 16 \tLoss: tensor(2.2603, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 17 \tLoss: tensor(2.2479, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 18 \tLoss: tensor(2.2502, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 19 \tLoss: tensor(2.2293, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 20 \tLoss: tensor(2.2131, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 21 \tLoss: tensor(2.2299, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 22 \tLoss: tensor(2.2051, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 23 \tLoss: tensor(2.1972, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 24 \tLoss: tensor(2.1567, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 25 \tLoss: tensor(2.1727, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 26 \tLoss: tensor(2.1382, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 27 \tLoss: tensor(2.1792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 28 \tLoss: tensor(2.1294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 29 \tLoss: tensor(2.1406, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 30 \tLoss: tensor(2.1219, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 31 \tLoss: tensor(2.1067, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 32 \tLoss: tensor(2.0719, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 33 \tLoss: tensor(2.0840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 34 \tLoss: tensor(2.1156, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 35 \tLoss: tensor(2.0604, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 36 \tLoss: tensor(2.0458, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 37 \tLoss: tensor(2.0417, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 38 \tLoss: tensor(1.9807, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 39 \tLoss: tensor(2.0429, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 40 \tLoss: tensor(1.9470, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 41 \tLoss: tensor(1.9545, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 42 \tLoss: tensor(1.9683, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 43 \tLoss: tensor(1.9549, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 44 \tLoss: tensor(1.8983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 45 \tLoss: tensor(1.9114, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 46 \tLoss: tensor(1.9139, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 47 \tLoss: tensor(1.8424, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 48 \tLoss: tensor(1.8559, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 49 \tLoss: tensor(1.8703, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 50 \tLoss: tensor(1.8567, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 51 \tLoss: tensor(1.8153, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 52 \tLoss: tensor(1.8527, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 53 \tLoss: tensor(1.8399, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 54 \tLoss: tensor(1.8152, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 55 \tLoss: tensor(1.7647, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 56 \tLoss: tensor(1.8794, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 57 \tLoss: tensor(1.8037, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 58 \tLoss: tensor(1.8417, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 59 \tLoss: tensor(1.8618, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 60 \tLoss: tensor(1.8152, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 61 \tLoss: tensor(1.8041, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 62 \tLoss: tensor(1.7774, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 63 \tLoss: tensor(1.8436, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 64 \tLoss: tensor(1.7598, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 65 \tLoss: tensor(1.7709, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 66 \tLoss: tensor(1.8030, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 67 \tLoss: tensor(1.7750, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 68 \tLoss: tensor(1.7837, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 69 \tLoss: tensor(1.7365, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 70 \tLoss: tensor(1.7661, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 71 \tLoss: tensor(1.7366, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 72 \tLoss: tensor(1.7136, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 73 \tLoss: tensor(1.7344, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 74 \tLoss: tensor(1.7419, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 75 \tLoss: tensor(1.7348, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 76 \tLoss: tensor(1.7298, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 77 \tLoss: tensor(1.7370, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 78 \tLoss: tensor(1.7091, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 79 \tLoss: tensor(1.7038, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 80 \tLoss: tensor(1.6996, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 81 \tLoss: tensor(1.7041, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 82 \tLoss: tensor(1.7567, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 83 \tLoss: tensor(1.7055, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 84 \tLoss: tensor(1.7224, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 85 \tLoss: tensor(1.6767, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 86 \tLoss: tensor(1.6976, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 87 \tLoss: tensor(1.6971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 88 \tLoss: tensor(1.7178, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 89 \tLoss: tensor(1.6950, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 90 \tLoss: tensor(1.6747, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 91 \tLoss: tensor(1.6967, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 92 \tLoss: tensor(1.6487, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 93 \tLoss: tensor(1.7354, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 94 \tLoss: tensor(1.6552, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 95 \tLoss: tensor(1.6628, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 96 \tLoss: tensor(1.6309, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 97 \tLoss: tensor(1.6800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 98 \tLoss: tensor(1.6521, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 99 \tLoss: tensor(1.6602, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 100 \tLoss: tensor(1.6755, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 101 \tLoss: tensor(1.6692, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 102 \tLoss: tensor(1.6562, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 103 \tLoss: tensor(1.6810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 104 \tLoss: tensor(1.6545, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 105 \tLoss: tensor(1.6293, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 106 \tLoss: tensor(1.6337, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 107 \tLoss: tensor(1.6328, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 108 \tLoss: tensor(1.6745, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 109 \tLoss: tensor(1.6700, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 110 \tLoss: tensor(1.6804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 111 \tLoss: tensor(1.6223, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 112 \tLoss: tensor(1.6457, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 113 \tLoss: tensor(1.6865, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 114 \tLoss: tensor(1.6547, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 115 \tLoss: tensor(1.6443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 116 \tLoss: tensor(1.6407, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 117 \tLoss: tensor(1.6188, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 118 \tLoss: tensor(1.6364, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 119 \tLoss: tensor(1.6189, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 120 \tLoss: tensor(1.6466, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 121 \tLoss: tensor(1.6241, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 122 \tLoss: tensor(1.5822, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 123 \tLoss: tensor(1.6250, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 124 \tLoss: tensor(1.6149, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 125 \tLoss: tensor(1.6528, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 126 \tLoss: tensor(1.6390, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 127 \tLoss: tensor(1.5957, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 128 \tLoss: tensor(1.6394, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 129 \tLoss: tensor(1.6393, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 130 \tLoss: tensor(1.5934, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 131 \tLoss: tensor(1.6407, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 132 \tLoss: tensor(1.6195, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 133 \tLoss: tensor(1.6099, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 134 \tLoss: tensor(1.6296, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 135 \tLoss: tensor(1.5913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 136 \tLoss: tensor(1.6037, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 137 \tLoss: tensor(1.6042, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 138 \tLoss: tensor(1.6028, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 139 \tLoss: tensor(1.5928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 140 \tLoss: tensor(1.6415, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 141 \tLoss: tensor(1.6449, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 142 \tLoss: tensor(1.6092, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 143 \tLoss: tensor(1.5697, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 144 \tLoss: tensor(1.5831, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 145 \tLoss: tensor(1.6129, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 146 \tLoss: tensor(1.6160, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 147 \tLoss: tensor(1.6006, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 148 \tLoss: tensor(1.5965, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 149 \tLoss: tensor(1.6412, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 150 \tLoss: tensor(1.5610, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 151 \tLoss: tensor(1.5951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 152 \tLoss: tensor(1.6134, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 153 \tLoss: tensor(1.5846, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 154 \tLoss: tensor(1.6156, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 155 \tLoss: tensor(1.6159, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 156 \tLoss: tensor(1.6342, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 157 \tLoss: tensor(1.6204, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 158 \tLoss: tensor(1.6282, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 159 \tLoss: tensor(1.5971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 160 \tLoss: tensor(1.6012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 161 \tLoss: tensor(1.5783, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 162 \tLoss: tensor(1.5826, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 163 \tLoss: tensor(1.6295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 164 \tLoss: tensor(1.5841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 165 \tLoss: tensor(1.5810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 166 \tLoss: tensor(1.6482, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 167 \tLoss: tensor(1.5834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 168 \tLoss: tensor(1.5934, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 169 \tLoss: tensor(1.6257, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 170 \tLoss: tensor(1.6291, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 171 \tLoss: tensor(1.5936, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 172 \tLoss: tensor(1.5775, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 173 \tLoss: tensor(1.6420, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 174 \tLoss: tensor(1.5547, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 175 \tLoss: tensor(1.6237, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 176 \tLoss: tensor(1.5720, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 177 \tLoss: tensor(1.5993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 178 \tLoss: tensor(1.5938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 179 \tLoss: tensor(1.5521, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 180 \tLoss: tensor(1.5974, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 181 \tLoss: tensor(1.6017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 182 \tLoss: tensor(1.5720, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 183 \tLoss: tensor(1.5883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 184 \tLoss: tensor(1.5846, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 185 \tLoss: tensor(1.5821, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 186 \tLoss: tensor(1.6176, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 187 \tLoss: tensor(1.5836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 188 \tLoss: tensor(1.5796, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 189 \tLoss: tensor(1.5780, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 190 \tLoss: tensor(1.6014, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 191 \tLoss: tensor(1.5819, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 192 \tLoss: tensor(1.5870, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 193 \tLoss: tensor(1.6207, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 194 \tLoss: tensor(1.6039, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 195 \tLoss: tensor(1.6170, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 196 \tLoss: tensor(1.5852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 197 \tLoss: tensor(1.5630, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 198 \tLoss: tensor(1.5956, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 199 \tLoss: tensor(1.5656, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 200 \tLoss: tensor(1.5382, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 201 \tLoss: tensor(1.5908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 202 \tLoss: tensor(1.5610, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 203 \tLoss: tensor(1.5992, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 204 \tLoss: tensor(1.6137, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 205 \tLoss: tensor(1.6151, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 206 \tLoss: tensor(1.6183, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 207 \tLoss: tensor(1.5807, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 208 \tLoss: tensor(1.6104, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 209 \tLoss: tensor(1.6066, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 210 \tLoss: tensor(1.5661, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 211 \tLoss: tensor(1.6150, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 212 \tLoss: tensor(1.5984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 213 \tLoss: tensor(1.6402, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 214 \tLoss: tensor(1.5851, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 215 \tLoss: tensor(1.5746, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 216 \tLoss: tensor(1.5792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 217 \tLoss: tensor(1.5800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 218 \tLoss: tensor(1.6070, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 219 \tLoss: tensor(1.6008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 220 \tLoss: tensor(1.5753, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 221 \tLoss: tensor(1.5997, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 222 \tLoss: tensor(1.5926, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 223 \tLoss: tensor(1.6205, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 224 \tLoss: tensor(1.6076, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 225 \tLoss: tensor(1.5901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 226 \tLoss: tensor(1.5749, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 227 \tLoss: tensor(1.5942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 228 \tLoss: tensor(1.5726, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 229 \tLoss: tensor(1.5866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 230 \tLoss: tensor(1.5854, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 231 \tLoss: tensor(1.5728, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 232 \tLoss: tensor(1.5878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 233 \tLoss: tensor(1.5504, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 234 \tLoss: tensor(1.5429, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 235 \tLoss: tensor(1.5504, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 236 \tLoss: tensor(1.5841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 237 \tLoss: tensor(1.6068, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 238 \tLoss: tensor(1.5595, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 239 \tLoss: tensor(1.5814, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 240 \tLoss: tensor(1.5588, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 241 \tLoss: tensor(1.5888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 242 \tLoss: tensor(1.5656, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 243 \tLoss: tensor(1.5667, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 244 \tLoss: tensor(1.5537, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 245 \tLoss: tensor(1.5775, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 246 \tLoss: tensor(1.5635, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 247 \tLoss: tensor(1.5684, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 248 \tLoss: tensor(1.5495, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 249 \tLoss: tensor(1.5625, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 250 \tLoss: tensor(1.5450, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 251 \tLoss: tensor(1.5505, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 252 \tLoss: tensor(1.5517, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 253 \tLoss: tensor(1.6169, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 254 \tLoss: tensor(1.5600, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 255 \tLoss: tensor(1.5655, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 256 \tLoss: tensor(1.6028, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 257 \tLoss: tensor(1.5619, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 258 \tLoss: tensor(1.5547, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 259 \tLoss: tensor(1.5785, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 260 \tLoss: tensor(1.5807, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 261 \tLoss: tensor(1.6214, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 262 \tLoss: tensor(1.5733, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 263 \tLoss: tensor(1.5291, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 264 \tLoss: tensor(1.5508, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 265 \tLoss: tensor(1.5989, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 266 \tLoss: tensor(1.5849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 267 \tLoss: tensor(1.5985, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 268 \tLoss: tensor(1.5936, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 269 \tLoss: tensor(1.5770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 270 \tLoss: tensor(1.5700, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 271 \tLoss: tensor(1.5744, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 272 \tLoss: tensor(1.5712, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 273 \tLoss: tensor(1.5577, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 274 \tLoss: tensor(1.5728, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 275 \tLoss: tensor(1.5267, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 276 \tLoss: tensor(1.5599, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 277 \tLoss: tensor(1.5779, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 278 \tLoss: tensor(1.5888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 279 \tLoss: tensor(1.6415, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 280 \tLoss: tensor(1.6072, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 281 \tLoss: tensor(1.6166, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 282 \tLoss: tensor(1.5608, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 283 \tLoss: tensor(1.5302, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 284 \tLoss: tensor(1.5773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 285 \tLoss: tensor(1.5744, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 286 \tLoss: tensor(1.5975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 287 \tLoss: tensor(1.5976, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 288 \tLoss: tensor(1.5865, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 289 \tLoss: tensor(1.6097, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 290 \tLoss: tensor(1.5475, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 291 \tLoss: tensor(1.5500, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 292 \tLoss: tensor(1.6163, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 293 \tLoss: tensor(1.5629, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 294 \tLoss: tensor(1.5870, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 295 \tLoss: tensor(1.5677, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 296 \tLoss: tensor(1.5728, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 297 \tLoss: tensor(1.5659, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 298 \tLoss: tensor(1.5794, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 299 \tLoss: tensor(1.5744, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 300 \tLoss: tensor(1.5707, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 301 \tLoss: tensor(1.5838, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 302 \tLoss: tensor(1.5465, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 303 \tLoss: tensor(1.5965, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 304 \tLoss: tensor(1.5193, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 305 \tLoss: tensor(1.5534, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 306 \tLoss: tensor(1.5473, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 307 \tLoss: tensor(1.5803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 308 \tLoss: tensor(1.5861, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 309 \tLoss: tensor(1.5300, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 310 \tLoss: tensor(1.5511, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 311 \tLoss: tensor(1.5579, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 312 \tLoss: tensor(1.5926, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 313 \tLoss: tensor(1.5503, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 314 \tLoss: tensor(1.5682, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 315 \tLoss: tensor(1.5288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 316 \tLoss: tensor(1.6106, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 317 \tLoss: tensor(1.6081, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 318 \tLoss: tensor(1.5952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 319 \tLoss: tensor(1.5455, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 320 \tLoss: tensor(1.5661, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 321 \tLoss: tensor(1.5859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 322 \tLoss: tensor(1.5457, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 323 \tLoss: tensor(1.5652, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 324 \tLoss: tensor(1.5598, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 325 \tLoss: tensor(1.5468, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 326 \tLoss: tensor(1.5752, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 327 \tLoss: tensor(1.5686, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 328 \tLoss: tensor(1.5579, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 329 \tLoss: tensor(1.6331, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 330 \tLoss: tensor(1.5577, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 331 \tLoss: tensor(1.5339, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 332 \tLoss: tensor(1.5752, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 333 \tLoss: tensor(1.5629, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 334 \tLoss: tensor(1.5576, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 335 \tLoss: tensor(1.5557, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 336 \tLoss: tensor(1.5651, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 337 \tLoss: tensor(1.5760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 338 \tLoss: tensor(1.5620, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 339 \tLoss: tensor(1.6075, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 340 \tLoss: tensor(1.5566, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 341 \tLoss: tensor(1.5490, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 342 \tLoss: tensor(1.5551, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 343 \tLoss: tensor(1.6030, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 344 \tLoss: tensor(1.6182, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 345 \tLoss: tensor(1.5434, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 346 \tLoss: tensor(1.5614, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 347 \tLoss: tensor(1.5504, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 348 \tLoss: tensor(1.5454, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 349 \tLoss: tensor(1.5542, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 350 \tLoss: tensor(1.5772, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 351 \tLoss: tensor(1.5284, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 352 \tLoss: tensor(1.5810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 353 \tLoss: tensor(1.5346, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 354 \tLoss: tensor(1.5828, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 355 \tLoss: tensor(1.5721, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 356 \tLoss: tensor(1.5534, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 357 \tLoss: tensor(1.5743, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 358 \tLoss: tensor(1.5403, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 359 \tLoss: tensor(1.5715, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 360 \tLoss: tensor(1.5738, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 361 \tLoss: tensor(1.5747, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 362 \tLoss: tensor(1.5553, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 363 \tLoss: tensor(1.5414, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 364 \tLoss: tensor(1.5483, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 365 \tLoss: tensor(1.5732, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 366 \tLoss: tensor(1.5722, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 367 \tLoss: tensor(1.5670, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 368 \tLoss: tensor(1.5328, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 369 \tLoss: tensor(1.5779, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 370 \tLoss: tensor(1.5358, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 371 \tLoss: tensor(1.5564, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 372 \tLoss: tensor(1.5809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 373 \tLoss: tensor(1.5344, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 374 \tLoss: tensor(1.5420, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 375 \tLoss: tensor(1.5466, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 376 \tLoss: tensor(1.5422, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 377 \tLoss: tensor(1.5369, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 378 \tLoss: tensor(1.6028, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 379 \tLoss: tensor(1.5687, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 380 \tLoss: tensor(1.5395, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 381 \tLoss: tensor(1.5629, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 382 \tLoss: tensor(1.5592, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 383 \tLoss: tensor(1.5454, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 384 \tLoss: tensor(1.5868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 385 \tLoss: tensor(1.5831, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 386 \tLoss: tensor(1.5726, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 387 \tLoss: tensor(1.5932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 388 \tLoss: tensor(1.5748, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 389 \tLoss: tensor(1.5331, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 390 \tLoss: tensor(1.5157, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 0 \tLoss: tensor(1.5504, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 1 \tLoss: tensor(1.5529, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 2 \tLoss: tensor(1.5567, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 3 \tLoss: tensor(1.5287, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 4 \tLoss: tensor(1.5713, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 5 \tLoss: tensor(1.5411, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 6 \tLoss: tensor(1.5623, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 7 \tLoss: tensor(1.5990, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 8 \tLoss: tensor(1.5173, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 9 \tLoss: tensor(1.5551, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 10 \tLoss: tensor(1.5768, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 11 \tLoss: tensor(1.5649, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 12 \tLoss: tensor(1.5646, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 13 \tLoss: tensor(1.5480, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 14 \tLoss: tensor(1.5599, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 15 \tLoss: tensor(1.5584, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 16 \tLoss: tensor(1.5885, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 17 \tLoss: tensor(1.5652, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 18 \tLoss: tensor(1.5446, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 19 \tLoss: tensor(1.5688, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 20 \tLoss: tensor(1.5435, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 21 \tLoss: tensor(1.5468, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 22 \tLoss: tensor(1.5844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 23 \tLoss: tensor(1.5926, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 24 \tLoss: tensor(1.5299, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 25 \tLoss: tensor(1.6074, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 26 \tLoss: tensor(1.5762, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 27 \tLoss: tensor(1.5613, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 28 \tLoss: tensor(1.5250, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 29 \tLoss: tensor(1.5414, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 30 \tLoss: tensor(1.5492, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 31 \tLoss: tensor(1.5429, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 32 \tLoss: tensor(1.5379, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 33 \tLoss: tensor(1.5401, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 34 \tLoss: tensor(1.5758, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 35 \tLoss: tensor(1.5616, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 36 \tLoss: tensor(1.5392, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 37 \tLoss: tensor(1.5228, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 38 \tLoss: tensor(1.5555, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 39 \tLoss: tensor(1.5380, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 40 \tLoss: tensor(1.5443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 41 \tLoss: tensor(1.5825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 42 \tLoss: tensor(1.5857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 43 \tLoss: tensor(1.5738, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 44 \tLoss: tensor(1.5434, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 45 \tLoss: tensor(1.5818, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 46 \tLoss: tensor(1.5608, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 47 \tLoss: tensor(1.5529, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 48 \tLoss: tensor(1.5180, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 49 \tLoss: tensor(1.5241, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 50 \tLoss: tensor(1.5534, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 51 \tLoss: tensor(1.5555, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 52 \tLoss: tensor(1.5362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 53 \tLoss: tensor(1.5546, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 54 \tLoss: tensor(1.5549, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 55 \tLoss: tensor(1.5640, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 56 \tLoss: tensor(1.5278, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 57 \tLoss: tensor(1.5313, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 58 \tLoss: tensor(1.5370, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 59 \tLoss: tensor(1.5474, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 60 \tLoss: tensor(1.5791, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 61 \tLoss: tensor(1.5362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 62 \tLoss: tensor(1.5398, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 63 \tLoss: tensor(1.5420, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 64 \tLoss: tensor(1.5667, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 65 \tLoss: tensor(1.5562, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 66 \tLoss: tensor(1.5770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 67 \tLoss: tensor(1.5290, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 68 \tLoss: tensor(1.5427, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 69 \tLoss: tensor(1.5310, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 70 \tLoss: tensor(1.5928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 71 \tLoss: tensor(1.5384, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 72 \tLoss: tensor(1.5594, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 73 \tLoss: tensor(1.5245, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 74 \tLoss: tensor(1.5565, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 75 \tLoss: tensor(1.5339, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 76 \tLoss: tensor(1.5379, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 77 \tLoss: tensor(1.5603, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 78 \tLoss: tensor(1.5284, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 79 \tLoss: tensor(1.5466, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 80 \tLoss: tensor(1.5473, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 81 \tLoss: tensor(1.5433, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 82 \tLoss: tensor(1.5339, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 83 \tLoss: tensor(1.5542, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 84 \tLoss: tensor(1.5875, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 85 \tLoss: tensor(1.5670, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 86 \tLoss: tensor(1.5624, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 87 \tLoss: tensor(1.5573, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 88 \tLoss: tensor(1.5280, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 89 \tLoss: tensor(1.5467, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 90 \tLoss: tensor(1.5509, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 91 \tLoss: tensor(1.5805, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 92 \tLoss: tensor(1.5182, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 93 \tLoss: tensor(1.5338, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 94 \tLoss: tensor(1.5562, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 95 \tLoss: tensor(1.5292, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 96 \tLoss: tensor(1.5314, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 97 \tLoss: tensor(1.5308, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 98 \tLoss: tensor(1.5703, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 99 \tLoss: tensor(1.5707, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 100 \tLoss: tensor(1.5566, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 101 \tLoss: tensor(1.5187, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 102 \tLoss: tensor(1.5415, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 103 \tLoss: tensor(1.5696, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 104 \tLoss: tensor(1.5678, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 105 \tLoss: tensor(1.6047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 106 \tLoss: tensor(1.5865, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 107 \tLoss: tensor(1.5784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 108 \tLoss: tensor(1.5265, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 109 \tLoss: tensor(1.5538, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 110 \tLoss: tensor(1.5327, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 111 \tLoss: tensor(1.5608, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 112 \tLoss: tensor(1.5540, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 113 \tLoss: tensor(1.5736, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 114 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 115 \tLoss: tensor(1.5653, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 116 \tLoss: tensor(1.5192, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 117 \tLoss: tensor(1.5251, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 118 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 119 \tLoss: tensor(1.5790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 120 \tLoss: tensor(1.5392, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 121 \tLoss: tensor(1.5659, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 122 \tLoss: tensor(1.5361, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 123 \tLoss: tensor(1.5138, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 124 \tLoss: tensor(1.5108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 125 \tLoss: tensor(1.5404, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 126 \tLoss: tensor(1.5154, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 127 \tLoss: tensor(1.5404, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 128 \tLoss: tensor(1.5845, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 129 \tLoss: tensor(1.5483, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 130 \tLoss: tensor(1.5180, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 131 \tLoss: tensor(1.5841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 132 \tLoss: tensor(1.5426, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 133 \tLoss: tensor(1.5537, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 134 \tLoss: tensor(1.5441, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 135 \tLoss: tensor(1.5833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 136 \tLoss: tensor(1.5451, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 137 \tLoss: tensor(1.5927, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 138 \tLoss: tensor(1.5727, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 139 \tLoss: tensor(1.5801, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 140 \tLoss: tensor(1.5316, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 141 \tLoss: tensor(1.5354, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 142 \tLoss: tensor(1.5162, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 143 \tLoss: tensor(1.5690, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 144 \tLoss: tensor(1.5565, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 145 \tLoss: tensor(1.5856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 146 \tLoss: tensor(1.5452, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 147 \tLoss: tensor(1.5891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 148 \tLoss: tensor(1.5261, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 149 \tLoss: tensor(1.5534, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 150 \tLoss: tensor(1.5519, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 151 \tLoss: tensor(1.5071, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 152 \tLoss: tensor(1.5455, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 153 \tLoss: tensor(1.5171, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 154 \tLoss: tensor(1.5499, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 155 \tLoss: tensor(1.5560, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 156 \tLoss: tensor(1.5363, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 157 \tLoss: tensor(1.5337, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 158 \tLoss: tensor(1.5316, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 159 \tLoss: tensor(1.5564, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 160 \tLoss: tensor(1.5576, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 161 \tLoss: tensor(1.5242, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 162 \tLoss: tensor(1.5624, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 163 \tLoss: tensor(1.5434, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 164 \tLoss: tensor(1.5372, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 165 \tLoss: tensor(1.5573, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 166 \tLoss: tensor(1.5513, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 167 \tLoss: tensor(1.5759, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 168 \tLoss: tensor(1.5609, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 169 \tLoss: tensor(1.5328, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 170 \tLoss: tensor(1.5249, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 171 \tLoss: tensor(1.5159, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 172 \tLoss: tensor(1.5815, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 173 \tLoss: tensor(1.5316, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 174 \tLoss: tensor(1.5609, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 175 \tLoss: tensor(1.5678, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 176 \tLoss: tensor(1.5481, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 177 \tLoss: tensor(1.5234, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 178 \tLoss: tensor(1.5481, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 179 \tLoss: tensor(1.5381, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 180 \tLoss: tensor(1.5831, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 181 \tLoss: tensor(1.5672, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 182 \tLoss: tensor(1.5393, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 183 \tLoss: tensor(1.5133, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 184 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 185 \tLoss: tensor(1.5210, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 186 \tLoss: tensor(1.5365, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 187 \tLoss: tensor(1.5145, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 188 \tLoss: tensor(1.5504, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 189 \tLoss: tensor(1.5674, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 190 \tLoss: tensor(1.5472, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 191 \tLoss: tensor(1.5391, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 192 \tLoss: tensor(1.5370, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 193 \tLoss: tensor(1.5330, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 194 \tLoss: tensor(1.5491, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 195 \tLoss: tensor(1.5728, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 196 \tLoss: tensor(1.5786, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 197 \tLoss: tensor(1.5519, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 198 \tLoss: tensor(1.5632, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 199 \tLoss: tensor(1.5288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 200 \tLoss: tensor(1.5875, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 201 \tLoss: tensor(1.5168, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 202 \tLoss: tensor(1.5970, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 203 \tLoss: tensor(1.5590, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 204 \tLoss: tensor(1.5211, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 205 \tLoss: tensor(1.5474, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 206 \tLoss: tensor(1.5445, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 207 \tLoss: tensor(1.5558, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 208 \tLoss: tensor(1.5502, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 209 \tLoss: tensor(1.5194, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 210 \tLoss: tensor(1.5577, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 211 \tLoss: tensor(1.5494, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 212 \tLoss: tensor(1.5035, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 213 \tLoss: tensor(1.5492, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 214 \tLoss: tensor(1.5554, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 215 \tLoss: tensor(1.5472, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 216 \tLoss: tensor(1.5260, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 217 \tLoss: tensor(1.5443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 218 \tLoss: tensor(1.5742, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 219 \tLoss: tensor(1.5811, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 220 \tLoss: tensor(1.5583, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 221 \tLoss: tensor(1.5292, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 222 \tLoss: tensor(1.5235, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 223 \tLoss: tensor(1.5229, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 224 \tLoss: tensor(1.5494, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 225 \tLoss: tensor(1.5262, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 226 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 227 \tLoss: tensor(1.5371, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 228 \tLoss: tensor(1.5259, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 229 \tLoss: tensor(1.5405, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 230 \tLoss: tensor(1.5439, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 231 \tLoss: tensor(1.5097, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 232 \tLoss: tensor(1.5404, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 233 \tLoss: tensor(1.5292, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 234 \tLoss: tensor(1.5526, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 235 \tLoss: tensor(1.5662, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 236 \tLoss: tensor(1.5276, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 237 \tLoss: tensor(1.5506, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 238 \tLoss: tensor(1.5294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 239 \tLoss: tensor(1.5450, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 240 \tLoss: tensor(1.5564, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 241 \tLoss: tensor(1.5979, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 242 \tLoss: tensor(1.5316, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 243 \tLoss: tensor(1.5955, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 244 \tLoss: tensor(1.5133, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 245 \tLoss: tensor(1.5203, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 246 \tLoss: tensor(1.5442, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 247 \tLoss: tensor(1.5320, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 248 \tLoss: tensor(1.5109, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 249 \tLoss: tensor(1.5325, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 250 \tLoss: tensor(1.5426, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 251 \tLoss: tensor(1.5707, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 252 \tLoss: tensor(1.5295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 253 \tLoss: tensor(1.5480, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 254 \tLoss: tensor(1.5079, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 255 \tLoss: tensor(1.5313, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 256 \tLoss: tensor(1.5501, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 257 \tLoss: tensor(1.5341, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 258 \tLoss: tensor(1.5718, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 259 \tLoss: tensor(1.5193, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 260 \tLoss: tensor(1.5409, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 261 \tLoss: tensor(1.5439, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 262 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 263 \tLoss: tensor(1.5399, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 264 \tLoss: tensor(1.5462, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 265 \tLoss: tensor(1.5235, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 266 \tLoss: tensor(1.5667, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 267 \tLoss: tensor(1.5453, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 268 \tLoss: tensor(1.5362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 269 \tLoss: tensor(1.4844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 270 \tLoss: tensor(1.5542, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 271 \tLoss: tensor(1.5472, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 272 \tLoss: tensor(1.5669, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 273 \tLoss: tensor(1.5328, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 274 \tLoss: tensor(1.5546, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 275 \tLoss: tensor(1.5295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 276 \tLoss: tensor(1.5372, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 277 \tLoss: tensor(1.5503, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 278 \tLoss: tensor(1.5432, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 279 \tLoss: tensor(1.5734, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 280 \tLoss: tensor(1.5218, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 281 \tLoss: tensor(1.5627, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 282 \tLoss: tensor(1.5281, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 283 \tLoss: tensor(1.5648, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 284 \tLoss: tensor(1.5427, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 285 \tLoss: tensor(1.5200, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 286 \tLoss: tensor(1.5562, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 287 \tLoss: tensor(1.5320, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 288 \tLoss: tensor(1.5299, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 289 \tLoss: tensor(1.5512, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 290 \tLoss: tensor(1.5358, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 291 \tLoss: tensor(1.5565, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 292 \tLoss: tensor(1.5446, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 293 \tLoss: tensor(1.5482, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 294 \tLoss: tensor(1.5465, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 295 \tLoss: tensor(1.5545, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 296 \tLoss: tensor(1.5482, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 297 \tLoss: tensor(1.5512, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 298 \tLoss: tensor(1.5340, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 299 \tLoss: tensor(1.5572, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 300 \tLoss: tensor(1.5573, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 301 \tLoss: tensor(1.5550, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 302 \tLoss: tensor(1.5745, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 303 \tLoss: tensor(1.5496, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 304 \tLoss: tensor(1.5679, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 305 \tLoss: tensor(1.5156, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 306 \tLoss: tensor(1.5237, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 307 \tLoss: tensor(1.5422, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 308 \tLoss: tensor(1.5345, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 309 \tLoss: tensor(1.5251, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 310 \tLoss: tensor(1.5826, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 311 \tLoss: tensor(1.5203, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 312 \tLoss: tensor(1.5442, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 313 \tLoss: tensor(1.5368, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 314 \tLoss: tensor(1.5573, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 315 \tLoss: tensor(1.5625, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 316 \tLoss: tensor(1.5336, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 317 \tLoss: tensor(1.5475, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 318 \tLoss: tensor(1.5780, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 319 \tLoss: tensor(1.5519, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 320 \tLoss: tensor(1.5082, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 321 \tLoss: tensor(1.5455, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 322 \tLoss: tensor(1.5194, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 323 \tLoss: tensor(1.5142, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 324 \tLoss: tensor(1.5527, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 325 \tLoss: tensor(1.5243, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 326 \tLoss: tensor(1.5533, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 327 \tLoss: tensor(1.5347, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 328 \tLoss: tensor(1.5251, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 329 \tLoss: tensor(1.5418, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 330 \tLoss: tensor(1.5597, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 331 \tLoss: tensor(1.5369, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 332 \tLoss: tensor(1.5375, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 333 \tLoss: tensor(1.5422, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 334 \tLoss: tensor(1.5557, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 335 \tLoss: tensor(1.5743, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 336 \tLoss: tensor(1.5498, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 337 \tLoss: tensor(1.5279, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 338 \tLoss: tensor(1.5429, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 339 \tLoss: tensor(1.5568, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 340 \tLoss: tensor(1.5572, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 341 \tLoss: tensor(1.5420, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 342 \tLoss: tensor(1.5460, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 343 \tLoss: tensor(1.5756, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 344 \tLoss: tensor(1.5450, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 345 \tLoss: tensor(1.5418, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 346 \tLoss: tensor(1.5450, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 347 \tLoss: tensor(1.5514, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 348 \tLoss: tensor(1.5459, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 349 \tLoss: tensor(1.5452, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 350 \tLoss: tensor(1.5552, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 351 \tLoss: tensor(1.5584, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 352 \tLoss: tensor(1.5543, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 353 \tLoss: tensor(1.5464, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 354 \tLoss: tensor(1.5352, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 355 \tLoss: tensor(1.5233, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 356 \tLoss: tensor(1.5252, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 357 \tLoss: tensor(1.5407, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 358 \tLoss: tensor(1.5372, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 359 \tLoss: tensor(1.5344, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 360 \tLoss: tensor(1.5462, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 361 \tLoss: tensor(1.5375, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 362 \tLoss: tensor(1.4982, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 363 \tLoss: tensor(1.5171, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 364 \tLoss: tensor(1.5358, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 365 \tLoss: tensor(1.5518, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 366 \tLoss: tensor(1.5856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 367 \tLoss: tensor(1.5402, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 368 \tLoss: tensor(1.5273, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 369 \tLoss: tensor(1.5210, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 370 \tLoss: tensor(1.5412, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 371 \tLoss: tensor(1.5388, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 372 \tLoss: tensor(1.5499, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 373 \tLoss: tensor(1.5472, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 374 \tLoss: tensor(1.5595, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 375 \tLoss: tensor(1.5494, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 376 \tLoss: tensor(1.5526, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 377 \tLoss: tensor(1.5428, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 378 \tLoss: tensor(1.5206, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 379 \tLoss: tensor(1.5223, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 380 \tLoss: tensor(1.5451, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 381 \tLoss: tensor(1.5234, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 382 \tLoss: tensor(1.5234, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 383 \tLoss: tensor(1.5210, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 384 \tLoss: tensor(1.5457, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 385 \tLoss: tensor(1.5556, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 386 \tLoss: tensor(1.5172, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 387 \tLoss: tensor(1.5663, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 388 \tLoss: tensor(1.5420, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 389 \tLoss: tensor(1.5251, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 390 \tLoss: tensor(1.6083, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "def train(model, epochs):\n",
    "    model.train()                                  # put the model into training mode (more on this later)\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader):\n",
    "            inputs, labels = minibatch\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            writer.add_scalar('Loss/Train', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "            \n",
    "            \n",
    "train(myNeuralNetwork, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the loss actually mean practically?\n",
    "\n",
    "The absolute value of the loss doesn't really mean much, it's just a way of continuously evaluating the relative performance of the model whilst it trains. The real metric of performance that we care about is the proportion of ***unseen*** examples that our neural network can correctly classify. These unseen examples are what the test loader consists of.\n",
    "\n",
    "Let's write the code to calculate that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def calc_accuracy(model, dataloader):\n",
    "    num_correct = 0\n",
    "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER\n",
    "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 93.176\n",
      "Validation Accuracy: 92.74\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy:', calc_accuracy(myNeuralNetwork, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(myNeuralNetwork, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 46.67\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy:', calc_accuracy(myNeuralNetwork, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. Compare the loss curves generated by using different batch sizes. What's the best? As you change the batch size, what variable do you need to change to give those curves the same domain over the x-axis (num writes to summary writer)\n",
    "2. It would be good to validate our model as we go along to ensure that we don't overfit. Let's write a training loop that tests the loss on the validation set after each epoch. Plot the validation error alongside What can you see on the graphs that indicates overfitting?\n",
    "3. What is the best accuracy you can achieve? Can you implement a grid search and a random search to try them automatically. Record all permutations that you try.\n",
    "4. What feature of the input data is our standard neural network not taking advantage of? Hint: '************* neural networks' take this into account.\n",
    "\n",
    "## Congratulations you boss, you've finished the notebook!\n",
    "\n",
    "Please provide your feedback [here](https://docs.google.com/forms/d/e/1FAIpQLSdZSxvkAE19vjDN4jpp0VvUBPGr_wdtayGAcRNfFGH7e7jQDQ/viewform?usp=sf_link). It means a lot to us.\n",
    "\n",
    "Next, you might want to check out:\n",
    "- [Convolutional Neural Networks](https://github.com/AI-Core/Convolutional-Neural-Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
