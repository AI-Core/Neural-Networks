{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- [Fundamental Python]()\n",
    "- [Linear modeals and optimisation]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do the simple models struggle with meaningful tasks?\n",
    "\n",
    "Whereas the size of a house and its price might be linearly correlated, the pixel intensities of an image are certainly not linearly correlated with whether it contains a dog or a cat.\n",
    "\n",
    "![](./images/complex-fn.png)\n",
    "\n",
    "We need to build much more complex models to solve harder problems.\n",
    "\n",
    "## Can we build more complex models by combining many simple transformations?\n",
    "\n",
    "The models we have just seen apply a single transformation to the data. However most problems of practical interest can't be solved using such simple models. \n",
    "\n",
    "Models with greater **capacity** are those which are able to model more complicated functions.\n",
    "\n",
    "A single linear transformation (multiplication by weights of model) stretches the input space by a certain factor in some direction, and adding a constant (bias) shifts it. \n",
    "We call models which apply  \n",
    "What if we applied more than one **layer** of transformations to our inputs, to create a **deep model**. Would we be able to increase the capacity of our model and make it able to model more complex input-output relationships, particularly non-linear ones?\n",
    "\n",
    "![](./images/shallow-vs-deep.png)\n",
    "\n",
    "...well, not quite yet.\n",
    "\n",
    "...if we repeatedly apply a linear transformation, the input can be factorised out of the output, showing that many repeated linear functions are eventually equal to a single linear transformation.\n",
    "\n",
    "![](./images/factor-proof.png)\n",
    "\n",
    "## So how can we increase the capacity of our models?\n",
    "\n",
    "We want to be able to model non-linear functions, so let's try to throw in some non-linear transformations into our model.\n",
    "\n",
    "![](./images/activation.png)\n",
    "\n",
    "These non-linear functions prevent the input being able to be factorised out of the model. Hence the overall transformation can represent non-linear input-output relationships.\n",
    "\n",
    "We call these non-linear functions **activation functions**.\n",
    "\n",
    "However, It's not like we want to introduce really complicated functions into our model - ideally we wouldn't even have to and we could keep things simple. So let's try and complicate things only a minimal amount by keeping our activation functions very simple.\n",
    "\n",
    "Here are some common activation functions. ReLU (Rectified Linear Unit) is by far the most widely used.\n",
    "\n",
    "![](./images/activ-fns.png)\n",
    "\n",
    "Now we have all of the ingredients to fully understand how we can model more complicated functions. Let's look at that all together:\n",
    "\n",
    "![](./images/full-nn.png)\n",
    "\n",
    "Guess what? That is a **neural network**. Surprise.\n",
    "\n",
    "It's just repeated simple linear transformations followed by simple non-linear transformations (activation functions). Simple.\n",
    "\n",
    "Let's learn some jargon.\n",
    "\n",
    "![](./images/nn.png)\n",
    "\n",
    "Neural networks have additional hyperparameters of the depth of the model and the width of each layer. These \n",
    "\n",
    "## What can neural networks do?\n",
    "\n",
    "The motivation that led us to deriving neural networks was that we wanted to model more complex functions. But what functions can a neural network actually represent? Well, as we show below they can actually represent almost all continuous functions. Neural Networks are **general function approximators**.\n",
    "\n",
    "![](./images/univ-approx.png)\n",
    "\n",
    "## How can our neural networks learn to model some function?\n",
    "\n",
    "As we did in the optimisation notebook, we can adjust our model parameters using gradient descent as such:\n",
    "1. Pass input data forward through model to output a prediction\n",
    "2. Calculate loss between predicted output and output label\n",
    "3. Find direction that moving the model parameters in will reduce the error\n",
    "4. Move model weights (parameters) a small amount in that direction \n",
    "\n",
    "![](./images/backprop.png)\n",
    "\n",
    "Here you can see that many terms reappear when computing the gradients of preceeding layers. \n",
    "By caching those terms, we save having to recompute them for these layers nearer the input. This makes finding the gradients of the loss with respect to each weight in the model much more efficient both in terms of memory and speed. \n",
    "This process of computing these gradients effectively is called the **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prepare our data\n",
    "\n",
    "Today we are going to look at a dataset called MNIST (em-nist). It consists of 70,000 images of hand drawn digits from 0-9. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=transforms.ToTensor(),          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           train=False,\n",
    "                          )\n",
    "\n",
    "# PRINT THEIR LENGTHS AND VISUALISE AN EXAMPLE\n",
    "x = train_data[0][0]    # get the first example\n",
    "tensor = # get the actual input data\n",
    "t = # create the transform that can be called to convert the tensor into a PIL Image\n",
    "img = t(img)    # call the transform on the tensor\n",
    "img.show()    # show the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Why should we not tune our hyperparameters based on our model's score on the test set?\n",
    "\n",
    "If we adjust our model's hyperparameters so that it performs well on the test set, then we are \n",
    "\n",
    "This is like training for a test and evaluating your performance based on how well you can answer the exact questions that come up.\n",
    "In real life you are unlikely to encounter exactly the same challenges, and so by training on them you will overfit, and not be able to generalise to *different* unseen answers.\n",
    "\n",
    "You may find that a certain set of hyperparameters perform well on the test set, but then fail to perform as well in the wild. \n",
    "Analagously, you may find that a particular \n",
    "\n",
    "## What else can we test them on? \n",
    "\n",
    "We can take some of the data that we plan to train the neural network's weights on and separate it from that main training set. \n",
    "We can then use this split-off data to validate that the current hyperparameters will make our model to perform well on unseen data (both the validation set and the test set are unseen).\n",
    "\n",
    "PyTorch has a utility method `torch.utils.data.random_split()` that makes it easy to randomly split a dataset. Check out the [docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Why should we not pass the whole dataset through the model for each update?\n",
    "We know that to perform gradient based optimisation we need to pass inputs through the model (forward pass), and then compute the loss and find how it changes with respect to each of our model's parameters (backward pass). Modern datasets can be abslutely huge. This means that the forward pass can take a long time, as the function which our model represents has to be applied to each and every input given to it for a forward pass.\n",
    "\n",
    "## Why not just pass a single datapoint to the model for each update?\n",
    "We want our model to perform well on all examples, not just single examples. So we want to compute the loss and associated gradients over several examples to get an average\n",
    "\n",
    "## Mini-batch training\n",
    "The modern way to do training is neither full-batch (whole dataset) or fully stochastic (single datapoint). Instead we use mini-batch training, where we sample several (but not all) datapoints to compute a sample of the gradient, which we then use to update the model. The size of the mini-batch is called the **batch size**. Mini-batches are commonly incorrectly referred to as batches, but it's not that deep.\n",
    "\n",
    "We will experiment with the effect of batch size on the training later.\n",
    "\n",
    "## PyTorch's `DataLoader` \n",
    "PyTorch has a handy utility called a `DataLoader` which can pass us our data in mini-batches of a specified batch size. It can also shuffle them for us.\n",
    "\n",
    "Let's use `torch.data.DataLoader` to create data loaders from our train, validation and test datasets now. Hint: look at the [docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification vs multiclass classification\n",
    "\n",
    "In binary classification the output must be either true or false. Either the example falls into this class, or it doesn't. We have seen that we can represent this by our model having a single output node whose value is forced between 0 and 1, and as such represents a confidence in the fact that the example belongs to the positive class. Alternatively, still for binary classification, we could have two output nodes, where the value of the first represents the confidence that the input belongs to the positive class (true/class 1) and the value of the second represents the confidence that the input belongs to the negative class (false/class 2). In this case, the values of each output node must be positive and they must sum to 1, because this output layer represents a probability distribution over the output classes. Treating true and false as separate classes with separate output nodes shows us how we can extend this idea to do multiclass classification; we simply add more nodes and ensure that their values are positive and sum to one.\n",
    "\n",
    "# multiclass diagram\n",
    "\n",
    "### What function can we use to convert the output layer into a distribution over classes?\n",
    "\n",
    "The **softmax function** exponentiates each value in a vector to make it positive and then divides each of them by their sum to normalise them (make them sum to 1). This ensures that the vector then can be interpreted as a probability distribution.\n",
    "\n",
    "# softmax equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a neural network with PyTorch\n",
    "\n",
    "PyTorch makes it really easy for us to build complex models that can be improved via gradient based optimisation. It does this by providing a class named `torch.nn.Module`. Our model classes should inherit from this class because it does a few very useful things for us:\n",
    "\n",
    "1. `torch.nn.Module` keeps track of all `torch.nn.Parameters` that are created within it. So when we add a linear layer to our model, the parameters (matrix of weights) in that layer will be added to a list of our model's parameters. We can retrieve all parameters of our model using its `parameters()` method. We will later pass this (`mymodel.parameters()`) to our optimiser when we tell it that *this* is what it should be optimising.\n",
    "\n",
    "\n",
    "2. `torch.nn.Module` treats the `forward` method (function) of any child class specially by assigning it to the `__call__` method. That means that running `mymodel.forward(some_data)` is equal to `mymodel(some_data)`. \n",
    "\n",
    "\n",
    "It contains many more useful tools\n",
    "\n",
    "[More detail](https://pytorch.org/tutorials/beginner/nn_tutorial.html) on `torch.nn.Module`\n",
    "Check out the docs [here]()\n",
    "\n",
    "Once we have created a class to represent our model, we need to define how it performs the forward pass. What layers of transformations do we need to give it? \n",
    "Check out these [docs](https://pytorch.org/docs/stable/nn.html#linear-layers) to look at all the layers PyTorch provides.\n",
    "Hint: what layer have I linked to?\n",
    "\n",
    "After we've defined some layers for our model we should implement the forward function that will define what happens when we call an instance of our class. This should pass the argument (our input data) through each of the layers, and apply an activation function to them between each, before returning the transformed input as the output. The output should represent a categorical probability distribution over which class the input belongs to. What shape does it need to be? What function does it need to have applied to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "class NeuralNetworkClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()    # initialise parent module\n",
    "        self.layer1 = torch.nn.Linear(784, 1024)\n",
    "        self.layer2 = torch.nn.Linear(1024, 256)\n",
    "        self.layer3 = torch.nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network and visualising it's performance\n",
    "\n",
    "Now we've actually made a template for our model, we can actually\n",
    "- instantiate it by creating an instance of it from our class template\n",
    "- define how we will improve it by specifying an optimiser\n",
    "- define how we will measure its performance by specifying a criterion\n",
    "- train it\n",
    "- write its loss to a graph and see how this changes as it continues to train\n",
    "\n",
    "Let's code that up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 0 \tLoss: tensor(2.3027, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 1 \tLoss: tensor(2.2930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 2 \tLoss: tensor(2.2799, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 3 \tLoss: tensor(2.2629, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 4 \tLoss: tensor(2.2299, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 5 \tLoss: tensor(2.1872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 6 \tLoss: tensor(2.1375, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 7 \tLoss: tensor(2.1033, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 8 \tLoss: tensor(2.0194, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 9 \tLoss: tensor(1.9594, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 10 \tLoss: tensor(1.9145, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 11 \tLoss: tensor(1.8701, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 12 \tLoss: tensor(1.8186, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 13 \tLoss: tensor(1.8581, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 14 \tLoss: tensor(1.8167, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 15 \tLoss: tensor(1.8483, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 16 \tLoss: tensor(1.7811, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 17 \tLoss: tensor(1.8155, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 18 \tLoss: tensor(1.7510, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 19 \tLoss: tensor(1.7859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 20 \tLoss: tensor(1.6983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 21 \tLoss: tensor(1.7304, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 22 \tLoss: tensor(1.7573, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 23 \tLoss: tensor(1.7031, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 24 \tLoss: tensor(1.6967, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 25 \tLoss: tensor(1.6876, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 26 \tLoss: tensor(1.6816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 27 \tLoss: tensor(1.7210, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 28 \tLoss: tensor(1.6762, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 29 \tLoss: tensor(1.6961, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 30 \tLoss: tensor(1.6443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 31 \tLoss: tensor(1.7483, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 32 \tLoss: tensor(1.7018, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 33 \tLoss: tensor(1.6841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 34 \tLoss: tensor(1.6643, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 35 \tLoss: tensor(1.6788, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 36 \tLoss: tensor(1.6528, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 37 \tLoss: tensor(1.6279, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 38 \tLoss: tensor(1.6593, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 39 \tLoss: tensor(1.6601, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 40 \tLoss: tensor(1.7102, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 41 \tLoss: tensor(1.6343, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 42 \tLoss: tensor(1.6823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 43 \tLoss: tensor(1.6285, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 44 \tLoss: tensor(1.6501, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 45 \tLoss: tensor(1.7110, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 46 \tLoss: tensor(1.6596, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 47 \tLoss: tensor(1.6725, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 48 \tLoss: tensor(1.6480, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 49 \tLoss: tensor(1.6066, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 50 \tLoss: tensor(1.6434, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 51 \tLoss: tensor(1.5946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 52 \tLoss: tensor(1.6377, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 53 \tLoss: tensor(1.6647, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 54 \tLoss: tensor(1.6494, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 55 \tLoss: tensor(1.5998, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 56 \tLoss: tensor(1.6303, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 57 \tLoss: tensor(1.7110, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 58 \tLoss: tensor(1.6598, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 59 \tLoss: tensor(1.6357, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 60 \tLoss: tensor(1.6793, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 61 \tLoss: tensor(1.6375, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 62 \tLoss: tensor(1.6391, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 63 \tLoss: tensor(1.6416, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 64 \tLoss: tensor(1.6128, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 65 \tLoss: tensor(1.6628, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 66 \tLoss: tensor(1.6332, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 67 \tLoss: tensor(1.6748, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 68 \tLoss: tensor(1.6288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 69 \tLoss: tensor(1.6349, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 70 \tLoss: tensor(1.6383, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 71 \tLoss: tensor(1.6213, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 72 \tLoss: tensor(1.6521, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 73 \tLoss: tensor(1.6431, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 74 \tLoss: tensor(1.6275, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 75 \tLoss: tensor(1.6570, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 76 \tLoss: tensor(1.6203, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 77 \tLoss: tensor(1.6594, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 78 \tLoss: tensor(1.6038, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 79 \tLoss: tensor(1.6171, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 80 \tLoss: tensor(1.6321, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 81 \tLoss: tensor(1.6405, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 82 \tLoss: tensor(1.6459, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 83 \tLoss: tensor(1.6119, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 84 \tLoss: tensor(1.6139, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 85 \tLoss: tensor(1.5681, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 86 \tLoss: tensor(1.5674, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 87 \tLoss: tensor(1.6040, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 88 \tLoss: tensor(1.5703, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 89 \tLoss: tensor(1.5676, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 90 \tLoss: tensor(1.5605, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 91 \tLoss: tensor(1.5930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 92 \tLoss: tensor(1.5770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 93 \tLoss: tensor(1.5938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 94 \tLoss: tensor(1.5623, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 95 \tLoss: tensor(1.5540, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 96 \tLoss: tensor(1.5847, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 97 \tLoss: tensor(1.5312, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 98 \tLoss: tensor(1.5461, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 99 \tLoss: tensor(1.5909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 100 \tLoss: tensor(1.5873, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 101 \tLoss: tensor(1.5842, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 102 \tLoss: tensor(1.5528, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 103 \tLoss: tensor(1.5423, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 104 \tLoss: tensor(1.5441, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 105 \tLoss: tensor(1.5671, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 106 \tLoss: tensor(1.5400, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 107 \tLoss: tensor(1.5452, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 108 \tLoss: tensor(1.5354, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 109 \tLoss: tensor(1.5709, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 110 \tLoss: tensor(1.5385, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 111 \tLoss: tensor(1.5497, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 112 \tLoss: tensor(1.6131, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 113 \tLoss: tensor(1.5381, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 114 \tLoss: tensor(1.5942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 115 \tLoss: tensor(1.5420, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 116 \tLoss: tensor(1.5540, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 117 \tLoss: tensor(1.5408, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 118 \tLoss: tensor(1.6051, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 119 \tLoss: tensor(1.5401, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 120 \tLoss: tensor(1.5606, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 121 \tLoss: tensor(1.5317, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 122 \tLoss: tensor(1.5503, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 123 \tLoss: tensor(1.5735, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 124 \tLoss: tensor(1.5900, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 125 \tLoss: tensor(1.5771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 126 \tLoss: tensor(1.5624, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 127 \tLoss: tensor(1.5139, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 128 \tLoss: tensor(1.5773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 129 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 130 \tLoss: tensor(1.5402, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 131 \tLoss: tensor(1.5323, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 132 \tLoss: tensor(1.5277, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 133 \tLoss: tensor(1.5432, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 134 \tLoss: tensor(1.5267, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 135 \tLoss: tensor(1.5696, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 136 \tLoss: tensor(1.5593, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 137 \tLoss: tensor(1.5377, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 138 \tLoss: tensor(1.5525, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 139 \tLoss: tensor(1.5271, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 140 \tLoss: tensor(1.5630, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 141 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 142 \tLoss: tensor(1.5555, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 143 \tLoss: tensor(1.5684, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 144 \tLoss: tensor(1.5868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 145 \tLoss: tensor(1.5569, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 146 \tLoss: tensor(1.5323, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 147 \tLoss: tensor(1.5481, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 148 \tLoss: tensor(1.5632, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 149 \tLoss: tensor(1.5464, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 150 \tLoss: tensor(1.5579, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 151 \tLoss: tensor(1.5683, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 152 \tLoss: tensor(1.5508, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 153 \tLoss: tensor(1.5403, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 154 \tLoss: tensor(1.5491, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 155 \tLoss: tensor(1.5433, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 156 \tLoss: tensor(1.5411, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 157 \tLoss: tensor(1.5545, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 158 \tLoss: tensor(1.5519, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 159 \tLoss: tensor(1.5462, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 160 \tLoss: tensor(1.5508, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 161 \tLoss: tensor(1.5236, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 162 \tLoss: tensor(1.5594, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 163 \tLoss: tensor(1.5343, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 164 \tLoss: tensor(1.5552, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 165 \tLoss: tensor(1.5534, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 166 \tLoss: tensor(1.5467, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 167 \tLoss: tensor(1.5441, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 168 \tLoss: tensor(1.5604, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 169 \tLoss: tensor(1.5502, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 170 \tLoss: tensor(1.5549, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 171 \tLoss: tensor(1.5650, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 172 \tLoss: tensor(1.5417, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 173 \tLoss: tensor(1.5260, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 174 \tLoss: tensor(1.5295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 175 \tLoss: tensor(1.5626, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 176 \tLoss: tensor(1.5367, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 177 \tLoss: tensor(1.5351, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 178 \tLoss: tensor(1.5292, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 179 \tLoss: tensor(1.5274, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 180 \tLoss: tensor(1.5508, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 181 \tLoss: tensor(1.5300, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 182 \tLoss: tensor(1.5540, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 183 \tLoss: tensor(1.5450, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 184 \tLoss: tensor(1.5108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 185 \tLoss: tensor(1.5288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 186 \tLoss: tensor(1.5406, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 187 \tLoss: tensor(1.5074, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 188 \tLoss: tensor(1.5349, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 189 \tLoss: tensor(1.5246, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 190 \tLoss: tensor(1.5318, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 191 \tLoss: tensor(1.5244, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 192 \tLoss: tensor(1.5347, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 193 \tLoss: tensor(1.5538, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 194 \tLoss: tensor(1.5388, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 195 \tLoss: tensor(1.5028, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 0 \tLoss: tensor(1.5214, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 1 \tLoss: tensor(1.5246, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 2 \tLoss: tensor(1.5164, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 3 \tLoss: tensor(1.5052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 4 \tLoss: tensor(1.5261, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 5 \tLoss: tensor(1.5384, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 6 \tLoss: tensor(1.5209, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 7 \tLoss: tensor(1.5173, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 8 \tLoss: tensor(1.5346, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 9 \tLoss: tensor(1.5266, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 10 \tLoss: tensor(1.5287, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 11 \tLoss: tensor(1.5310, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 12 \tLoss: tensor(1.5277, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 13 \tLoss: tensor(1.5330, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 14 \tLoss: tensor(1.5250, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 15 \tLoss: tensor(1.5537, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 16 \tLoss: tensor(1.5302, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 17 \tLoss: tensor(1.5104, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 18 \tLoss: tensor(1.5193, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 19 \tLoss: tensor(1.5158, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 20 \tLoss: tensor(1.5222, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 21 \tLoss: tensor(1.5058, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 22 \tLoss: tensor(1.5717, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 23 \tLoss: tensor(1.5310, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 24 \tLoss: tensor(1.5492, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 25 \tLoss: tensor(1.5273, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 26 \tLoss: tensor(1.5333, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 27 \tLoss: tensor(1.5207, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 28 \tLoss: tensor(1.5185, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 29 \tLoss: tensor(1.5284, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 30 \tLoss: tensor(1.5220, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 31 \tLoss: tensor(1.5061, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 32 \tLoss: tensor(1.5278, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 33 \tLoss: tensor(1.5323, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 34 \tLoss: tensor(1.5391, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 35 \tLoss: tensor(1.5345, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 36 \tLoss: tensor(1.5363, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 37 \tLoss: tensor(1.5176, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 38 \tLoss: tensor(1.5450, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 39 \tLoss: tensor(1.5219, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 40 \tLoss: tensor(1.5036, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 41 \tLoss: tensor(1.5273, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 42 \tLoss: tensor(1.5372, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 43 \tLoss: tensor(1.5297, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 44 \tLoss: tensor(1.5108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 45 \tLoss: tensor(1.5235, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 46 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 47 \tLoss: tensor(1.5289, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 48 \tLoss: tensor(1.5199, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 49 \tLoss: tensor(1.5276, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 50 \tLoss: tensor(1.5289, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 51 \tLoss: tensor(1.5624, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 52 \tLoss: tensor(1.5055, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 53 \tLoss: tensor(1.5162, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 54 \tLoss: tensor(1.5168, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 55 \tLoss: tensor(1.5365, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 56 \tLoss: tensor(1.5299, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 57 \tLoss: tensor(1.5362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 58 \tLoss: tensor(1.5422, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 59 \tLoss: tensor(1.5475, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 60 \tLoss: tensor(1.5119, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 61 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 62 \tLoss: tensor(1.5225, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 63 \tLoss: tensor(1.5537, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 64 \tLoss: tensor(1.5147, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 65 \tLoss: tensor(1.5280, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 66 \tLoss: tensor(1.5238, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 67 \tLoss: tensor(1.5180, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 68 \tLoss: tensor(1.5015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 69 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 70 \tLoss: tensor(1.4983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 71 \tLoss: tensor(1.5111, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 72 \tLoss: tensor(1.5249, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 73 \tLoss: tensor(1.5070, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 74 \tLoss: tensor(1.5169, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 75 \tLoss: tensor(1.5240, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 76 \tLoss: tensor(1.5143, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 77 \tLoss: tensor(1.5115, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 78 \tLoss: tensor(1.5319, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 79 \tLoss: tensor(1.5222, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 80 \tLoss: tensor(1.5262, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 81 \tLoss: tensor(1.5426, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 82 \tLoss: tensor(1.5305, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 83 \tLoss: tensor(1.5179, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 84 \tLoss: tensor(1.5190, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 85 \tLoss: tensor(1.5061, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 86 \tLoss: tensor(1.5203, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 87 \tLoss: tensor(1.5285, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 88 \tLoss: tensor(1.5660, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 89 \tLoss: tensor(1.5162, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 90 \tLoss: tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 91 \tLoss: tensor(1.5002, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 92 \tLoss: tensor(1.5246, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 93 \tLoss: tensor(1.5178, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 94 \tLoss: tensor(1.5223, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 95 \tLoss: tensor(1.5209, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 96 \tLoss: tensor(1.5067, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 97 \tLoss: tensor(1.5315, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 98 \tLoss: tensor(1.5109, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 99 \tLoss: tensor(1.5037, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 100 \tLoss: tensor(1.5244, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 101 \tLoss: tensor(1.5372, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 102 \tLoss: tensor(1.5245, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 103 \tLoss: tensor(1.5217, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 104 \tLoss: tensor(1.5294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 105 \tLoss: tensor(1.5261, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 106 \tLoss: tensor(1.5133, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 107 \tLoss: tensor(1.5069, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 108 \tLoss: tensor(1.5001, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 109 \tLoss: tensor(1.5066, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 110 \tLoss: tensor(1.5238, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 111 \tLoss: tensor(1.5263, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 112 \tLoss: tensor(1.5248, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 113 \tLoss: tensor(1.5357, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 114 \tLoss: tensor(1.5171, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 115 \tLoss: tensor(1.5082, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 116 \tLoss: tensor(1.5289, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 117 \tLoss: tensor(1.5304, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 118 \tLoss: tensor(1.5051, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 119 \tLoss: tensor(1.5168, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 120 \tLoss: tensor(1.5165, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 121 \tLoss: tensor(1.5354, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 122 \tLoss: tensor(1.5234, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 123 \tLoss: tensor(1.5224, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 124 \tLoss: tensor(1.5124, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 125 \tLoss: tensor(1.5457, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 126 \tLoss: tensor(1.5293, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 127 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 128 \tLoss: tensor(1.5429, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 129 \tLoss: tensor(1.5367, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 130 \tLoss: tensor(1.5078, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 131 \tLoss: tensor(1.5163, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 132 \tLoss: tensor(1.5095, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 133 \tLoss: tensor(1.5167, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 134 \tLoss: tensor(1.5356, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 135 \tLoss: tensor(1.5248, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 136 \tLoss: tensor(1.5152, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 137 \tLoss: tensor(1.5155, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 138 \tLoss: tensor(1.5066, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 139 \tLoss: tensor(1.5107, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 140 \tLoss: tensor(1.5102, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 141 \tLoss: tensor(1.5067, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 142 \tLoss: tensor(1.4974, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 143 \tLoss: tensor(1.5056, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 144 \tLoss: tensor(1.4970, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 145 \tLoss: tensor(1.5002, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 146 \tLoss: tensor(1.5161, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 147 \tLoss: tensor(1.5217, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 148 \tLoss: tensor(1.5290, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 149 \tLoss: tensor(1.5093, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 150 \tLoss: tensor(1.5193, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 151 \tLoss: tensor(1.5138, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 152 \tLoss: tensor(1.5300, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 153 \tLoss: tensor(1.5165, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 154 \tLoss: tensor(1.5063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 155 \tLoss: tensor(1.5051, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 156 \tLoss: tensor(1.5015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 157 \tLoss: tensor(1.5068, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 158 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 159 \tLoss: tensor(1.4998, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 160 \tLoss: tensor(1.5154, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 161 \tLoss: tensor(1.5162, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 162 \tLoss: tensor(1.5176, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 163 \tLoss: tensor(1.5035, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 164 \tLoss: tensor(1.5130, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 165 \tLoss: tensor(1.5020, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 166 \tLoss: tensor(1.5147, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 167 \tLoss: tensor(1.5194, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 168 \tLoss: tensor(1.5192, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 169 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 170 \tLoss: tensor(1.5135, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 171 \tLoss: tensor(1.5177, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 172 \tLoss: tensor(1.5380, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 173 \tLoss: tensor(1.5394, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 174 \tLoss: tensor(1.5454, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 175 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 176 \tLoss: tensor(1.5137, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 177 \tLoss: tensor(1.5288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 178 \tLoss: tensor(1.5175, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 179 \tLoss: tensor(1.4895, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 180 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 181 \tLoss: tensor(1.5085, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 182 \tLoss: tensor(1.5099, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 183 \tLoss: tensor(1.5016, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 184 \tLoss: tensor(1.5087, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 185 \tLoss: tensor(1.5035, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 186 \tLoss: tensor(1.5103, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 187 \tLoss: tensor(1.5160, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 188 \tLoss: tensor(1.5173, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 189 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 190 \tLoss: tensor(1.4933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 191 \tLoss: tensor(1.5210, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 192 \tLoss: tensor(1.5076, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 193 \tLoss: tensor(1.4984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 194 \tLoss: tensor(1.5127, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 195 \tLoss: tensor(1.5007, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 0 \tLoss: tensor(1.5074, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 1 \tLoss: tensor(1.4783, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 2 \tLoss: tensor(1.4977, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 3 \tLoss: tensor(1.5182, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 4 \tLoss: tensor(1.5091, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 5 \tLoss: tensor(1.4997, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 6 \tLoss: tensor(1.4992, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 7 \tLoss: tensor(1.5019, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 8 \tLoss: tensor(1.4905, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 9 \tLoss: tensor(1.5105, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 10 \tLoss: tensor(1.5312, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 11 \tLoss: tensor(1.5245, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 12 \tLoss: tensor(1.4878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 13 \tLoss: tensor(1.5009, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 14 \tLoss: tensor(1.5331, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 15 \tLoss: tensor(1.4994, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 16 \tLoss: tensor(1.5076, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 17 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 18 \tLoss: tensor(1.5156, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 19 \tLoss: tensor(1.5130, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 20 \tLoss: tensor(1.5162, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 21 \tLoss: tensor(1.5292, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 22 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 23 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 24 \tLoss: tensor(1.4956, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 25 \tLoss: tensor(1.5169, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 26 \tLoss: tensor(1.5033, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 27 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 28 \tLoss: tensor(1.4992, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 29 \tLoss: tensor(1.5099, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 30 \tLoss: tensor(1.5052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 31 \tLoss: tensor(1.4965, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 32 \tLoss: tensor(1.5045, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 33 \tLoss: tensor(1.5319, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 34 \tLoss: tensor(1.4946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 35 \tLoss: tensor(1.5164, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 36 \tLoss: tensor(1.5124, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 37 \tLoss: tensor(1.5136, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 38 \tLoss: tensor(1.5362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 39 \tLoss: tensor(1.5040, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 40 \tLoss: tensor(1.4982, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 41 \tLoss: tensor(1.5212, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 42 \tLoss: tensor(1.5132, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 43 \tLoss: tensor(1.5133, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 44 \tLoss: tensor(1.5040, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 45 \tLoss: tensor(1.5093, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 46 \tLoss: tensor(1.5040, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 47 \tLoss: tensor(1.5013, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 48 \tLoss: tensor(1.4941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 49 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 50 \tLoss: tensor(1.5055, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 51 \tLoss: tensor(1.5036, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 52 \tLoss: tensor(1.5126, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 53 \tLoss: tensor(1.5247, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 54 \tLoss: tensor(1.5142, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 55 \tLoss: tensor(1.4962, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 56 \tLoss: tensor(1.5031, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 57 \tLoss: tensor(1.4865, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 58 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 59 \tLoss: tensor(1.5016, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 60 \tLoss: tensor(1.5175, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 61 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 62 \tLoss: tensor(1.5240, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 63 \tLoss: tensor(1.5129, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 64 \tLoss: tensor(1.5094, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 65 \tLoss: tensor(1.5123, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 66 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 67 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 68 \tLoss: tensor(1.5040, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 69 \tLoss: tensor(1.5063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 70 \tLoss: tensor(1.5038, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 71 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 72 \tLoss: tensor(1.4963, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 73 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 74 \tLoss: tensor(1.5137, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 75 \tLoss: tensor(1.4882, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 76 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 77 \tLoss: tensor(1.4991, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 78 \tLoss: tensor(1.5054, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 79 \tLoss: tensor(1.4994, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 80 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 81 \tLoss: tensor(1.5029, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 82 \tLoss: tensor(1.5025, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 83 \tLoss: tensor(1.5094, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 84 \tLoss: tensor(1.4965, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 85 \tLoss: tensor(1.4966, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 86 \tLoss: tensor(1.5115, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 87 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 88 \tLoss: tensor(1.4949, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 89 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 90 \tLoss: tensor(1.4972, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 91 \tLoss: tensor(1.4998, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 92 \tLoss: tensor(1.5009, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 93 \tLoss: tensor(1.5041, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 94 \tLoss: tensor(1.5126, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 95 \tLoss: tensor(1.5194, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 96 \tLoss: tensor(1.4938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 97 \tLoss: tensor(1.5086, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 98 \tLoss: tensor(1.5204, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 99 \tLoss: tensor(1.4951, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tBatch: 100 \tLoss: tensor(1.5075, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 101 \tLoss: tensor(1.4956, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 102 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 103 \tLoss: tensor(1.4927, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 104 \tLoss: tensor(1.4876, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 105 \tLoss: tensor(1.5003, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 106 \tLoss: tensor(1.4985, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 107 \tLoss: tensor(1.5054, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 108 \tLoss: tensor(1.5116, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 109 \tLoss: tensor(1.5006, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 110 \tLoss: tensor(1.5068, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 111 \tLoss: tensor(1.4998, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 112 \tLoss: tensor(1.5154, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 113 \tLoss: tensor(1.4979, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 114 \tLoss: tensor(1.5057, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 115 \tLoss: tensor(1.5108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 116 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 117 \tLoss: tensor(1.5117, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 118 \tLoss: tensor(1.5258, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 119 \tLoss: tensor(1.4979, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 120 \tLoss: tensor(1.5105, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 121 \tLoss: tensor(1.5150, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 122 \tLoss: tensor(1.5097, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 123 \tLoss: tensor(1.5197, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 124 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 125 \tLoss: tensor(1.4914, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 126 \tLoss: tensor(1.5066, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 127 \tLoss: tensor(1.5108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 128 \tLoss: tensor(1.5007, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 129 \tLoss: tensor(1.4996, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 130 \tLoss: tensor(1.4983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 131 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 132 \tLoss: tensor(1.5092, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 133 \tLoss: tensor(1.5011, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 134 \tLoss: tensor(1.5054, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 135 \tLoss: tensor(1.4907, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 136 \tLoss: tensor(1.5070, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 137 \tLoss: tensor(1.4922, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 138 \tLoss: tensor(1.4943, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 139 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 140 \tLoss: tensor(1.4953, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 141 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 142 \tLoss: tensor(1.5155, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 143 \tLoss: tensor(1.4917, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 144 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 145 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 146 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 147 \tLoss: tensor(1.4909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 148 \tLoss: tensor(1.4920, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 149 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 150 \tLoss: tensor(1.4870, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 151 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 152 \tLoss: tensor(1.5229, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 153 \tLoss: tensor(1.5064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 154 \tLoss: tensor(1.5084, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 155 \tLoss: tensor(1.4992, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 156 \tLoss: tensor(1.5054, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 157 \tLoss: tensor(1.4864, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 158 \tLoss: tensor(1.5045, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 159 \tLoss: tensor(1.4965, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 160 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 161 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 162 \tLoss: tensor(1.5223, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 163 \tLoss: tensor(1.5020, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 164 \tLoss: tensor(1.4927, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 165 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 166 \tLoss: tensor(1.4911, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 167 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 168 \tLoss: tensor(1.5027, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 169 \tLoss: tensor(1.5068, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 170 \tLoss: tensor(1.5256, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 171 \tLoss: tensor(1.4876, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 172 \tLoss: tensor(1.4994, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 173 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 174 \tLoss: tensor(1.4903, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 175 \tLoss: tensor(1.5023, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 176 \tLoss: tensor(1.5146, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 177 \tLoss: tensor(1.4888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 178 \tLoss: tensor(1.5341, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 179 \tLoss: tensor(1.5079, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 180 \tLoss: tensor(1.5009, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 181 \tLoss: tensor(1.5089, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 182 \tLoss: tensor(1.4962, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 183 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 184 \tLoss: tensor(1.4932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 185 \tLoss: tensor(1.5065, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 186 \tLoss: tensor(1.5086, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 187 \tLoss: tensor(1.5232, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 188 \tLoss: tensor(1.5166, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 189 \tLoss: tensor(1.4990, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 190 \tLoss: tensor(1.5045, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 191 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 192 \tLoss: tensor(1.5027, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 193 \tLoss: tensor(1.5110, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 194 \tLoss: tensor(1.5087, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 195 \tLoss: tensor(1.4746, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 0 \tLoss: tensor(1.4844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 1 \tLoss: tensor(1.5206, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 2 \tLoss: tensor(1.5010, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 3 \tLoss: tensor(1.4981, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 4 \tLoss: tensor(1.4916, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 5 \tLoss: tensor(1.4977, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 6 \tLoss: tensor(1.4946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 7 \tLoss: tensor(1.4976, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 8 \tLoss: tensor(1.4966, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 9 \tLoss: tensor(1.4920, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 10 \tLoss: tensor(1.4982, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 11 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 12 \tLoss: tensor(1.4933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 13 \tLoss: tensor(1.5126, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 14 \tLoss: tensor(1.4949, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 15 \tLoss: tensor(1.5046, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 16 \tLoss: tensor(1.5108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 17 \tLoss: tensor(1.4903, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 18 \tLoss: tensor(1.4732, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 19 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 20 \tLoss: tensor(1.5319, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 21 \tLoss: tensor(1.5090, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 22 \tLoss: tensor(1.4937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 23 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 24 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 25 \tLoss: tensor(1.4974, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 26 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 27 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 28 \tLoss: tensor(1.5175, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 29 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 30 \tLoss: tensor(1.4814, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 31 \tLoss: tensor(1.5062, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 32 \tLoss: tensor(1.5041, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 33 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 34 \tLoss: tensor(1.4976, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 35 \tLoss: tensor(1.4999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 36 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 37 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 38 \tLoss: tensor(1.5045, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 39 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 40 \tLoss: tensor(1.5078, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 41 \tLoss: tensor(1.5035, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 42 \tLoss: tensor(1.4991, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 43 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 44 \tLoss: tensor(1.4821, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 45 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 46 \tLoss: tensor(1.5128, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 47 \tLoss: tensor(1.5041, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 48 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 49 \tLoss: tensor(1.5016, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 50 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 51 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 52 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 53 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 54 \tLoss: tensor(1.5056, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 55 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 56 \tLoss: tensor(1.4736, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 57 \tLoss: tensor(1.4809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 58 \tLoss: tensor(1.4741, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 59 \tLoss: tensor(1.4951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 60 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 61 \tLoss: tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 62 \tLoss: tensor(1.4899, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 63 \tLoss: tensor(1.4921, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 64 \tLoss: tensor(1.5082, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 65 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 66 \tLoss: tensor(1.4970, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 67 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 68 \tLoss: tensor(1.4914, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 69 \tLoss: tensor(1.4984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 70 \tLoss: tensor(1.5042, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 71 \tLoss: tensor(1.4796, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 72 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 73 \tLoss: tensor(1.4810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 74 \tLoss: tensor(1.4978, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 75 \tLoss: tensor(1.4955, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 76 \tLoss: tensor(1.5241, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 77 \tLoss: tensor(1.4963, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 78 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 79 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 80 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 81 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 82 \tLoss: tensor(1.5041, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 83 \tLoss: tensor(1.5251, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 84 \tLoss: tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 85 \tLoss: tensor(1.5025, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 86 \tLoss: tensor(1.4937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 87 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 88 \tLoss: tensor(1.5055, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 89 \tLoss: tensor(1.5099, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 90 \tLoss: tensor(1.4917, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 91 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 92 \tLoss: tensor(1.4710, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 93 \tLoss: tensor(1.4812, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 94 \tLoss: tensor(1.4811, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 95 \tLoss: tensor(1.5013, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 96 \tLoss: tensor(1.4980, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 97 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 98 \tLoss: tensor(1.4749, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 99 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 100 \tLoss: tensor(1.5007, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 101 \tLoss: tensor(1.5037, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 102 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 103 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 104 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 105 \tLoss: tensor(1.4946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 106 \tLoss: tensor(1.4970, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 107 \tLoss: tensor(1.4827, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 108 \tLoss: tensor(1.4965, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 109 \tLoss: tensor(1.4983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 110 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 111 \tLoss: tensor(1.5112, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 112 \tLoss: tensor(1.4941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 113 \tLoss: tensor(1.5068, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 114 \tLoss: tensor(1.4984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 115 \tLoss: tensor(1.5010, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 116 \tLoss: tensor(1.4962, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 117 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 118 \tLoss: tensor(1.4927, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 119 \tLoss: tensor(1.4910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 120 \tLoss: tensor(1.4974, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 121 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 122 \tLoss: tensor(1.4994, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 123 \tLoss: tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 124 \tLoss: tensor(1.5034, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 125 \tLoss: tensor(1.4989, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 126 \tLoss: tensor(1.4844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 127 \tLoss: tensor(1.5005, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 128 \tLoss: tensor(1.4968, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 129 \tLoss: tensor(1.5011, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 130 \tLoss: tensor(1.5099, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 131 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 132 \tLoss: tensor(1.4949, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 133 \tLoss: tensor(1.4851, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 134 \tLoss: tensor(1.4907, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 135 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 136 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 137 \tLoss: tensor(1.4829, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 138 \tLoss: tensor(1.4882, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 139 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 140 \tLoss: tensor(1.4853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 141 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 142 \tLoss: tensor(1.4879, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 143 \tLoss: tensor(1.4861, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 144 \tLoss: tensor(1.4793, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 145 \tLoss: tensor(1.4834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 146 \tLoss: tensor(1.4996, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 147 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 148 \tLoss: tensor(1.4780, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 149 \tLoss: tensor(1.4846, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 150 \tLoss: tensor(1.4997, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 151 \tLoss: tensor(1.4937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 152 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 153 \tLoss: tensor(1.4913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 154 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 155 \tLoss: tensor(1.5242, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 156 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 157 \tLoss: tensor(1.5207, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 158 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 159 \tLoss: tensor(1.4968, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 160 \tLoss: tensor(1.5002, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 161 \tLoss: tensor(1.4932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 162 \tLoss: tensor(1.5004, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 163 \tLoss: tensor(1.4876, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 164 \tLoss: tensor(1.4935, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 165 \tLoss: tensor(1.4986, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 166 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 167 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 168 \tLoss: tensor(1.4864, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 169 \tLoss: tensor(1.4729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 170 \tLoss: tensor(1.4882, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 171 \tLoss: tensor(1.5014, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 172 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 173 \tLoss: tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 174 \tLoss: tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 175 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 176 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 177 \tLoss: tensor(1.4983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 178 \tLoss: tensor(1.5122, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 179 \tLoss: tensor(1.5128, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 180 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 181 \tLoss: tensor(1.4992, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 182 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 183 \tLoss: tensor(1.5069, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 184 \tLoss: tensor(1.4871, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 185 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 186 \tLoss: tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 187 \tLoss: tensor(1.4810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 188 \tLoss: tensor(1.5072, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 189 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 190 \tLoss: tensor(1.4814, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 191 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 192 \tLoss: tensor(1.4986, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 193 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 194 \tLoss: tensor(1.5019, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 195 \tLoss: tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 0 \tLoss: tensor(1.4856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 1 \tLoss: tensor(1.5072, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 2 \tLoss: tensor(1.4957, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 3 \tLoss: tensor(1.4730, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 4 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 5 \tLoss: tensor(1.5020, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 6 \tLoss: tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 7 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 8 \tLoss: tensor(1.4683, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 9 \tLoss: tensor(1.5019, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 10 \tLoss: tensor(1.4976, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 11 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 12 \tLoss: tensor(1.4910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 13 \tLoss: tensor(1.4899, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 14 \tLoss: tensor(1.4948, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 15 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 16 \tLoss: tensor(1.4919, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 17 \tLoss: tensor(1.4895, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 18 \tLoss: tensor(1.4934, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 19 \tLoss: tensor(1.5005, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 20 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 21 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 22 \tLoss: tensor(1.4842, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 23 \tLoss: tensor(1.4894, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 24 \tLoss: tensor(1.4882, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 25 \tLoss: tensor(1.5067, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 26 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 27 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 28 \tLoss: tensor(1.5014, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 29 \tLoss: tensor(1.4864, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 30 \tLoss: tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 31 \tLoss: tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 32 \tLoss: tensor(1.4918, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 33 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 34 \tLoss: tensor(1.4970, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 35 \tLoss: tensor(1.4783, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 36 \tLoss: tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 37 \tLoss: tensor(1.4907, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 38 \tLoss: tensor(1.4789, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 39 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 40 \tLoss: tensor(1.4962, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 41 \tLoss: tensor(1.5052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 42 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 43 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 44 \tLoss: tensor(1.4746, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 45 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 46 \tLoss: tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 47 \tLoss: tensor(1.4846, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 48 \tLoss: tensor(1.5056, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 49 \tLoss: tensor(1.4977, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 50 \tLoss: tensor(1.4947, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 51 \tLoss: tensor(1.4878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 52 \tLoss: tensor(1.5072, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 53 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 54 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 55 \tLoss: tensor(1.4827, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 56 \tLoss: tensor(1.4808, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 57 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 58 \tLoss: tensor(1.4940, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 59 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 60 \tLoss: tensor(1.5006, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 61 \tLoss: tensor(1.4727, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 62 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 63 \tLoss: tensor(1.4725, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 64 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 65 \tLoss: tensor(1.4909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 66 \tLoss: tensor(1.5074, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 67 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 68 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 69 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 70 \tLoss: tensor(1.4865, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 71 \tLoss: tensor(1.5065, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 72 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 73 \tLoss: tensor(1.4982, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 74 \tLoss: tensor(1.4882, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 75 \tLoss: tensor(1.4977, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 76 \tLoss: tensor(1.4735, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 77 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 78 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 79 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 80 \tLoss: tensor(1.4729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 81 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 82 \tLoss: tensor(1.4948, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 83 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 84 \tLoss: tensor(1.4998, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 85 \tLoss: tensor(1.4767, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 86 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 87 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 88 \tLoss: tensor(1.5030, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 89 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 90 \tLoss: tensor(1.5179, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 91 \tLoss: tensor(1.4871, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 92 \tLoss: tensor(1.4918, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 93 \tLoss: tensor(1.4745, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 94 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 95 \tLoss: tensor(1.4898, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 96 \tLoss: tensor(1.4826, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 97 \tLoss: tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 98 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 99 \tLoss: tensor(1.4963, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 100 \tLoss: tensor(1.4940, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 101 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 102 \tLoss: tensor(1.4968, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 103 \tLoss: tensor(1.4875, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 104 \tLoss: tensor(1.4809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 105 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 106 \tLoss: tensor(1.4982, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 107 \tLoss: tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 108 \tLoss: tensor(1.4779, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 109 \tLoss: tensor(1.4946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 110 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 111 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 112 \tLoss: tensor(1.4943, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 113 \tLoss: tensor(1.5054, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 114 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 115 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 116 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 117 \tLoss: tensor(1.4817, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 118 \tLoss: tensor(1.4842, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 119 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 120 \tLoss: tensor(1.5096, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 121 \tLoss: tensor(1.4853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 122 \tLoss: tensor(1.4810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 123 \tLoss: tensor(1.4767, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 124 \tLoss: tensor(1.5058, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 125 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 126 \tLoss: tensor(1.5091, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 127 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 128 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 129 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 130 \tLoss: tensor(1.4902, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 131 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 132 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 133 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 134 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 135 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 136 \tLoss: tensor(1.4864, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 137 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 138 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 139 \tLoss: tensor(1.5029, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 140 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 141 \tLoss: tensor(1.4871, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 142 \tLoss: tensor(1.4919, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 143 \tLoss: tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 144 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 145 \tLoss: tensor(1.4848, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 146 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 147 \tLoss: tensor(1.4816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 148 \tLoss: tensor(1.4919, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 149 \tLoss: tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 150 \tLoss: tensor(1.4725, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 151 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 152 \tLoss: tensor(1.4926, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 153 \tLoss: tensor(1.4724, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 154 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 155 \tLoss: tensor(1.4814, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 156 \tLoss: tensor(1.4961, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 157 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 158 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 159 \tLoss: tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 160 \tLoss: tensor(1.4935, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 161 \tLoss: tensor(1.4876, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 162 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 163 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 164 \tLoss: tensor(1.4873, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 165 \tLoss: tensor(1.4827, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 166 \tLoss: tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 167 \tLoss: tensor(1.4817, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 168 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 169 \tLoss: tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 170 \tLoss: tensor(1.5049, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 171 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 172 \tLoss: tensor(1.4898, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 173 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 174 \tLoss: tensor(1.4816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 175 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 176 \tLoss: tensor(1.5000, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 177 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 178 \tLoss: tensor(1.4992, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 179 \tLoss: tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 180 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 181 \tLoss: tensor(1.4922, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 182 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 183 \tLoss: tensor(1.4782, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 184 \tLoss: tensor(1.5010, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 185 \tLoss: tensor(1.4969, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 186 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 187 \tLoss: tensor(1.4846, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 188 \tLoss: tensor(1.5018, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 189 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 190 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 191 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 192 \tLoss: tensor(1.4978, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 193 \tLoss: tensor(1.4782, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 194 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 195 \tLoss: tensor(1.4734, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 0 \tLoss: tensor(1.4925, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 1 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 2 \tLoss: tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 3 \tLoss: tensor(1.4963, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 4 \tLoss: tensor(1.4940, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 5 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 6 \tLoss: tensor(1.4775, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 7 \tLoss: tensor(1.4813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 8 \tLoss: tensor(1.4914, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 9 \tLoss: tensor(1.4949, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 10 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 11 \tLoss: tensor(1.4922, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 12 \tLoss: tensor(1.4768, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 13 \tLoss: tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 14 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 15 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 16 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 17 \tLoss: tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 18 \tLoss: tensor(1.4878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 19 \tLoss: tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 20 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 21 \tLoss: tensor(1.4865, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 22 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 23 \tLoss: tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 24 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 25 \tLoss: tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 26 \tLoss: tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 27 \tLoss: tensor(1.4915, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 28 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 29 \tLoss: tensor(1.4808, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 30 \tLoss: tensor(1.4645, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 31 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 32 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 33 \tLoss: tensor(1.5167, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 34 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 35 \tLoss: tensor(1.4842, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 36 \tLoss: tensor(1.4844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 37 \tLoss: tensor(1.4956, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 38 \tLoss: tensor(1.4864, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 39 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 40 \tLoss: tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 41 \tLoss: tensor(1.4918, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 42 \tLoss: tensor(1.4917, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 43 \tLoss: tensor(1.4811, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 44 \tLoss: tensor(1.4878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 45 \tLoss: tensor(1.4764, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 46 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 47 \tLoss: tensor(1.4759, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 48 \tLoss: tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 49 \tLoss: tensor(1.4715, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 50 \tLoss: tensor(1.5090, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 51 \tLoss: tensor(1.5021, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 52 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 53 \tLoss: tensor(1.4888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 54 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 55 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 56 \tLoss: tensor(1.4747, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 57 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 58 \tLoss: tensor(1.4742, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 59 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 60 \tLoss: tensor(1.4834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 61 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 62 \tLoss: tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 63 \tLoss: tensor(1.4874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 64 \tLoss: tensor(1.4813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 65 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 66 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 67 \tLoss: tensor(1.4910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 68 \tLoss: tensor(1.4932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 69 \tLoss: tensor(1.4738, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 70 \tLoss: tensor(1.4729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 71 \tLoss: tensor(1.4932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 72 \tLoss: tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 73 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 74 \tLoss: tensor(1.4884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 75 \tLoss: tensor(1.4846, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 76 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 77 \tLoss: tensor(1.4947, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 78 \tLoss: tensor(1.4946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 79 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 80 \tLoss: tensor(1.4842, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 81 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 82 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 83 \tLoss: tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 84 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 85 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 86 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 87 \tLoss: tensor(1.4820, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 88 \tLoss: tensor(1.4694, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 89 \tLoss: tensor(1.4926, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 90 \tLoss: tensor(1.4899, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 91 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 92 \tLoss: tensor(1.4750, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 93 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 94 \tLoss: tensor(1.5092, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 95 \tLoss: tensor(1.4951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 96 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 97 \tLoss: tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 98 \tLoss: tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 99 \tLoss: tensor(1.4899, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 100 \tLoss: tensor(1.4683, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 101 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 102 \tLoss: tensor(1.4736, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 103 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 104 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 105 \tLoss: tensor(1.4964, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 106 \tLoss: tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 107 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 108 \tLoss: tensor(1.4796, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 109 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 110 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 111 \tLoss: tensor(1.4940, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 112 \tLoss: tensor(1.4669, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 113 \tLoss: tensor(1.4834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 114 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tBatch: 115 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 116 \tLoss: tensor(1.4707, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 117 \tLoss: tensor(1.4715, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 118 \tLoss: tensor(1.4903, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 119 \tLoss: tensor(1.4948, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 120 \tLoss: tensor(1.4832, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 121 \tLoss: tensor(1.4737, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 122 \tLoss: tensor(1.4819, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 123 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 124 \tLoss: tensor(1.4974, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 125 \tLoss: tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 126 \tLoss: tensor(1.4814, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 127 \tLoss: tensor(1.5020, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 128 \tLoss: tensor(1.4961, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 129 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 130 \tLoss: tensor(1.4826, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 131 \tLoss: tensor(1.4888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 132 \tLoss: tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 133 \tLoss: tensor(1.4894, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 134 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 135 \tLoss: tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 136 \tLoss: tensor(1.5013, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 137 \tLoss: tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 138 \tLoss: tensor(1.4891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 139 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 140 \tLoss: tensor(1.4694, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 141 \tLoss: tensor(1.4915, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 142 \tLoss: tensor(1.4972, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 143 \tLoss: tensor(1.4718, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 144 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 145 \tLoss: tensor(1.4727, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 146 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 147 \tLoss: tensor(1.4865, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 148 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 149 \tLoss: tensor(1.4920, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 150 \tLoss: tensor(1.4795, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 151 \tLoss: tensor(1.4940, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 152 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 153 \tLoss: tensor(1.4939, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 154 \tLoss: tensor(1.4816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 155 \tLoss: tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 156 \tLoss: tensor(1.4837, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 157 \tLoss: tensor(1.4813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 158 \tLoss: tensor(1.4775, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 159 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 160 \tLoss: tensor(1.4873, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 161 \tLoss: tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 162 \tLoss: tensor(1.4882, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 163 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 164 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 165 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 166 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 167 \tLoss: tensor(1.4737, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 168 \tLoss: tensor(1.4918, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 169 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 170 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 171 \tLoss: tensor(1.4809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 172 \tLoss: tensor(1.4937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 173 \tLoss: tensor(1.4854, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 174 \tLoss: tensor(1.4729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 175 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 176 \tLoss: tensor(1.4796, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 177 \tLoss: tensor(1.4829, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 178 \tLoss: tensor(1.4761, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 179 \tLoss: tensor(1.4834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 180 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 181 \tLoss: tensor(1.4819, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 182 \tLoss: tensor(1.4748, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 183 \tLoss: tensor(1.4767, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 184 \tLoss: tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 185 \tLoss: tensor(1.4871, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 186 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 187 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 188 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 189 \tLoss: tensor(1.4756, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 190 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 191 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 192 \tLoss: tensor(1.4844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 193 \tLoss: tensor(1.4733, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 194 \tLoss: tensor(1.4856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 195 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 0 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 1 \tLoss: tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 2 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 3 \tLoss: tensor(1.4724, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 4 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 5 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 6 \tLoss: tensor(1.4783, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 7 \tLoss: tensor(1.4932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 8 \tLoss: tensor(1.4894, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 9 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 10 \tLoss: tensor(1.4735, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 11 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 12 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 13 \tLoss: tensor(1.4805, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 14 \tLoss: tensor(1.4780, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 15 \tLoss: tensor(1.4789, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 16 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 17 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 18 \tLoss: tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 19 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 20 \tLoss: tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 21 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 22 \tLoss: tensor(1.4780, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 23 \tLoss: tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 24 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 25 \tLoss: tensor(1.4949, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 26 \tLoss: tensor(1.4826, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 27 \tLoss: tensor(1.5024, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 28 \tLoss: tensor(1.4816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 29 \tLoss: tensor(1.4710, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 30 \tLoss: tensor(1.4664, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 31 \tLoss: tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 32 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 33 \tLoss: tensor(1.4699, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 34 \tLoss: tensor(1.4782, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 35 \tLoss: tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 36 \tLoss: tensor(1.5012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 37 \tLoss: tensor(1.4878, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tBatch: 38 \tLoss: tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 39 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 40 \tLoss: tensor(1.4680, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 41 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 42 \tLoss: tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 43 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 44 \tLoss: tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 45 \tLoss: tensor(1.4856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 46 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 47 \tLoss: tensor(1.4910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 48 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 49 \tLoss: tensor(1.4834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 50 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 51 \tLoss: tensor(1.4809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 52 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 53 \tLoss: tensor(1.4826, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 54 \tLoss: tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 55 \tLoss: tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 56 \tLoss: tensor(1.4775, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 57 \tLoss: tensor(1.4864, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 58 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 59 \tLoss: tensor(1.4838, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 60 \tLoss: tensor(1.4748, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 61 \tLoss: tensor(1.4694, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 62 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 63 \tLoss: tensor(1.4749, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 64 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 65 \tLoss: tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 66 \tLoss: tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 67 \tLoss: tensor(1.4748, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 68 \tLoss: tensor(1.4816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 69 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 70 \tLoss: tensor(1.4816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 71 \tLoss: tensor(1.4743, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 72 \tLoss: tensor(1.4725, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 73 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 74 \tLoss: tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 75 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 76 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 77 \tLoss: tensor(1.4712, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 78 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 79 \tLoss: tensor(1.4826, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 80 \tLoss: tensor(1.4750, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 81 \tLoss: tensor(1.4776, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 82 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 83 \tLoss: tensor(1.4749, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 84 \tLoss: tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 85 \tLoss: tensor(1.4722, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 86 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 87 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 88 \tLoss: tensor(1.4723, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 89 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 90 \tLoss: tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 91 \tLoss: tensor(1.4727, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 92 \tLoss: tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 93 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 94 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 95 \tLoss: tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 96 \tLoss: tensor(1.4819, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 97 \tLoss: tensor(1.4680, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 98 \tLoss: tensor(1.4910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 99 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 100 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 101 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 102 \tLoss: tensor(1.4845, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 103 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 104 \tLoss: tensor(1.4827, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 105 \tLoss: tensor(1.4845, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 106 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 107 \tLoss: tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 108 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 109 \tLoss: tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 110 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 111 \tLoss: tensor(1.4861, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 112 \tLoss: tensor(1.4695, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 113 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 114 \tLoss: tensor(1.4826, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 115 \tLoss: tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 116 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 117 \tLoss: tensor(1.4874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 118 \tLoss: tensor(1.4750, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 119 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 120 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 121 \tLoss: tensor(1.4884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 122 \tLoss: tensor(1.4724, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 123 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 124 \tLoss: tensor(1.4810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 125 \tLoss: tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 126 \tLoss: tensor(1.4998, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 127 \tLoss: tensor(1.4764, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 128 \tLoss: tensor(1.4913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 129 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 130 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 131 \tLoss: tensor(1.4842, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 132 \tLoss: tensor(1.4670, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 133 \tLoss: tensor(1.4738, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 134 \tLoss: tensor(1.4744, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 135 \tLoss: tensor(1.4657, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 136 \tLoss: tensor(1.4761, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 137 \tLoss: tensor(1.4716, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 138 \tLoss: tensor(1.4838, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 139 \tLoss: tensor(1.4816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 140 \tLoss: tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 141 \tLoss: tensor(1.4820, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 142 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 143 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 144 \tLoss: tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 145 \tLoss: tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 146 \tLoss: tensor(1.4744, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 147 \tLoss: tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 148 \tLoss: tensor(1.4727, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 149 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 150 \tLoss: tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 151 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 152 \tLoss: tensor(1.4791, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 153 \tLoss: tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 154 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 155 \tLoss: tensor(1.4667, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 156 \tLoss: tensor(1.4916, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tBatch: 157 \tLoss: tensor(1.4692, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 158 \tLoss: tensor(1.4811, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 159 \tLoss: tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 160 \tLoss: tensor(1.4814, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 161 \tLoss: tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 162 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 163 \tLoss: tensor(1.4808, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 164 \tLoss: tensor(1.4821, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 165 \tLoss: tensor(1.4968, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 166 \tLoss: tensor(1.5004, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 167 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 168 \tLoss: tensor(1.4831, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 169 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 170 \tLoss: tensor(1.4728, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 171 \tLoss: tensor(1.4898, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 172 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 173 \tLoss: tensor(1.4680, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 174 \tLoss: tensor(1.4831, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 175 \tLoss: tensor(1.4656, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 176 \tLoss: tensor(1.4644, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 177 \tLoss: tensor(1.4907, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 178 \tLoss: tensor(1.4813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 179 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 180 \tLoss: tensor(1.4939, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 181 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 182 \tLoss: tensor(1.4700, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 183 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 184 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 185 \tLoss: tensor(1.4758, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 186 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 187 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 188 \tLoss: tensor(1.4744, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 189 \tLoss: tensor(1.4811, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 190 \tLoss: tensor(1.4686, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 191 \tLoss: tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 192 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 193 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 194 \tLoss: tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 195 \tLoss: tensor(1.4693, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 0 \tLoss: tensor(1.4695, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 1 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 2 \tLoss: tensor(1.4854, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 3 \tLoss: tensor(1.5002, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 4 \tLoss: tensor(1.4780, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 5 \tLoss: tensor(1.4752, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 6 \tLoss: tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 7 \tLoss: tensor(1.4711, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 8 \tLoss: tensor(1.4776, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 9 \tLoss: tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 10 \tLoss: tensor(1.4734, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 11 \tLoss: tensor(1.4657, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 12 \tLoss: tensor(1.4764, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 13 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 14 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 15 \tLoss: tensor(1.4708, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 16 \tLoss: tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 17 \tLoss: tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 18 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 19 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 20 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 21 \tLoss: tensor(1.4751, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 22 \tLoss: tensor(1.4813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 23 \tLoss: tensor(1.4775, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 24 \tLoss: tensor(1.4948, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 25 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 26 \tLoss: tensor(1.4976, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 27 \tLoss: tensor(1.4694, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 28 \tLoss: tensor(1.4668, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 29 \tLoss: tensor(1.4757, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 30 \tLoss: tensor(1.4974, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 31 \tLoss: tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 32 \tLoss: tensor(1.4687, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 33 \tLoss: tensor(1.4864, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 34 \tLoss: tensor(1.4891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 35 \tLoss: tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 36 \tLoss: tensor(1.4794, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 37 \tLoss: tensor(1.4757, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 38 \tLoss: tensor(1.4680, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 39 \tLoss: tensor(1.4805, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 40 \tLoss: tensor(1.4617, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 41 \tLoss: tensor(1.4831, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 42 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 43 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 44 \tLoss: tensor(1.4832, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 45 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 46 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 47 \tLoss: tensor(1.4684, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 48 \tLoss: tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 49 \tLoss: tensor(1.4832, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 50 \tLoss: tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 51 \tLoss: tensor(1.4723, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 52 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 53 \tLoss: tensor(1.4829, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 54 \tLoss: tensor(1.4699, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 55 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 56 \tLoss: tensor(1.4758, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 57 \tLoss: tensor(1.4786, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 58 \tLoss: tensor(1.4661, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 59 \tLoss: tensor(1.4809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 60 \tLoss: tensor(1.4731, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 61 \tLoss: tensor(1.4851, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 62 \tLoss: tensor(1.4984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 63 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 64 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 65 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 66 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 67 \tLoss: tensor(1.4761, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 68 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 69 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 70 \tLoss: tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 71 \tLoss: tensor(1.4664, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 72 \tLoss: tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 73 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 74 \tLoss: tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 75 \tLoss: tensor(1.4698, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 76 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 77 \tLoss: tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 78 \tLoss: tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 79 \tLoss: tensor(1.4739, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tBatch: 80 \tLoss: tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 81 \tLoss: tensor(1.4767, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 82 \tLoss: tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 83 \tLoss: tensor(1.4693, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 84 \tLoss: tensor(1.4729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 85 \tLoss: tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 86 \tLoss: tensor(1.4953, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 87 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 88 \tLoss: tensor(1.4972, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 89 \tLoss: tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 90 \tLoss: tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 91 \tLoss: tensor(1.4819, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 92 \tLoss: tensor(1.4819, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 93 \tLoss: tensor(1.4751, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 94 \tLoss: tensor(1.4737, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 95 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 96 \tLoss: tensor(1.4776, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 97 \tLoss: tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 98 \tLoss: tensor(1.4690, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 99 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 100 \tLoss: tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 101 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 102 \tLoss: tensor(1.4671, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 103 \tLoss: tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 104 \tLoss: tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 105 \tLoss: tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 106 \tLoss: tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 107 \tLoss: tensor(1.4728, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 108 \tLoss: tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 109 \tLoss: tensor(1.4941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 110 \tLoss: tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 111 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 112 \tLoss: tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 113 \tLoss: tensor(1.4832, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 114 \tLoss: tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 115 \tLoss: tensor(1.4809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 116 \tLoss: tensor(1.4682, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 117 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 118 \tLoss: tensor(1.4788, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 119 \tLoss: tensor(1.4728, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 120 \tLoss: tensor(1.4696, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 121 \tLoss: tensor(1.4826, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 122 \tLoss: tensor(1.4660, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 123 \tLoss: tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 124 \tLoss: tensor(1.4747, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 125 \tLoss: tensor(1.4853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 126 \tLoss: tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 127 \tLoss: tensor(1.4711, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 128 \tLoss: tensor(1.4749, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 129 \tLoss: tensor(1.4789, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 130 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 131 \tLoss: tensor(1.4853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 132 \tLoss: tensor(1.4748, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 133 \tLoss: tensor(1.4923, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 134 \tLoss: tensor(1.4691, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 135 \tLoss: tensor(1.4715, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 136 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 137 \tLoss: tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 138 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 139 \tLoss: tensor(1.4758, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 140 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 141 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 142 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 143 \tLoss: tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 144 \tLoss: tensor(1.4751, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 145 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 146 \tLoss: tensor(1.4856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 147 \tLoss: tensor(1.4738, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 148 \tLoss: tensor(1.4689, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 149 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 150 \tLoss: tensor(1.4884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 151 \tLoss: tensor(1.4705, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 152 \tLoss: tensor(1.4749, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 153 \tLoss: tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 154 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 155 \tLoss: tensor(1.4736, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 156 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 157 \tLoss: tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 158 \tLoss: tensor(1.4810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 159 \tLoss: tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 160 \tLoss: tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 161 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 162 \tLoss: tensor(1.4794, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 163 \tLoss: tensor(1.4788, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 164 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 165 \tLoss: tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 166 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 167 \tLoss: tensor(1.4863, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 168 \tLoss: tensor(1.4856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 169 \tLoss: tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 170 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 171 \tLoss: tensor(1.5018, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 172 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 173 \tLoss: tensor(1.4675, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 174 \tLoss: tensor(1.4746, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 175 \tLoss: tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 176 \tLoss: tensor(1.4934, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 177 \tLoss: tensor(1.4786, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 178 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 179 \tLoss: tensor(1.4732, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 180 \tLoss: tensor(1.5103, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 181 \tLoss: tensor(1.4758, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 182 \tLoss: tensor(1.4741, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 183 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 184 \tLoss: tensor(1.4743, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 185 \tLoss: tensor(1.4838, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 186 \tLoss: tensor(1.4750, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 187 \tLoss: tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 188 \tLoss: tensor(1.4738, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 189 \tLoss: tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 190 \tLoss: tensor(1.4842, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 191 \tLoss: tensor(1.4728, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 192 \tLoss: tensor(1.4687, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 193 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 194 \tLoss: tensor(1.4795, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 195 \tLoss: tensor(1.4768, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "myNeuralNetwork = NeuralNetworkClass()\n",
    "\n",
    "# CREATE OUR OPTIMISER\n",
    "optimiser = torch.optim.Adam(              # what optimiser should we use?\n",
    "    myNeuralNetwork.parameters(),          # what should it optimise?\n",
    "    lr=learning_rate                       # using what learning rate?\n",
    ")\n",
    "\n",
    "# CREATE OUR CRITERION\n",
    "criterion = torch.nn.CrossEntropyLoss()             # callable class that compares our predictions to our labels and returns our loss\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()                            # we will use this to show our models performance on a graph\n",
    "    \n",
    "# TRAINING LOOP\n",
    "def train(model, epochs):\n",
    "    model.train()                                  # put the model into training mode (more on this later)\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader):\n",
    "            inputs, labels = minibatch\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            writer.add_scalar('Loss/Train', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "            \n",
    "            \n",
    "train(myNeuralNetwork, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the loss actually mean practically?\n",
    "\n",
    "The absolute value of the loss doesn't really mean much, it's just a way of continuously evaluating the relative performance of the model whilst it trains. The real metric of performance that we care about is the proportion of ***unseen*** examples that our neural network can correctly classify. These unseen examples are what the test loader consists of.\n",
    "\n",
    "Let's write the code to calculate that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def test(model):\n",
    "    num_correct = 0\n",
    "    num_examples = len(test_data)                       # test DATA not test LOADER\n",
    "    for inputs, labels in test_loader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    print('Accuracy:', percent_correct)\n",
    "    \n",
    "test(myNeuralNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. Compare the loss curves generated by using different batch sizes. What's the best? As you change the batch size, what variable do you need to change to give those curves the same domain over the x-axis (num writes to summary writer)\n",
    "2. It would be good to validate our model as we go along to ensure that we don't overfit. Let's write a training loop that tests the loss on the validation set after each epoch. Plot the validation error alongside What can you see on the graphs that indicates overfitting?\n",
    "3. What is the best accuracy you can achieve? Can you implement a grid search and a random search to try them automatically. Record all permutations that you try.\n",
    "4. What feature of the input data is our standard neural network not taking advantage of? Hint: '************* neural networks' take this into account.\n",
    "\n",
    "## Congratulations you boss, you've finished the notebook!\n",
    "\n",
    "Please provide your feedback [here](https://docs.google.com/forms/d/e/1FAIpQLSdZSxvkAE19vjDN4jpp0VvUBPGr_wdtayGAcRNfFGH7e7jQDQ/viewform?usp=sf_link). It means a lot to us.\n",
    "\n",
    "Next, you might want to check out:\n",
    "- [Convolutional Neural Networks](https://github.com/AI-Core/Convolutional-Neural-Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
